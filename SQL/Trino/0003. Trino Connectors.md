# ðŸ”¹ 1. Configure the Connector

Trino uses configuration files located in the **`etc/catalog/`** directory of your Trino installation.
Each connector has its **own properties file**, named `<catalog-name>.properties`.

ðŸ‘‰ Example: If you want a **MySQL** connector, you create:

`etc/catalog/mysql.properties`

```ini
connector.name=mysql
connection-url=jdbc:mysql://mysql-host:3306
connection-user=myuser
connection-password=mypassword
```

* `connector.name` â†’ type of connector (mysql, hive, postgresql, kafka, etc.)
* Other properties depend on the connector (URL, credentials, warehouse dir, etc.).

---

# ðŸ”¹ 2. Restart Trino

After creating or updating the properties file, restart your **Trino server** (or Kubernetes pod if deployed in k8s).
The connector will now be available as a **catalog**.

---

# ðŸ”¹ 3. Query Data Using the Connector

Once the catalog is added, you can query data using:

```
<catalog>.<schema>.<table>
```

For example:

### âœ… Query MySQL Table

```sql
SELECT * 
FROM mysql.sales_db.orders 
WHERE order_date > DATE '2023-01-01';
```

### âœ… Query Hive Table (from data lake)

```sql
SELECT customer_id, SUM(amount) 
FROM hive.analytics.transactions 
GROUP BY customer_id;
```

### âœ… Query Kafka Stream

```sql
SELECT * 
FROM kafka.default.clickstream 
LIMIT 10;
```

---

# ðŸ”¹ 4. Federated Queries Across Multiple Sources

One of Trinoâ€™s biggest strengths is querying across **different connectors** in one query.

Example: Join **MySQL orders** with **Hive customer data**:

```sql
SELECT o.order_id, c.customer_name, o.amount
FROM mysql.sales_db.orders o
JOIN hive.analytics.customers c
  ON o.customer_id = c.customer_id
WHERE o.amount > 1000;
```

ðŸ‘‰ No ETL needed â€” Trino pushes computation to workers and fetches results directly from both sources.

---

# ðŸ”¹ 5. Connector Examples

* **Hive / Iceberg / Delta Lake** â†’ query S3, ADLS, GCS data lakes.
* **RDBMS (MySQL, PostgreSQL, Oracle, SQL Server, DB2, etc.)** â†’ query transactional databases.
* **Kafka** â†’ query streaming data in real-time.
* **ElasticSearch / MongoDB / Cassandra** â†’ query NoSQL.
* **Pinot, Druid, BigQuery, Redshift, Snowflake** â†’ query specialized warehouses.

---

# ðŸ”¹ 6. Security & Access Control

* Use **Kerberos, LDAP, OAuth, or certificates** depending on connector.
* You can also apply **Trino access control rules** to restrict catalogs, schemas, or tables.

---

# ðŸ”¹ Summary

âœ… **Add connector config** â†’ `etc/catalog/<catalog>.properties`
âœ… **Restart Trino** â†’ Connector becomes available as a **catalog**
âœ… **Query using SQL** â†’ `catalog.schema.table`
âœ… **Join across connectors** â†’ Federated analytics without ETL
