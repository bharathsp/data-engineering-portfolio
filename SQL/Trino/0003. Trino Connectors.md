# 🔹 1. Configure the Connector

Trino uses configuration files located in the **`etc/catalog/`** directory of your Trino installation.
Each connector has its **own properties file**, named `<catalog-name>.properties`.

👉 Example: If you want a **MySQL** connector, you create:

`etc/catalog/mysql.properties`

```ini
connector.name=mysql
connection-url=jdbc:mysql://mysql-host:3306
connection-user=myuser
connection-password=mypassword
```

* `connector.name` → type of connector (mysql, hive, postgresql, kafka, etc.)
* Other properties depend on the connector (URL, credentials, warehouse dir, etc.).

---

# 🔹 2. Restart Trino

After creating or updating the properties file, restart your **Trino server** (or Kubernetes pod if deployed in k8s).
The connector will now be available as a **catalog**.

---

# 🔹 3. Query Data Using the Connector

Once the catalog is added, you can query data using:

```
<catalog>.<schema>.<table>
```

For example:

### ✅ Query MySQL Table

```sql
SELECT * 
FROM mysql.sales_db.orders 
WHERE order_date > DATE '2023-01-01';
```

### ✅ Query Hive Table (from data lake)

```sql
SELECT customer_id, SUM(amount) 
FROM hive.analytics.transactions 
GROUP BY customer_id;
```

### ✅ Query Kafka Stream

```sql
SELECT * 
FROM kafka.default.clickstream 
LIMIT 10;
```

---

# 🔹 4. Federated Queries Across Multiple Sources

One of Trino’s biggest strengths is querying across **different connectors** in one query.

Example: Join **MySQL orders** with **Hive customer data**:

```sql
SELECT o.order_id, c.customer_name, o.amount
FROM mysql.sales_db.orders o
JOIN hive.analytics.customers c
  ON o.customer_id = c.customer_id
WHERE o.amount > 1000;
```

👉 No ETL needed — Trino pushes computation to workers and fetches results directly from both sources.

---

# 🔹 5. Connector Examples

* **Hive / Iceberg / Delta Lake** → query S3, ADLS, GCS data lakes.
* **RDBMS (MySQL, PostgreSQL, Oracle, SQL Server, DB2, etc.)** → query transactional databases.
* **Kafka** → query streaming data in real-time.
* **ElasticSearch / MongoDB / Cassandra** → query NoSQL.
* **Pinot, Druid, BigQuery, Redshift, Snowflake** → query specialized warehouses.

---

# 🔹 6. Security & Access Control

* Use **Kerberos, LDAP, OAuth, or certificates** depending on connector.
* You can also apply **Trino access control rules** to restrict catalogs, schemas, or tables.

---

# Troubleshooting a **connector failure in Trino**.

## 📌 Step 1: Identify the Connector & Error

1. 🔍 Check the **error message** returned by Trino.

   * Example: `org.apache.hive.HiveException: Unable to connect to Hive Metastore`
2. Note the **connector type**: Hive, MySQL, PostgreSQL, Iceberg, Kafka, etc.

> This helps narrow down whether the problem is **Trino config, network, authentication, or source data**.

## 📌 Step 2: Check Connector Configuration

1. 🔧 Open the connector configuration file in `etc/catalog/<connector>.properties`.

   * Example: Hive connector (`hive.properties`):

```properties
connector.name=hive-hadoop2
hive.metastore.uri=thrift://hive-metastore-host:9083
hive.config.resources=/etc/hadoop/conf/core-site.xml,/etc/hadoop/conf/hdfs-site.xml
```

2. ✅ Common things to verify:

   * Hostname/IP and port of data source
   * Credentials and authentication type
   * Database/schema names
   * File format support (ORC, Parquet, Iceberg, etc.)

## 📌 Step 3: Test Network & Connectivity

1. 🖥 From Trino coordinator/worker:

```bash
# Ping the data source
ping <hostname>

# Test port connectivity
telnet <hostname> <port>
# or using nc
nc -vz <hostname> <port>
```

2. 🔄 Ensure firewall or network rules allow access to the source.

## 📌 Step 4: Check Trino Logs

* 📝 Logs are located in `var/log/trino/`.

  * Coordinator: `server.log`
  * Worker: `node.log`

* Look for:

  * Connection errors
  * Authentication failures
  * Timeout or resource errors

> Example log snippet:

```
2025-09-23 12:10:22 ERROR o.t.c.h.HiveClient: Unable to connect to Hive Metastore at thrift://hive-metastore-host:9083
```

## 📌 Step 5: Test Queries on Connector

* Run a **simple test query** to check connectivity:

```sql
SELECT * FROM hive.default.some_table LIMIT 10;
```

* If it works → issue may be **query-specific**.
* If it fails → issue may be **connector-level** (config, network, credentials).

## 📌 Step 6: Check Version Compatibility

* ⚠️ Ensure **Trino version** is compatible with the connector and the source.

  * Example: Hive 3.x metastore vs Hive connector version in Trino.
* Some connectors require **specific Hadoop, JDBC, or client library versions**.

## 📌 Step 7: Validate Authentication & Permissions

* Check if the user in Trino has proper access:

  * JDBC credentials for relational databases
  * HDFS permissions for Hive tables
  * S3 bucket permissions for Iceberg/Hive

## 📌 Step 8: Test on CLI/Other Clients

* Use **beeline, JDBC, or psql/mysql CLI** to verify connectivity.
* Helps isolate if the issue is **Trino-specific** or **general connectivity**.

## 📌 Step 9: Restart Trino Services

* Sometimes connector issues are due to **stale connections** or **classloader issues**.

```bash
bin/launcher restart
```

## 📌 Step 10: Escalate & Document

* If still failing:

  * Gather logs, config, Trino version, connector version
  * Open a ticket with your **DB/Storage team**
* Document the troubleshooting steps for future reference.

## 💡 Pro Tips

* Use **EXPLAIN or DESCRIBE** queries to see if Trino can access metadata.
* Enable **debug logging** for the connector in `log.properties`:

```
io.trino.plugin.hive=DEBUG
```

* For cloud connectors (S3, ADLS, GCS), check **network endpoints, credentials, and IAM policies**.

---

# 🔹 Summary

✅ **Add connector config** → `etc/catalog/<catalog>.properties`
✅ **Restart Trino** → Connector becomes available as a **catalog**
✅ **Query using SQL** → `catalog.schema.table`
✅ **Join across connectors** → Federated analytics without ETL
