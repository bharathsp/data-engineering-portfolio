# ğŸ”¹ 1. Configure the Connector

Trino uses configuration files located in the **`etc/catalog/`** directory of your Trino installation.
Each connector has its **own properties file**, named `<catalog-name>.properties`.

ğŸ‘‰ Example: If you want a **MySQL** connector, you create:

`etc/catalog/mysql.properties`

```ini
connector.name=mysql
connection-url=jdbc:mysql://mysql-host:3306
connection-user=myuser
connection-password=mypassword
```

* `connector.name` â†’ type of connector (mysql, hive, postgresql, kafka, etc.)
* Other properties depend on the connector (URL, credentials, warehouse dir, etc.).

---

# ğŸ”¹ 2. Restart Trino

After creating or updating the properties file, restart your **Trino server** (or Kubernetes pod if deployed in k8s).
The connector will now be available as a **catalog**.

---

# ğŸ”¹ 3. Query Data Using the Connector

Once the catalog is added, you can query data using:

```
<catalog>.<schema>.<table>
```

For example:

### âœ… Query MySQL Table

```sql
SELECT * 
FROM mysql.sales_db.orders 
WHERE order_date > DATE '2023-01-01';
```

### âœ… Query Hive Table (from data lake)

```sql
SELECT customer_id, SUM(amount) 
FROM hive.analytics.transactions 
GROUP BY customer_id;
```

### âœ… Query Kafka Stream

```sql
SELECT * 
FROM kafka.default.clickstream 
LIMIT 10;
```

---

# ğŸ”¹ 4. Federated Queries Across Multiple Sources

One of Trinoâ€™s biggest strengths is querying across **different connectors** in one query.

Example: Join **MySQL orders** with **Hive customer data**:

```sql
SELECT o.order_id, c.customer_name, o.amount
FROM mysql.sales_db.orders o
JOIN hive.analytics.customers c
  ON o.customer_id = c.customer_id
WHERE o.amount > 1000;
```

ğŸ‘‰ No ETL needed â€” Trino pushes computation to workers and fetches results directly from both sources.

---

# ğŸ”¹ 5. Connector Examples

* **Hive / Iceberg / Delta Lake** â†’ query S3, ADLS, GCS data lakes.
* **RDBMS (MySQL, PostgreSQL, Oracle, SQL Server, DB2, etc.)** â†’ query transactional databases.
* **Kafka** â†’ query streaming data in real-time.
* **ElasticSearch / MongoDB / Cassandra** â†’ query NoSQL.
* **Pinot, Druid, BigQuery, Redshift, Snowflake** â†’ query specialized warehouses.

---

# ğŸ”¹ 6. Security & Access Control

* Use **Kerberos, LDAP, OAuth, or certificates** depending on connector.
* You can also apply **Trino access control rules** to restrict catalogs, schemas, or tables.

---

# Troubleshooting a **connector failure in Trino**.

## ğŸ“Œ Step 1: Identify the Connector & Error

1. ğŸ” Check the **error message** returned by Trino.

   * Example: `org.apache.hive.HiveException: Unable to connect to Hive Metastore`
2. Note the **connector type**: Hive, MySQL, PostgreSQL, Iceberg, Kafka, etc.

> This helps narrow down whether the problem is **Trino config, network, authentication, or source data**.

## ğŸ“Œ Step 2: Check Connector Configuration

1. ğŸ”§ Open the connector configuration file in `etc/catalog/<connector>.properties`.

   * Example: Hive connector (`hive.properties`):

```properties
connector.name=hive-hadoop2
hive.metastore.uri=thrift://hive-metastore-host:9083
hive.config.resources=/etc/hadoop/conf/core-site.xml,/etc/hadoop/conf/hdfs-site.xml
```

2. âœ… Common things to verify:

   * Hostname/IP and port of data source
   * Credentials and authentication type
   * Database/schema names
   * File format support (ORC, Parquet, Iceberg, etc.)

## ğŸ“Œ Step 3: Test Network & Connectivity

1. ğŸ–¥ From Trino coordinator/worker:

```bash
# Ping the data source
ping <hostname>

# Test port connectivity
telnet <hostname> <port>
# or using nc
nc -vz <hostname> <port>
```

2. ğŸ”„ Ensure firewall or network rules allow access to the source.

## ğŸ“Œ Step 4: Check Trino Logs

* ğŸ“ Logs are located in `var/log/trino/`.

  * Coordinator: `server.log`
  * Worker: `node.log`

* Look for:

  * Connection errors
  * Authentication failures
  * Timeout or resource errors

> Example log snippet:

```
2025-09-23 12:10:22 ERROR o.t.c.h.HiveClient: Unable to connect to Hive Metastore at thrift://hive-metastore-host:9083
```

## ğŸ“Œ Step 5: Test Queries on Connector

* Run a **simple test query** to check connectivity:

```sql
SELECT * FROM hive.default.some_table LIMIT 10;
```

* If it works â†’ issue may be **query-specific**.
* If it fails â†’ issue may be **connector-level** (config, network, credentials).

## ğŸ“Œ Step 6: Check Version Compatibility

* âš ï¸ Ensure **Trino version** is compatible with the connector and the source.

  * Example: Hive 3.x metastore vs Hive connector version in Trino.
* Some connectors require **specific Hadoop, JDBC, or client library versions**.

## ğŸ“Œ Step 7: Validate Authentication & Permissions

* Check if the user in Trino has proper access:

  * JDBC credentials for relational databases
  * HDFS permissions for Hive tables
  * S3 bucket permissions for Iceberg/Hive

## ğŸ“Œ Step 8: Test on CLI/Other Clients

* Use **beeline, JDBC, or psql/mysql CLI** to verify connectivity.
* Helps isolate if the issue is **Trino-specific** or **general connectivity**.

## ğŸ“Œ Step 9: Restart Trino Services

* Sometimes connector issues are due to **stale connections** or **classloader issues**.

```bash
bin/launcher restart
```

## ğŸ“Œ Step 10: Escalate & Document

* If still failing:

  * Gather logs, config, Trino version, connector version
  * Open a ticket with your **DB/Storage team**
* Document the troubleshooting steps for future reference.

## ğŸ’¡ Pro Tips

* Use **EXPLAIN or DESCRIBE** queries to see if Trino can access metadata.
* Enable **debug logging** for the connector in `log.properties`:

```
io.trino.plugin.hive=DEBUG
```

* For cloud connectors (S3, ADLS, GCS), check **network endpoints, credentials, and IAM policies**.

---

# ğŸ”¹ Summary

âœ… **Add connector config** â†’ `etc/catalog/<catalog>.properties`
âœ… **Restart Trino** â†’ Connector becomes available as a **catalog**
âœ… **Query using SQL** â†’ `catalog.schema.table`
âœ… **Join across connectors** â†’ Federated analytics without ETL
