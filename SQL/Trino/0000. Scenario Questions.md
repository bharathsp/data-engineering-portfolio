# Question 1

---

There is data in S3 and some data in SQL <br> 
S3->Orders data <br>
SQL -> Product data <br>
Use trino to combine the above data <br>

---

## 🔹 Step 1. Configure Trino Connectors

Trino connects to both **S3** (via Hive or Iceberg/Delta/Lakehouse connector) and **SQL** (e.g., MySQL, Postgres, SQL Server) by defining catalogs in the Trino `etc/catalog` directory.

### Example: S3 (Orders data via Hive catalog)

Create file: `etc/catalog/hive.properties`

```ini
connector.name=hive
hive.metastore.uri=thrift://<metastore-host>:9083
hive.s3.aws-access-key=YOUR_AWS_KEY
hive.s3.aws-secret-key=YOUR_AWS_SECRET
hive.s3.region=us-east-1
```

This lets you query tables like `hive.default.orders` (assuming Orders data is in S3 as Parquet/ORC/CSV).

---

### Example: SQL (Products data via MySQL)

Create file: `etc/catalog/mysql.properties`

```ini
connector.name=mysql
connection-url=jdbc:mysql://<mysql-host>:3306/productsdb
connection-user=trino_user
connection-password=your_password
```

This lets you query tables like `mysql.productsdb.products`.

---

## 🔹 Step 2. Example Tables

* **Orders (S3 via Hive)**

  ```sql
  CREATE TABLE hive.default.orders (
      order_id BIGINT,
      product_id BIGINT,
      customer_id BIGINT,
      quantity INT,
      order_date DATE
  )
  WITH (
      format = 'PARQUET',
      external_location = 's3a://your-bucket/orders/'
  );
  ```

* **Products (MySQL)**

  ```sql
  CREATE TABLE mysql.productsdb.products (
      product_id BIGINT,
      product_name VARCHAR,
      price DECIMAL(10,2),
      category VARCHAR
  );
  ```

---

## 🔹 Step 3. Query to Combine Data

Now you can **join across catalogs** in Trino:

```sql
SELECT 
    o.order_id,
    o.customer_id,
    o.order_date,
    p.product_name,
    p.category,
    o.quantity,
    (o.quantity * p.price) AS total_value
FROM hive.default.orders o
JOIN mysql.productsdb.products p
    ON o.product_id = p.product_id
WHERE o.order_date >= DATE '2025-01-01'
ORDER BY o.order_date DESC;
```

✅ This query scans **Orders from S3** and **Products from SQL** → joins them inside Trino without ETL.

---

## 🔹 Step 4. Optimization Tips

* Use **partitioning in S3** (e.g., partition Orders by `year`, `month`) so queries prune data:

  ```sql
  SELECT * FROM hive.default.orders WHERE order_date >= DATE '2025-09-01';
  ```

  → Only reads September 2025 partitions.

* If the SQL table (Products) is small, use **broadcast join**:

  ```sql
  SET SESSION join_distribution_type = 'BROADCAST';
  ```

* For frequent queries, consider **materializing the join into a new table** in Iceberg/Delta on S3.

👉 This way, Trino acts as a **federated query engine** combining your **data lake (S3)** and **database (SQL)** in one SQL query.

---

# Question 2

---

How do you consume data from a Kafka topic in Trino

---

Trino can consume data directly from **Kafka topics** using its **Kafka Connector**. It doesn’t store messages itself but exposes topics as **tables** you can query with SQL.

## 🔹 Step 1. Configure Kafka Connector

In Trino, create a **catalog properties file** in `etc/catalog/kafka.properties`:

```ini
connector.name=kafka
kafka.nodes=broker1:9092,broker2:9092
kafka.default-schema=default
kafka.table-names=orders,customers
```

* `kafka.nodes` → Kafka brokers
* `kafka.table-names` → List of Kafka topics exposed as tables

---

## 🔹 Step 2. Define JSON/Avro/Delimited Schema

Trino needs to know how to interpret the Kafka message payload.
Schemas are stored in **topic definition JSON files** under `etc/kafka/`.

Example: `etc/kafka/orders.json`

```json
{
  "tableName": "orders",
  "topicName": "orders",
  "message": {
    "dataFormat": "json",
    "fields": [
      { "name": "order_id", "type": "bigint" },
      { "name": "customer_id", "type": "bigint" },
      { "name": "product_id", "type": "bigint" },
      { "name": "quantity", "type": "integer" },
      { "name": "order_date", "type": "varchar" }
    ]
  }
}
```

Here:

* `dataFormat` could be `json`, `csv`, `raw`, or `avro`.
* You define message fields as columns.

---

## 🔹 Step 3. Query Kafka Topic in Trino

Now the Kafka topic appears as a SQL table:

```sql
SELECT order_id, customer_id, product_id, quantity, order_date
FROM kafka.default.orders
WHERE quantity > 5;
```

⚡ This query reads **Kafka messages** and projects them into structured rows.

---

## 🔹 Step 4. Special Kafka Metadata Columns

Trino adds **hidden metadata columns** for each Kafka record:

* `_partition_id` → Kafka partition number
* `_offset` → Offset in partition
* `_key` → Kafka message key
* `_timestamp` → Kafka message timestamp

Example:

```sql
SELECT order_id, quantity, _partition_id, _offset, _timestamp
FROM kafka.default.orders
ORDER BY _timestamp DESC;
```

---

## 🔹 Step 5. Use Cases

* Real-time analytics on Kafka streams
* Joining Kafka events with batch data (e.g., Orders in Kafka + Product catalog in SQL/S3)
* Debugging Kafka payloads with SQL

---

# Question 3

---

To handle auto scaling for a Trino cluster using Kubernetes? 

---

Autoscaling a Trino cluster on Kubernetes is a practical way to keep queries fast and costs under control. Below I’ll give a complete, pragmatic guide you can implement: concepts, architecture, components to wire together, concrete YAML examples (CPU-based HPA and a custom-metric HPA), safe scale-down procedures, common pitfalls, and tuning tips.

## Summary (one-line)

Autoscale **Trino workers** (stateless) with a Kubernetes **Horizontal Pod Autoscaler (HPA)** (based on CPU or custom Trino metrics exposed via Prometheus Adapter) and use the **Cluster Autoscaler** to grow/shrink node pool capacity. Keep the **coordinator** stable (no aggressive autoscale), ensure graceful termination of workers, and manage query concurrency with Trino resource groups to avoid overload.

---

## Architecture & components you’ll use

* **Trino Coordinator** — 1 (or active + standby) pod/service. Don’t autoscale aggressively.
* **Trino Worker Deployment** — stateless pods; autoscalable.
* **Kubernetes HPA** — scales the worker Deployment replicas.

  * Metric options:

    * Built-in resource metrics (CPU/memory via Metrics Server).
    * Custom metrics from Prometheus (e.g., queued queries, running queries, spill bytes) via **prometheus-adapter**.
* **Cluster Autoscaler** — adds/removes VMs to the node pool when pods are Pending due to lack of capacity (cloud-specific: AKS/GKE/EKS).
* **Prometheus + Grafana** — collect JMX metrics from Trino (via JMX exporter) and visualize.
* **Prometheus Adapter** — exposes Prometheus metrics to Kubernetes’ Custom Metrics API so HPA can use them.
* **Pod lifecycle hooks + readiness probes** — to perform graceful drain/decommission before pod termination.
* **PodDisruptionBudget (PDB)** — prevents too many workers being drained at once.

---

## High-level workflow

1. Prometheus collects Trino worker metrics (JVM, CPU, queries, spilled bytes, running tasks).
2. Prometheus Adapter exposes selected metrics to Kubernetes Custom Metrics API.
3. HPA watches a metric (CPU or custom e.g. `trino_running_queries`) and scales the Trino worker Deployment up/down.
4. If pods exceed node capacity, **Cluster Autoscaler** requests more VM nodes.
5. On scale-down, each worker pod uses a `preStop` hook and readiness probe to stop accepting new work and wait for current tasks to finish before terminating.

---

## What to autoscale (and what not to)

* **Autoscale: Trino workers** (Deployment) — they are stateless and register with coordinator.
* **Do not autoscale** the coordinator aggressively — it’s stateful; use 1 primary and optionally warm standby(s).
* Scale node pools with **Cluster Autoscaler** (cloud-managed).

---

## Option A — Simple, easy start: HPA on CPU

Use Kubernetes Metrics Server and scale on CPU utilization.

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: trino-worker-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: trino-worker
  minReplicas: 3
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 60
```

* Pros: easy, no Prometheus required.
* Cons: CPU alone may not capture query pressure (e.g., queued queries while CPU is low).

---

## Option B — Better: HPA using custom Trino metrics (recommended)

Expose query-level metrics (e.g., `trino_running_queries`, `trino_queued_queries`, `trino_spill_bytes`) via Prometheus, then use **prometheus-adapter** to expose them to HPA.

### Steps

1. **Expose Trino JMX metrics**: run a JMX exporter sidecar or scrape via JMX exporter to Prometheus.
2. **Prometheus** scrapes metrics.
3. **prometheus-adapter** maps Prometheus metrics to Kubernetes custom metrics.
4. **HPA** uses the custom metric.

### Example HPA using custom metric `trino_running_queries`:

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: trino-worker-hpa-custom
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: trino-worker
  minReplicas: 2
  maxReplicas: 50
  metrics:
  - type: Pods
    pods:
      metric:
        name: trino_running_queries
      target:
        type: AverageValue
        averageValue: "5"   # average running queries per pod target
```

* This asks: keep \~5 running queries per worker pod on average; when avg >5, add pods.

---

## Prometheus → Kubernetes custom metric mapping (brief)

`prometheus-adapter` configuration maps a Prometheus query to the metric `trino_running_queries`. Example mapping (in adapter config):

```yaml
rules:
- seriesQuery: 'trino_runtime_queries{job="trino",type="running"}'
  resources:
    overrides:
      namespace: {resource: "namespace"}
  name:
    matches: "trino_runtime_queries"
    as: "trino_running_queries"
  metricsQuery: 'sum(rate(trino_runtime_queries{job="trino",type="running"}[1m])) by (pod)'
```

(Exact mapping depends on your metric names/labels.)

---

## Safe scale-down (graceful termination)

When Kubernetes kills a pod (scale-down), we must ensure a worker doesn't abruptly lose tasks or cause query failures.

Recommended pattern:

1. **Readiness probe** indicates if pod can accept new tasks. When readiness fails, the coordinator will not send new splits.
2. **preStop hook** sets readiness to false and waits for active tasks to complete.
3. Give a large `terminationGracePeriodSeconds` so the pod can finish work.

Example `Deployment` fragment:

```yaml
spec:
  terminationGracePeriodSeconds: 600   # allow up to 10 minutes to drain
  template:
    spec:
      containers:
      - name: trino-worker
        image: <...>
        readinessProbe:
          httpGet:
            path: /v1/info   # or a custom health endpoint
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 10
        lifecycle:
          preStop:
            exec:
              command:
                - /bin/sh
                - -c
                - |
                  # 1) mark not ready (depends on your setup)
                  # 2) poll Trino system tables via REST/CLI until no tasks on this node
                  for i in $(seq 1 60); do
                    running=$(curl -s http://coordinator:8080/v1/query?state=running | jq '...') # pseudocode
                    if [ "$running" = "0" ]; then
                      break
                    fi
                    sleep 10
                  done
```

> **Notes / Caution:** The preStop script above is pseudocode: implement a safe decommission script that queries `system.runtime.tasks` for tasks on this node and waits (or calls any supported decommission API if available). The key ideas are: stop accepting new work, wait for current work to finish, then exit.

Alternative: implement a sidecar HTTP endpoint that flips readiness off and the coordinator will stop scheduling splits to this pod.

---

## Decommissioning considerations

* Trino does not automatically reassign in-progress tasks mid-way (task retries will be scheduled on failure). Best approach: finish tasks then exit.
* If you must immediate scale-down, expect queries to be retried; set `retry-policy` appropriately.

---

## Cluster Autoscaler (node scale)

* HPA scales pods; if pods cannot be scheduled due to lack of nodes, Kubernetes will leave them Pending until Cluster Autoscaler requests more VMs.
* On cloud (AKS/EKS/GKE) enable **Cluster Autoscaler** on the node pool where Trino workers run.
* Configure node groups with appropriate instance types (enough local storage if using spill-to-disk SSDs).

---

## Key Trino/Kubernetes config & best practices

1. **Resource requests & limits**: set CPU & memory requests so scheduler places pods appropriately; HPA uses requests to compute utilization.
2. **Local ephemeral storage for spills**: if using Trino spill-to-disk, use local SSD PVs (hostPath or local PVs) rather than network storage to preserve performance. Ensure node autoscaler scales node types with local storage available.
3. **PodDisruptionBudget**: prevent too many workers being evicted at once.
4. **Coordinator service**: use a stable Service DNS that workers use to register (env var or config).
5. **Health/readiness probes**: ensure correct readiness so workers don’t receive new tasks when draining.
6. **Resource groups & queueing** in Trino: use resource groups to limit concurrency and protect the cluster (avoid sudden spikes that cause autoscale thrash).
7. **Cooldowns & scaling thresholds**: HPA has stabilizationWindow and behavior fields in `autoscaling/v2` to avoid thrashing (rapid scale up/down).
8. **Max/min replicas**: set conservative bounds to control cost.

---

## Example: HPA with behavior (avoid thrash)

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: trino-worker-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: trino-worker
  minReplicas: 3
  maxReplicas: 30
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 65
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
        - type: Percent
          value: 100
          periodSeconds: 60
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
        - type: Percent
          value: 10
          periodSeconds: 60
```

---

## Monitoring & Alerts

* **Prometheus**: scrape Trino metrics + node metrics.
* **Grafana** dashboards: CPU/memory per worker, running/queued queries, spilled bytes, query durations, worker registrations.
* **Alerts**: high query queue growth, high spill rate, node resource pressure, HPA reaching max replicas.

---

## Common pitfalls & how to avoid them

* **Thrashing**: rapid scale up/down. Avoid by stabilization windows, conservative thresholds, and autoscale cooldowns.
* **Slow scale-out vs query latency**: HPA + Cluster Autoscaler take time. Ensure baseline capacity covers typical spikes or use pre-warming.
* **Scale-down kills in-progress work**: use graceful termination and PDBs.
* **Spill performance loss after scale**: ensure local disk per node; when cluster scales, new nodes must have equivalent disk and networking characteristics.
* **Coordinator overload**: many small workers or too many short-lived pods can flood coordinator — monitor coordinator metrics and avoid autoscaling coordinator.

---

## Example end-to-end checklist before enabling autoscale

1. Deploy Prometheus + JMX exporter for Trino metrics.
2. Deploy `prometheus-adapter` and map target metrics.
3. Create HPA (start with CPU-based to validate).
4. Enable Cluster Autoscaler for node pool.
5. Configure preStop/graceful drain script & readiness probe.
6. Add PDB to protect capacity.
7. Create resource groups in Trino to isolate workloads.
8. Test scaling by simulating load and watching query behavior.
9. Tune throttles, HPA stabilization, and max/min replicas.

---

## When autoscaling is **not** the full solution

* For very short spikes, autoscale might be too slow — consider **pre-warming** or having a larger baseline.
* For strict SLAs, tune resource groups + reserve capacity; use autoscale as cost optimization, not only reliability.

---

## Quick reference resources (what to set up)

* `prometheus` + `jmx_exporter` for Trino
* `prometheus-adapter` (expose custom metrics to HPA)
* `HorizontalPodAutoscaler` manifests (v2 API for custom metrics & behavior)
* `Cluster Autoscaler` for your cloud provider
* `PodDisruptionBudget`, readiness probes, preStop hook
* Trino resource-group config to avoid overload

---

# Question 4

---

Your Trino queries on a large dataset are running slower than expected.
How would you identify slow queries in Trino?
What strategies can you use to optimize query performance in Trino?
How do you handle data skew or uneven data distribution in Trino queries?
How would you optimize joins between large tables in Trino?
Scenario: You have a query joining a 1B row table with a 10K row table. How would you optimize it?

---

## 🐢 Identifying Slow Queries in Trino

1. **Trino Web UI / Coordinator**

   * Trino’s **web interface** shows all running and completed queries.
   * Key metrics: query runtime, stages, CPU time, memory usage.

2. **Query Logs**

   * Check the **`system.runtime.queries`** table:

     ```sql
     SELECT query_id, state, query, elapsed, cpu_time, memory_pool
     FROM system.runtime.queries
     WHERE state = 'RUNNING';
     ```
   * Look for queries with **high elapsed time or memory usage**.

3. **Stages and Tasks Analysis**

   * Slow queries often have **long-running stages**.
   * Check `system.runtime.tasks` to identify stages with uneven processing or tasks taking longer than others.

4. **Connector / Source Bottleneck**

   * Some sources (S3, Hive, MongoDB, etc.) may be **slow or rate-limited**.
   * Use connector-specific logs for further diagnostics.

## ⚡ Strategies to Optimize Query Performance

1. **Pushdown Filters & Projections** 🔽

   * Only fetch required rows and columns.
   * Example: `WHERE date > '2025-01-01'` and `SELECT only_needed_columns`.

2. **Partition Pruning** 📂

   * Query only relevant **partitions** in Hive, Iceberg, Delta Lake, or S3.
   * Example: `WHERE dt = '2025-09-23'` instead of scanning all dates.

3. **Use Columnar Formats** 🗃️

   * Parquet / ORC for storage → reduces disk I/O and speeds up queries.

4. **Pre-Aggregations / Materialized Views** 📊

   * For repetitive queries, pre-compute results to reduce computation at runtime.

5. **Memory & Concurrency Tuning** ⚙️

   * Adjust **`query.max-memory-per-node`**, **`query.max-total-memory-per-node`**, and **`task.concurrency`**.
   * Avoid spilling to disk unless necessary.

6. **Avoid SELECT \*\*\* unless needed** ❌

   * Only select columns that are required.

7. **Join / Aggregation Optimizations** 🔗

   * Use **broadcast joins** for small tables.
   * Use **partitioned joins** for large tables.

## 🏗️ Handling Data Skew or Uneven Distribution

* **Detect Skew:**

  * Query runtime tasks: some tasks take much longer → indicates skew.
  * Example: `system.runtime.tasks` table shows uneven splits.

* **Strategies:**

  1. **Repartition / Bucket Tables:**

     * Distribute data more evenly by hash on join keys.
  2. **Use Broadcast Join for Small Tables:**

     * If one table is small (fits in memory), send it to all nodes.
  3. **Salting Technique:**

     * Add a random prefix to keys for skewed partitions, then join on prefixed keys.

## 🔗 Optimizing Joins Between Large Tables

1. **Small + Large Table** → **Broadcast Join**

   * Small table is replicated across all nodes.
   * Example: `JOIN 10K rows table with 1B rows table`.

2. **Two Large Tables** → **Partitioned Join**

   * Redistribute tables on join key to balance tasks.

3. **Use Appropriate Join Type**

   * **INNER JOIN** vs LEFT/RIGHT OUTER → avoid unnecessary large intermediate results.

4. **Filter Early**

   * Apply WHERE clauses before the join to reduce data size.

## 🔍 Scenario: 1B Row Table JOIN 10K Row Table

* ✅ **Optimization Steps:**

1. **Broadcast Join**

   * Small 10K table fits in memory → broadcast it to all nodes.

   ```sql
   SELECT /*+ BROADCAST(small_table) */
          a.*, b.info
   FROM large_table a
   JOIN small_table b
   ON a.key = b.key;
   ```

2. **Pushdown Filters**

   * Filter `large_table` rows early with WHERE clauses.

3. **Column Pruning**

   * Only select necessary columns from both tables.

4. **Partitioning Check (Optional)**

   * If the 1B row table is partitioned on the join key, Trino can leverage it to avoid full scan.

5. **Use Materialized View (Optional)**

   * If this join is repeated frequently, precompute results.

✅ **Key Takeaways:**

* **Pushdown + Broadcast Join** is the go-to for large-small table joins.
* **Partitioning, salting, and filters** help mitigate skew and reduce data movement.
* Always monitor query stages/tasks to identify bottlenecks.

---

