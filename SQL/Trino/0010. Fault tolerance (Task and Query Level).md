## ğŸ”¹ What is Fault Tolerance?

Fault tolerance is the systemâ€™s ability to **keep queries running despite failures** in the cluster (like worker crashes, network blips, or node preemptions).

Without fault tolerance â†’ a single worker failure could make the **entire query fail**.
With fault tolerance â†’ Trino can **retry parts of the query or even the entire query**, so you donâ€™t have to restart from scratch.

---

## ğŸ”¹ Why is Fault Tolerance Needed?

* In large clusters, failures are **normal, not exceptional**.
* Cloud platforms often use **spot/preemptible instances**, which can be terminated anytime.
* Without fault tolerance â†’ long queries may fail frequently.
* With fault tolerance â†’ Trino becomes **production-grade** for critical workloads.

---

## ğŸ”¹ Analogy ğŸ—ï¸

Imagine building a skyscraper:

* **No fault tolerance**: If one worker drops a tool, the whole project stops and restarts.
* **Task-level fault tolerance**: Another worker just picks up the dropped tool and continues.
* **Query-level fault tolerance**: If the whole construction site shuts down temporarily, you donâ€™t start from scratchâ€”you continue from the last completed floor.

---

# ğŸ”¹ 1. **Task-Level Retry**

* A **query** in Trino is broken into **stages**, and each stage has multiple **tasks**.
* If a **task fails** (e.g., node crash, network timeout, JVM OOM), Trino can **retry that task** on another worker.
* **How it works:**

  * Input splits assigned to the failed task are rescheduled to a different worker.
  * Since splits are stateless units of work (e.g., reading a portion of a file), they can be retried safely.
* **Config knobs:**

  * `retry-policy=TASK` â†’ enables task-level retries.
  * `task.max-worker-threads` and `task.max-memory` â†’ prevent overload.

âœ… Benefit: Handles **transient failures** (node crashes, network blips) without killing the whole query.
âš ï¸ Limitation: If the same task keeps failing (bad data, corrupted file), retries wonâ€™t help â€” query fails.

---

# ğŸ”¹ 2. **Query-Level Retry**

* If a **whole query fails** due to infrastructure or coordinator issues, Trino can **retry the entire query**.
* This is especially useful in **fault-tolerant execution mode** (introduced in newer versions).
* Instead of redoing all work from scratch, Trino uses **intermediate result persistence**:

  * Intermediate stage outputs are written to **object storage (e.g., S3, GCS, ADLS)**.
  * On retry, Trino **reuses persisted results** for completed stages instead of recomputing.
* **Config knobs:**

  * `retry-policy=QUERY` â†’ enables query retries.
  * `fault-tolerant-execution-enabled=true` â†’ turns on checkpointing of stage results.
  * `query.max-retries` â†’ number of retries allowed.

âœ… Benefit: Query can survive coordinator failover or worker loss without redoing all the work.
âš ï¸ Limitation: Overhead of **writing intermediate data** to cloud storage (extra latency, storage cost).

---

# ğŸ”¹ Example Flow

**Scenario:** You run a query joining 2 TB of Parquet files.

1. Worker A fails while processing a file split â†’ **task-level retry** moves that split to Worker B.
2. Midway through execution, coordinator crashes â†’ **query-level retry** restarts query but reuses already persisted stage outputs.

---

# ğŸ”¹ When to Use What?

* **Task-level retry**: Default, lightweight, best for most queries.
* **Query-level retry**: Use for **long-running or mission-critical queries** in production (especially on the cloud, where preemptions happen).

---

ğŸ‘‰ In short:

* **Task retry = reschedule failed work unit (split) on another worker.**
* **Query retry = restart whole query, optionally reusing intermediate results.**
