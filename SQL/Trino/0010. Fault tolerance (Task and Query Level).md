## 🔹 What is Fault Tolerance?

Fault tolerance is the system’s ability to **keep queries running despite failures** in the cluster (like worker crashes, network blips, or node preemptions).

Without fault tolerance → a single worker failure could make the **entire query fail**.
With fault tolerance → Trino can **retry parts of the query or even the entire query**, so you don’t have to restart from scratch.

---

## 🔹 Why is Fault Tolerance Needed?

* In large clusters, failures are **normal, not exceptional**.
* Cloud platforms often use **spot/preemptible instances**, which can be terminated anytime.
* Without fault tolerance → long queries may fail frequently.
* With fault tolerance → Trino becomes **production-grade** for critical workloads.

---

## 🔹 Analogy 🏗️

Imagine building a skyscraper:

* **No fault tolerance**: If one worker drops a tool, the whole project stops and restarts.
* **Task-level fault tolerance**: Another worker just picks up the dropped tool and continues.
* **Query-level fault tolerance**: If the whole construction site shuts down temporarily, you don’t start from scratch—you continue from the last completed floor.

---

# 🔹 1. **Task-Level Retry**

* A **query** in Trino is broken into **stages**, and each stage has multiple **tasks**.
* If a **task fails** (e.g., node crash, network timeout, JVM OOM), Trino can **retry that task** on another worker.
* **How it works:**

  * Input splits assigned to the failed task are rescheduled to a different worker.
  * Since splits are stateless units of work (e.g., reading a portion of a file), they can be retried safely.
* **Config knobs:**

  * `retry-policy=TASK` → enables task-level retries.
  * `task.max-worker-threads` and `task.max-memory` → prevent overload.

✅ Benefit: Handles **transient failures** (node crashes, network blips) without killing the whole query.
⚠️ Limitation: If the same task keeps failing (bad data, corrupted file), retries won’t help — query fails.

---

# 🔹 2. **Query-Level Retry**

* If a **whole query fails** due to infrastructure or coordinator issues, Trino can **retry the entire query**.
* This is especially useful in **fault-tolerant execution mode** (introduced in newer versions).
* Instead of redoing all work from scratch, Trino uses **intermediate result persistence**:

  * Intermediate stage outputs are written to **object storage (e.g., S3, GCS, ADLS)**.
  * On retry, Trino **reuses persisted results** for completed stages instead of recomputing.
* **Config knobs:**

  * `retry-policy=QUERY` → enables query retries.
  * `fault-tolerant-execution-enabled=true` → turns on checkpointing of stage results.
  * `query.max-retries` → number of retries allowed.

✅ Benefit: Query can survive coordinator failover or worker loss without redoing all the work.
⚠️ Limitation: Overhead of **writing intermediate data** to cloud storage (extra latency, storage cost).

---

# 🔹 Example Flow

**Scenario:** You run a query joining 2 TB of Parquet files.

1. Worker A fails while processing a file split → **task-level retry** moves that split to Worker B.
2. Midway through execution, coordinator crashes → **query-level retry** restarts query but reuses already persisted stage outputs.

---

# 🔹 When to Use What?

* **Task-level retry**: Default, lightweight, best for most queries.
* **Query-level retry**: Use for **long-running or mission-critical queries** in production (especially on the cloud, where preemptions happen).

---

👉 In short:

* **Task retry = reschedule failed work unit (split) on another worker.**
* **Query retry = restart whole query, optionally reusing intermediate results.**
