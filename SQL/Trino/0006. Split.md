# ğŸ”¹ What is a Split in Trino?

Think of a **split** as the **smallest unit of work** that Trino assigns to a worker.

* When Trino reads data (from Hive, Iceberg, JDBC, etc.), it **divides the dataset into chunks**.
* Each chunk = **split**.
* Splits are then processed in parallel across workers.

ğŸ“Œ Example:

* You query a **1 TB Parquet dataset** in S3.
* Trino divides it into **splits of 256 MB each**.
* Result = \~4000 splits.
* If you have 20 worker nodes, splits are distributed so all workers process them in parallel.

ğŸ‘‰ More splits = more parallelism.
ğŸ‘‰ Too many small splits = overhead.
ğŸ‘‰ Too few large splits = underutilization.

---

# ğŸ”¹ Why are Splits Important?

1. **Parallelism** â†’ More splits â†’ better distribution of workload.
2. **Cluster utilization** â†’ Keeps all worker threads busy.
3. **Performance tuning** â†’ Right split size avoids bottlenecks (e.g., too many tasks vs. idle CPUs).

---

# ğŸ”¹ How to Configure Splits

Configuration depends on the **connector**.
Letâ€™s focus on the most common case: **Hive / Iceberg / Delta Lake**.

### Key configs in `hive.properties` (or `iceberg.properties`):

1. **Split Size**

   * `hive.max-split-size` â†’ Default: **64MB**.
   * Increase to **256MBâ€“512MB** for large analytical queries (reduces task scheduling overhead).
   * Keep smaller (64MBâ€“128MB) for workloads with **high concurrency** or lots of small queries.

   Example:

   ```properties
   hive.max-split-size=256MB
   ```

---

2. **Open Splits per Node**

   * `hive.max-open-splits` â†’ Max splits being processed at once per node (default 1000).
   * If you have large datasets, you may need to **increase this** to keep workers busy.

   Example:

   ```properties
   hive.max-open-splits=2000
   ```

---

3. **Outstanding Splits per Node**

   * `hive.max-outstanding-splits` â†’ Queue size of pending splits per node (default 1000).
   * Prevents overwhelming a worker with too many scheduled tasks.

   Example:

   ```properties
   hive.max-outstanding-splits=3000
   ```

---

4. **Split Loader Parallelism**

   * `hive.split-loader-concurrency` â†’ Controls how many splits are loaded in parallel.
   * Useful for large partitioned datasets.

---

# ğŸ”¹ Choosing Split Size (Rule of Thumb)

âœ… **Small datasets / interactive BI queries** â†’ 64MBâ€“128MB
âœ… **Large batch queries (ETL, reporting)** â†’ 256MBâ€“512MB
âœ… **Very large files (hundreds of GB)** â†’ Increase split size to avoid too many tasks

---

# ğŸ”¹ Analogy: Splits = Pizza Slices ğŸ•

* Dataset = Whole Pizza ğŸ•
* Splits = Pizza slices ğŸ•ğŸ•ğŸ•
* If you cut pizza into **too many tiny slices** â†’ overhead in serving, people wait longer.
* If you cut into **too few giant slices** â†’ some people (workers) stay idle while others struggle with a huge slice.
* The sweet spot = balanced slices so everyone eats at the same time.

---

# ğŸ”¹ Example in Action

Suppose you query a **2 TB Hive table stored in S3 (Parquet)**:

* If `hive.max-split-size=64MB` â†’ \~32,000 splits â†’ high scheduling overhead.
* If `hive.max-split-size=256MB` â†’ \~8,000 splits â†’ fewer tasks, better performance.
* If `hive.max-split-size=1GB` â†’ \~2,000 splits â†’ may underutilize cluster if you have many workers.

---

âœ… **In summary**:

* A **split = smallest work unit in Trino**.
* **Tune split size** to balance between **parallelism** and **task overhead**.
* Start with **256MB splits** for large data and adjust based on cluster size & workload.
