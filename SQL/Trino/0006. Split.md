# 🔹 What is a Split in Trino?

Think of a **split** as the **smallest unit of work** that Trino assigns to a worker.

* When Trino reads data (from Hive, Iceberg, JDBC, etc.), it **divides the dataset into chunks**.
* Each chunk = **split**.
* Splits are then processed in parallel across workers.

📌 Example:

* You query a **1 TB Parquet dataset** in S3.
* Trino divides it into **splits of 256 MB each**.
* Result = \~4000 splits.
* If you have 20 worker nodes, splits are distributed so all workers process them in parallel.

👉 More splits = more parallelism.
👉 Too many small splits = overhead.
👉 Too few large splits = underutilization.

---

# 🔹 Why are Splits Important?

1. **Parallelism** → More splits → better distribution of workload.
2. **Cluster utilization** → Keeps all worker threads busy.
3. **Performance tuning** → Right split size avoids bottlenecks (e.g., too many tasks vs. idle CPUs).

---

# 🔹 How to Configure Splits

Configuration depends on the **connector**.
Let’s focus on the most common case: **Hive / Iceberg / Delta Lake**.

### Key configs in `hive.properties` (or `iceberg.properties`):

1. **Split Size**

   * `hive.max-split-size` → Default: **64MB**.
   * Increase to **256MB–512MB** for large analytical queries (reduces task scheduling overhead).
   * Keep smaller (64MB–128MB) for workloads with **high concurrency** or lots of small queries.

   Example:

   ```properties
   hive.max-split-size=256MB
   ```

---

2. **Open Splits per Node**

   * `hive.max-open-splits` → Max splits being processed at once per node (default 1000).
   * If you have large datasets, you may need to **increase this** to keep workers busy.

   Example:

   ```properties
   hive.max-open-splits=2000
   ```

---

3. **Outstanding Splits per Node**

   * `hive.max-outstanding-splits` → Queue size of pending splits per node (default 1000).
   * Prevents overwhelming a worker with too many scheduled tasks.

   Example:

   ```properties
   hive.max-outstanding-splits=3000
   ```

---

4. **Split Loader Parallelism**

   * `hive.split-loader-concurrency` → Controls how many splits are loaded in parallel.
   * Useful for large partitioned datasets.

---

# 🔹 Choosing Split Size (Rule of Thumb)

✅ **Small datasets / interactive BI queries** → 64MB–128MB
✅ **Large batch queries (ETL, reporting)** → 256MB–512MB
✅ **Very large files (hundreds of GB)** → Increase split size to avoid too many tasks

---

# 🔹 Analogy: Splits = Pizza Slices 🍕

* Dataset = Whole Pizza 🍕
* Splits = Pizza slices 🍕🍕🍕
* If you cut pizza into **too many tiny slices** → overhead in serving, people wait longer.
* If you cut into **too few giant slices** → some people (workers) stay idle while others struggle with a huge slice.
* The sweet spot = balanced slices so everyone eats at the same time.

---

# 🔹 Example in Action

Suppose you query a **2 TB Hive table stored in S3 (Parquet)**:

* If `hive.max-split-size=64MB` → \~32,000 splits → high scheduling overhead.
* If `hive.max-split-size=256MB` → \~8,000 splits → fewer tasks, better performance.
* If `hive.max-split-size=1GB` → \~2,000 splits → may underutilize cluster if you have many workers.

---

✅ **In summary**:

* A **split = smallest work unit in Trino**.
* **Tune split size** to balance between **parallelism** and **task overhead**.
* Start with **256MB splits** for large data and adjust based on cluster size & workload.
