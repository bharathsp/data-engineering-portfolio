# ðŸ”¹ What are Query Queues in Trino?

* Trino can run **many queries concurrently**, but if too many heavy queries start at once, it can **exhaust memory/CPU** and bring down the cluster.
* To prevent this, Trino uses **resource groups and query queues**.
* A **queue** ensures:

  * Only a certain number of queries run at once.
  * Other queries wait in line until resources are free.
  * Fairness and priority can be enforced (e.g., BI dashboards before ad-hoc queries).

ðŸ‘‰ This is managed through the **resource group manager** (`resource-groups.properties` or JSON configs).

---

# ðŸ”¹ How Queues Work

1. User submits a query.
2. Trino checks the **resource group rules** (based on user, source, query type, etc.).
3. Query is assigned to a **resource group** (which is effectively a queue).
4. If the group has available slots â†’ query starts immediately.
5. If not â†’ query waits until another finishes.

---

# ðŸ”¹ Example Config â€“ File-based Resource Groups

ðŸ“Œ `resource-groups.properties`

```properties
resource-groups.configuration-manager=file
resource-groups.config-file=/etc/trino/resource-groups.json
```

ðŸ“Œ `resource-groups.json`

```json
{
  "rootGroups": [
    {
      "name": "global",
      "softMemoryLimit": "80%",
      "hardConcurrencyLimit": 100,
      "schedulingPolicy": "fair",
      "subGroups": [
        {
          "name": "bi",
          "softMemoryLimit": "30%",
          "hardConcurrencyLimit": 20,
          "schedulingPolicy": "fair",
          "schedulingWeight": 2
        },
        {
          "name": "etl",
          "softMemoryLimit": "40%",
          "hardConcurrencyLimit": 10,
          "schedulingPolicy": "weighted_fair",
          "schedulingWeight": 1
        },
        {
          "name": "adhoc",
          "softMemoryLimit": "10%",
          "hardConcurrencyLimit": 5,
          "schedulingPolicy": "weighted_fair",
          "schedulingWeight": 1
        }
      ]
    }
  ],
  "selectors": [
    {
      "user": "bi_user",
      "group": "global.bi"
    },
    {
      "source": "scheduler",
      "group": "global.etl"
    },
    {
      "source": "cli",
      "group": "global.adhoc"
    }
  ]
}
```

### Explanation:

* `rootGroups`: Define global concurrency & memory (cluster-wide).
* `subGroups`: Create queues for BI, ETL, and ad-hoc queries.
* `hardConcurrencyLimit`: Max number of queries running at once in that queue.
* `softMemoryLimit`: Memory allocation target for that queue.
* `selectors`: Rules to assign queries â†’ BI users go to `bi` queue, CLI queries go to `adhoc`, etc.

---

# ðŸ”¹ Scheduling Policies

* **fair** â†’ equal priority among queries in that group.
* **weighted\_fair** â†’ allocate based on `schedulingWeight`.

  * Example: If BI has weight 2 and Adhoc has weight 1, BI gets \~2x more slots.
* **query\_priority** â†’ based on explicit query priority values.

---

# ðŸ”¹ Monitoring Queues

You can monitor query queues:

* **Trino UI** â†’ shows queued vs running queries.
* **JMX metrics** â†’ `trino.execution.executor.queued-queries`
* **Logs** â†’ queries in `QUEUED` state will mention waiting for a resource group.

---

# ðŸ”¹ Why Use Queues?

âœ… Prevents cluster overload.
âœ… Guarantees **fairness** (BI dashboards wonâ€™t starve because of one massive ad-hoc query).
âœ… Improves **predictability** in multi-tenant environments.
âœ… Lets you enforce **priorities & SLAs**.

---

âš¡ In short:
Trino query queues = **controlled concurrency via resource groups**. They let you define how many queries run, how much memory they get, and which users/sources get priority.
