# ⚙️ **Trino Configuration Tuning**

Trino has three main configuration files on the **coordinator** and **workers**:

1. `config.properties` → General Trino configs (per node).
2. `jvm.config` → JVM-level tuning.
3. `catalog/*.properties` → Connector configs (Hive, Iceberg, MySQL, etc.).

---

## 🔹 1. Coordinator Config Tuning (`config.properties`)

These impact query scheduling, memory, and cluster management.

### Key Parameters:

* `query.max-memory` → Total memory a query can use cluster-wide.
* `query.max-memory-per-node` → Memory per query per worker.
* `query.max-total-memory-per-node` → Hard cap on per-node query memory (including system overhead).
* `query.queue-config-file` → Controls query queues, priorities.
* `query.max-execution-time` → Timeout for long queries.
* `query.low-memory-killer.policy=total-reservation` → Kills largest queries if memory runs out (default).

👉 **Tuning tip**:

* Set `query.max-memory` to \~50–60% of cluster memory.
* Balance `query.max-memory-per-node` vs. `query.max-total-memory-per-node` to avoid a single query hogging a node.

---

## 🔹 2. Worker Config Tuning (`config.properties`)

Workers handle most of the heavy lifting.

* `task.concurrency` → Number of concurrent splits (default 16).

  * Higher = better parallelism, but increases memory pressure.
* `task.max-worker-threads` → Max worker threads. Default = cores.
* `sink.max-buffer-size` → Buffer for output before flushing to disk (default 32MB).
* `exchange.client-threads` → Threads for data exchange between workers.

👉 **Tuning tip**:

* Start with `task.concurrency = 16`, increase to 32 if queries underutilize CPUs.
* Keep `task.max-worker-threads` = #cores.

---

## 🔹 3. JVM Tuning (`jvm.config`)

Trino is **memory intensive**, so JVM configs are critical.

* `-Xmx` → Max heap size. (Set to \~80% of node memory).
* `-XX:+UseG1GC` → Garbage collector (default in Trino).
* `-XX:+UseGCOverheadLimit` → Prevents excessive GC.
* `-XX:+ExitOnOutOfMemoryError` → Restart on OOM.

👉 **Tuning tip**:

* Use **G1GC** (default).
* Avoid too small heaps → will cause frequent GC pauses.
* Example:

  ```bash
  -Xmx64G
  -XX:+UseG1GC
  -XX:+ExitOnOutOfMemoryError
  ```

---

## 🔹 4. Connector-Level Tuning (`catalog/*.properties`)

### Hive/Iceberg:

* `hive.max-split-size` → Default 64MB. Increase to 256MB–512MB to reduce number of splits.
* `hive.max-outstanding-splits` → Pending splits per node (default 1000). Increase for large datasets.
* `hive.parallel-partitioned-bucketed-inserts` → Enables parallel writes.

### JDBC (MySQL, Postgres):

* `connection-pool-size` → Increase if running many parallel queries.
* `metadata-cache-ttl` → Cache metadata to reduce DB hits.

---

## 🔹 5. Query Queues (Optional but Important)

Define queues to prevent large queries from starving smaller ones. Example `resource-groups.json`:

```json
{
  "rootGroups": [
    {
      "name": "global",
      "softMemoryLimit": "80%",
      "hardConcurrencyLimit": 100,
      "subGroups": [
        {
          "name": "etl",
          "softMemoryLimit": "60%",
          "hardConcurrencyLimit": 20,
          "schedulingPolicy": "fair"
        },
        {
          "name": "adhoc",
          "softMemoryLimit": "20%",
          "hardConcurrencyLimit": 10,
          "schedulingPolicy": "fair"
        }
      ]
    }
  ]
}
```

👉 Separates **ETL workloads** from **ad-hoc queries**.

---

## 🔹 6. Other Important Tuning Areas

* **Spill to Disk**: Enable if queries exceed memory.

  ```
  spill-enabled=true
  spiller-spill-path=/mnt/disks/trino_spill
  spiller-max-used-space-threshold=0.9
  ```
* **Fault Tolerance** (for retries):

  ```
  retry-policy=query
  ```

---

# ✅ Best Practices for Trino Tuning

1. **Memory-aware tuning** → Balance `query.max-memory` and heap size.
2. **Bigger splits** for large datasets (reduce overhead).
3. **Use ORC/Parquet** instead of row-based formats (better I/O).
4. **Enable Spill-to-Disk** if memory is a bottleneck.
5. **Set up query queues** to isolate ETL and BI workloads.
6. **Monitor with Prometheus/Grafana** to see real bottlenecks.
