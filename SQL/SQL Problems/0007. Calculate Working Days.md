You are given:

#### `Range_table`:

| Group\_id | Start\_Date | End\_Date  |
| --------- | ----------- | ---------- |
| 1         | 20-06-2024  | 30-06-2024 |
| 2         | 13-08-2025  | 24-08-2025 |

#### `Holiday_table`:

| Holiday\_Reason | Date       |
| --------------- | ---------- |
| May Day         | 01-05-2024 |

---

### âœ… **Goal:**

For each `Group_id`, calculate the number of **working days** (Monâ€“Fri) between `Start_Date` and `End_Date`, excluding:

* Saturdays and Sundays
* Dates in the `Holiday_table`

---

### âœ… **SQL Server Solution:**

```sql
-- Step 1: Create a recursive CTE to generate dates for each group
WITH Date_CTE AS (
    SELECT 
        Group_id,
        CAST(Start_Date AS DATE) AS CurrentDate,
        CAST(End_Date AS DATE) AS EndDate
    FROM Range_table

    UNION ALL

    SELECT 
        Group_id,
        DATEADD(DAY, 1, CurrentDate),
        EndDate
    FROM Date_CTE
    WHERE DATEADD(DAY, 1, CurrentDate) <= EndDate
)

-- Step 2: Exclude weekends and holidays, then count working days
SELECT 
    Group_id,
    COUNT(*) AS Working_Days
FROM Date_CTE
WHERE 
    DATEPART(WEEKDAY, CurrentDate) NOT IN (1, 7) -- 1 = Sunday, 7 = Saturday (depends on @@DATEFIRST setting)
    AND CurrentDate NOT IN (SELECT CAST([Date] AS DATE) FROM Holiday_table)
GROUP BY Group_id
ORDER BY Group_id
OPTION (MAXRECURSION 1000); -- Extend recursion limit if needed
```

---

### ðŸ“ Notes:

* `DATEPART(WEEKDAY, CurrentDate)` excludes Sunday (1) and Saturday (7); adjust if your server uses different `@@DATEFIRST`.
* `OPTION (MAXRECURSION 1000)` is important to avoid hitting the default limit of 100 rows. You can increase it based on the maximum date range.
* This assumes that `Range_table.Start_Date`, `Range_table.End_Date`, and `Holiday_table.Date` are all valid `DATE` or `DATETIME` types.

---

### âœ… **PySpark Solution:**
---

### âœ… **Step-by-step Objective:**

* Generate all dates between `Start_Date` and `End_Date`
* Exclude weekends (Saturday, Sunday)
* Exclude public holidays from a separate `Holiday_table`

---

### âœ… **Sample Data**

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, explode, sequence, to_date, dayofweek

spark = SparkSession.builder.getOrCreate()

# Range_table with start and end dates
range_data = [
    (1, "2024-06-20", "2024-06-30"),
    (2, "2025-08-13", "2025-08-24")
]

range_df = spark.createDataFrame(range_data, ["Group_id", "Start_Date", "End_Date"]) \
    .withColumn("Start_Date", to_date("Start_Date")) \
    .withColumn("End_Date", to_date("End_Date"))

# Holiday_table
holiday_data = [
    ("May Day", "2024-05-01"),
    ("Independence Day", "2025-08-15")
]

holiday_df = spark.createDataFrame(holiday_data, ["Holiday_Reason", "Date"]) \
    .withColumn("Date", to_date("Date"))
```

---

### âœ… **Generate all dates between Start and End**

```python
from pyspark.sql.functions import dayofweek

# Step 1: Create a row per date using sequence + explode
date_exploded_df = range_df.withColumn(
    "Date",
    explode(sequence(col("Start_Date"), col("End_Date")))
).select("Group_id", "Date")
```

---

### âœ… **Filter Out Weekends (Saturday=7, Sunday=1)**

```python
weekday_df = date_exploded_df.filter(
    ~dayofweek("Date").isin(1, 7)  # Remove Sundays (1) and Saturdays (7)
)
```

---

### âœ… **Exclude Public Holidays**

```python
working_days_df = weekday_df.join(
    holiday_df,
    on="Date",
    how="left_anti"  # Keep only those dates not in holiday table
)
```

---

### âœ… **Count Working Days per Group**

```python
result_df = working_days_df.groupBy("Group_id").count().withColumnRenamed("count", "Working_Days")
result_df.show()
```

---

### âœ… **Expected Output:**

| Group\_id | Working\_Days |
| --------- | ------------- |
| 1         | 7             |
| 2         | 9             |

*(Assuming applicable weekdays and holiday on 2025-08-15 are excluded)*

---

### ðŸ” Summary:

* `sequence()` generates the range of dates
* `explode()` flattens date arrays
* `dayofweek()` removes weekends
* `left_anti` excludes holidays
