### SQL or PySpark â€“ Cumulative Sum of Sales

---

### ðŸ§¾ **Input Table:**

| Date       | Sales |
| ---------- | ----- |
| 2023-05-01 | 100   |
| 2023-05-02 | 200   |
| 2023-05-03 | 150   |

---

### ðŸŽ¯ **Goal:**

Add a `CumulativeSales` column which is the running total of `Sales` ordered by `Date`.

---

## âœ… **SQL Solution (Using `SUM() OVER()`):**

```sql
SELECT 
  Date, 
  Sales,
  SUM(Sales) OVER (ORDER BY Date) AS CumulativeSales
FROM sales_table;
```

---

## âœ… **PySpark Solution (Using `Window`):**

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import sum as _sum
from pyspark.sql.window import Window

# Sample data
data = [("2023-05-01", 100), ("2023-05-02", 200), ("2023-05-03", 150)]
columns = ["Date", "Sales"]

# Create SparkSession and DataFrame
spark = SparkSession.builder.appName("CumulativeSum").getOrCreate()
df = spark.createDataFrame(data, columns)

# Define window
windowSpec = Window.orderBy("Date")

# Add cumulative sum column
result_df = df.withColumn("CumulativeSales", _sum("Sales").over(windowSpec))

result_df.show()
```

---

### ðŸ“¤ **Output:**

| Date       | Sales | CumulativeSales |
| ---------- | ----- | --------------- |
| 2023-05-01 | 100   | 100             |
| 2023-05-02 | 200   | 300             |
| 2023-05-03 | 150   | 450             |
