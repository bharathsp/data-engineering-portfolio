### 🧠 Dynamic DAGs & Task Mapping in Apache Airflow

Apache Airflow supports **dynamic DAG generation** and **task mapping** to help you **scale workflows** and avoid repetitive code. These are two powerful features for managing large or variable datasets.

---

## ⚙️ 1. **Dynamic DAGs** (DAG Factory Pattern)

Dynamic DAGs are created **programmatically using loops or external inputs**.

### ✅ Use Case:

* You want to create **multiple DAGs** from a list of clients, tables, or configs.

### 🧾 Example:

```python
from airflow import DAG
from airflow.operators.dummy import DummyOperator
from datetime import datetime

clients = ['client_a', 'client_b', 'client_c']

for client in clients:
    with DAG(
        dag_id=f"process_data_{client}",
        start_date=datetime(2025, 7, 1),
        schedule_interval='@daily',
        catchup=False,
        tags=['dynamic']
    ) as dag:

        start = DummyOperator(task_id='start')
        end = DummyOperator(task_id='end')

        start >> end

        globals()[dag.dag_id] = dag  # Register the DAG
```

📝 This creates 3 separate DAGs: `process_data_client_a`, `process_data_client_b`, etc.

---

## 🧩 2. **Task Mapping** (Airflow 2.3+ Feature)

Task Mapping allows you to **dynamically create multiple tasks at runtime** from a list (or other iterable) **without hardcoding them**.

### ✅ Use Case:

* You want one task to **run multiple times in parallel**, once for each input (like files, table names, API payloads, etc.)

---

### 📦 Basic Example:

```python
from airflow.decorators import dag, task
from datetime import datetime

@dag(start_date=datetime(2025, 7, 1), schedule_interval="@daily", catchup=False)
def mapped_example_dag():

    @task
    def generate_inputs():
        return ['foo', 'bar', 'baz']

    @task
    def process(item):
        print(f"Processing: {item}")

    inputs = generate_inputs()
    process.expand(item=inputs)

mapped_example_dag()
```

This dynamically creates 3 task instances for `process`:

* `process[0]` with `foo`
* `process[1]` with `bar`
* `process[2]` with `baz`

---

## 🔁 More Advanced Mapping Examples

### 📁 Map over a list of dicts:

```python
@task
def process_config(cfg):
    print(cfg['source'], cfg['target'])

configs = [
    {"source": "s3://bucket1", "target": "db1"},
    {"source": "s3://bucket2", "target": "db2"}
]

process_config.expand(cfg=configs)
```

---

## 📌 Differences Between Dynamic DAGs and Task Mapping

| Feature         | Dynamic DAGs        | Task Mapping                                   |
| --------------- | ------------------- | ---------------------------------------------- |
| Scope           | Creates entire DAGs | Creates parallel task instances within one DAG |
| Use case        | Different pipelines | Same logic for multiple inputs                 |
| When generated  | At DAG parse time   | At DAG run time                                |
| Recommended for | Multiple pipelines  | Processing variable data (e.g. list of files)  |

---

## 🧠 Best Practices

* Use **dynamic DAGs** sparingly — too many DAGs can overload the scheduler.
* Use **task mapping** when the number of inputs varies **per DAG run**.
* Prefer **`expand()`** for clean, scalable task creation.
