## **What is Teradata?** 🗄️

<img width="200" height="64" alt="image" src="https://github.com/user-attachments/assets/04c2682e-44dd-4828-a1fe-ea4ca3888bde" />

* **Teradata** is a **Massively Parallel Processing (MPP) relational database system** designed for **data warehousing and analytics**.
* It’s used by enterprises to store and process **huge volumes of structured data** (think terabytes to petabytes).
* It can run **complex analytical queries very fast** because it distributes data and workload across multiple parallel processing nodes.

**Key Features:**

| Icon | Feature                   | Why It’s Important                                       |
| ---- | ------------------------- | -------------------------------------------------------- |
| ⚡    | **High Performance**      | Handles big queries efficiently via parallelism.         |
| 📊   | **Scalability**           | Can grow from small systems to huge enterprise clusters. |
| 🔄   | **Data Warehousing**      | Stores historical + current business data for analytics. |
| 🔐   | **Security & Governance** | Role-based access, auditing, encryption.                 |

---

## **What are Teradata Tables?** 📋

In Teradata, **tables** are where data is stored, just like in any other RDBMS (e.g., PostgreSQL, Oracle), but **they’re optimized for parallel processing**.
Different table types exist for different storage/usage needs.

---

### **Main Teradata Table Types**

| Table Type                       | Icon | Description                                                             | Use Case                                                  |
| -------------------------------- | ---- | ----------------------------------------------------------------------- | --------------------------------------------------------- |
| **Permanent Table**              | 📦   | Stores persistent data in database until explicitly deleted.            | Main business data storage.                               |
| **Volatile Table**               | ⚡    | Temporary table stored in session memory; disappears when session ends. | Staging data for transformations in a session.            |
| **Global Temporary Table (GTT)** | 🌐   | Defined once and can be reused; data is temporary per session.          | Multiple sessions need same structure but different data. |
| **Derived Table**                | 🧮   | Exists only in a query; not physically stored.                          | Intermediate result sets in SQL queries.                  |
| **NoPI Table**                   | 🚫📍 | Table without a Primary Index; rows are randomly distributed.           | Fast loading of raw data without worrying about skew.     |
| **Columnar Table**               | 📊➡  | Stores data column-wise instead of row-wise.                            | Analytical queries reading few columns.                   |

---

### **Example – Creating a Permanent Table**

```sql
CREATE TABLE Customer (
    CustomerID INTEGER,
    Name VARCHAR(100),
    Email VARCHAR(100),
    PRIMARY KEY (CustomerID)
);
```

---

## **Where Teradata Fits in Data Engineering** ⚙️

```
📂 Data Sources (ERP, CRM, IoT)
       ⬇
🚚 ETL/ELT Pipelines
       ⬇
🗄️ Teradata Tables (Permanent / Temp)
       ⬇
📊 BI Tools (Tableau, Power BI) / ML Models
```

---

# Connect a **Teradata table** with a **Cloudera cluster**

To connect a **Teradata table** with a **Cloudera cluster**, the idea is to **move data between Teradata (RDBMS)** and **HDFS (Hadoop Distributed File System)** or directly into Hive/Spark on Cloudera.

---

## **1. Understand the Goal** 🎯

* **Source** → Teradata database table (structured data).
* **Target** → HDFS in Cloudera (for analytics, Spark/Hive processing).
* **Connection Layer** → A connector or ingestion tool (Sqoop, NiFi, etc.).

---

## **2. Common Integration Methods**

| Method                 | Best For                                            | Tools                               |
| ---------------------- | --------------------------------------------------- | ----------------------------------- |
| **Apache Sqoop**       | Bulk imports/exports between Teradata and HDFS/Hive | Sqoop + Teradata JDBC driver        |
| **Apache NiFi**        | Continuous or scheduled ingestion                   | NiFi + QueryDatabaseTable processor |
| **JDBC in Spark**      | Direct queries from Spark jobs                      | Spark JDBC API + Teradata driver    |
| **Custom Python/Java** | Custom pipelines                                    | `teradatasql` Python package / JDBC |
| **ETL Tools**          | Enterprise-level ingestion                          | Informatica, Talend, DataStage      |

---

## **3. Example: Using Apache Sqoop (Most Common in Cloudera)**

### **Step 1: Download Teradata JDBC Driver**

* Go to: [Teradata JDBC Driver Download](https://downloads.teradata.com/download/connectivity/jdbc-driver)
* Place the `.jar` file in Sqoop’s `lib/` directory on the Cloudera node.

---

### **Step 2: Run Sqoop Import Command**

```bash
sqoop import \
  --connect jdbc:teradata://<TERADATA_HOST>/DATABASE=<DB_NAME> \
  --username myuser \
  --password mypass \
  --table customer_table \
  --target-dir /user/cloudera/teradata_data \
  --driver com.teradata.jdbc.TeraDriver \
  --num-mappers 4
```

**What this does:**

* Connects to Teradata using JDBC.
* Pulls the `customer_table` into HDFS (`/user/cloudera/teradata_data`).
* Uses parallel mappers for faster import.

---

## **4. Example: Using Apache NiFi (For Automated Flows)**

1. Install **Teradata JDBC driver** in NiFi’s `lib` folder.
2. Create a **DBCPConnectionPool** controller service in NiFi pointing to the Teradata database.
3. Use **QueryDatabaseTable** or **ExecuteSQL** processor to fetch data.
4. Send output to **PutHDFS** or **PutHiveStreaming** processor.

---

## **5. Example: Using Spark JDBC**

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("TeradataToHDFS").getOrCreate()

df = spark.read \
    .format("jdbc") \
    .option("url", "jdbc:teradata://<TERADATA_HOST>/DATABASE=<DB_NAME>") \
    .option("dbtable", "customer_table") \
    .option("user", "myuser") \
    .option("password", "mypass") \
    .option("driver", "com.teradata.jdbc.TeraDriver") \
    .load()

df.write.mode("overwrite").parquet("/user/cloudera/teradata_data")
```

---

## **6. Best Practices** 🛠

* Use **parallelism** (`--num-mappers` in Sqoop, partitioning in Spark) for large tables.
* Schedule jobs via **Oozie**, **Airflow**, or **NiFi** for automation.
* Ensure **Kerberos authentication** is handled if Cloudera is secure.
* For massive tables, consider **incremental imports** instead of full loads.
