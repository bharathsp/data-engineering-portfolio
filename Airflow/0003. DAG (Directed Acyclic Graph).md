## DAG (Directed Acyclic Graph)

A **DAG (Directed Acyclic Graph)** is a **graph structure** used to represent workflows or processes, where:

* **Directed**: Each edge (arrow) has a direction, indicating the flow (e.g., task A → task B).
* **Acyclic**: There are **no loops or cycles**, meaning a task cannot depend on itself—directly or indirectly.

---

### 🔁 **In simple terms:**

> A DAG is a **flowchart of tasks**, where each task must run **after certain others**, and the flow **never loops back**.

---

### 📦 **In Apache Airflow:**

A **DAG** defines the **structure of a workflow**—how tasks are organized and in what order they should run.

---

### 🧠 **Example DAG Flow:**

```text
Start
  |
Extract Data
  |
Transform Data
  |
Load to Database
  |
Send Notification
```

This flow can be represented as a DAG:

* `Extract → Transform → Load → Notify`
* Each arrow shows task **dependencies**
* No backward flow = **acyclic**

---

### ✅ **Key Properties:**

| Property     | Meaning                                                              |
| ------------ | -------------------------------------------------------------------- |
| **Directed** | Arrows point from one task to the next (shows dependency)            |
| **Acyclic**  | No cycles—ensures a task doesn’t run forever or cause infinite loops |
| **Graph**    | Represents nodes (tasks) and edges (dependencies)                    |

---

### 🛠️ **Airflow Code Example:**

```python
from airflow import DAG
from airflow.operators.dummy import DummyOperator
from datetime import datetime

with DAG("simple_dag", start_date=datetime(2023, 1, 1), schedule_interval="@daily") as dag:
    start = DummyOperator(task_id="start")
    task_a = DummyOperator(task_id="task_a")
    task_b = DummyOperator(task_id="task_b")
    end = DummyOperator(task_id="end")

    start >> [task_a, task_b] >> end
```

This DAG does:

```
         → task_a →
start →             → end
         → task_b →
```

---

### 🛠️ **DAG in Airflow UI:**

<img width="1919" height="1047" alt="image" src="https://github.com/user-attachments/assets/8a7b647f-c2b9-4b1c-81e0-2bbea824c159" />

### 🛠️ **Airflow DAG code for above:**

I’ll use **PythonOperator** for simplicity:

```python
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator

# ------------------------------
# Dummy task functions
# ------------------------------
def collect_data():
    print("Collecting raw data...")

def aggregate_data():
    print("Aggregating data...")

def transform_data():
    print("Transforming data...")

def store_data():
    print("Storing data to target system...")

# ------------------------------
# Default args
# ------------------------------
default_args = {
    "owner": "data_engineer",
    "depends_on_past": False,  # Can override in tasks
    "email_on_failure": False,
    "email_on_retry": False,
    "retries": 2,
    "retry_delay": timedelta(minutes=5),
}

# ------------------------------
# DAG Definition
# ------------------------------
with DAG(
    dag_id="etl_pipeline_dag",
    default_args=default_args,
    description="ETL pipeline with multiple tasks",
    schedule_interval=timedelta(days=1),
    start_date=datetime(2025, 9, 1),
    catchup=False,
    tags=["example", "etl"],
) as dag:

    # Task 1: Collect Data
    collect = PythonOperator(
        task_id="collect_data",
        python_callable=collect_data,
        depends_on_past=True,  # Will only run if previous run succeeded
        retries=3,
        retry_delay=timedelta(minutes=2),
        execution_timeout=timedelta(minutes=10),
    )

    # Task 2: Aggregate Data
    aggregate = PythonOperator(
        task_id="aggregate_data",
        python_callable=aggregate_data,
        depends_on_past=True,
        retries=2,
        retry_delay=timedelta(minutes=3),
        execution_timeout=timedelta(minutes=15),
    )

    # Task 3: Transform Data
    transform = PythonOperator(
        task_id="transform_data",
        python_callable=transform_data,
        depends_on_past=False,  # Can start fresh each day
        retries=1,
        retry_delay=timedelta(minutes=5),
        execution_timeout=timedelta(minutes=20),
    )

    # Task 4: Store Data
    store = PythonOperator(
        task_id="store_data",
        python_callable=store_data,
        depends_on_past=False,
        retries=2,
        retry_delay=timedelta(minutes=5),
        execution_timeout=timedelta(minutes=10),
    )

    # ------------------------------
    # Task Dependencies
    # ------------------------------
    collect >> aggregate >> transform >> store
```

---

### 🔑 Highlights:

* Each task has:

  * ✅ `task_id` → unique within DAG
  * ✅ `depends_on_past` → forces sequential daily dependencies
  * ✅ `retries` & `retry_delay` → retry policy per task
  * ✅ `execution_timeout` → fails if task runs too long
* DAG runs **daily** (`schedule_interval=timedelta(days=1)`)
* Task order: **collect → aggregate → transform → store**
