## DAG (Directed Acyclic Graph)

A **DAG (Directed Acyclic Graph)** is a **graph structure** used to represent workflows or processes, where:

* **Directed**: Each edge (arrow) has a direction, indicating the flow (e.g., task A â†’ task B).
* **Acyclic**: There are **no loops or cycles**, meaning a task cannot depend on itselfâ€”directly or indirectly.

---

### ğŸ” **In simple terms:**

> A DAG is a **flowchart of tasks**, where each task must run **after certain others**, and the flow **never loops back**.

---

### ğŸ“¦ **In Apache Airflow:**

A **DAG** defines the **structure of a workflow**â€”how tasks are organized and in what order they should run.

---

### ğŸ§  **Example DAG Flow:**

```text
Start
  |
Extract Data
  |
Transform Data
  |
Load to Database
  |
Send Notification
```

This flow can be represented as a DAG:

* `Extract â†’ Transform â†’ Load â†’ Notify`
* Each arrow shows task **dependencies**
* No backward flow = **acyclic**

---

### âœ… **Key Properties:**

| Property     | Meaning                                                              |
| ------------ | -------------------------------------------------------------------- |
| **Directed** | Arrows point from one task to the next (shows dependency)            |
| **Acyclic**  | No cyclesâ€”ensures a task doesnâ€™t run forever or cause infinite loops |
| **Graph**    | Represents nodes (tasks) and edges (dependencies)                    |

---

### ğŸ› ï¸ **Airflow Code Example:**

```python
from airflow import DAG
from airflow.operators.dummy import DummyOperator
from datetime import datetime

with DAG("simple_dag", start_date=datetime(2023, 1, 1), schedule_interval="@daily") as dag:
    start = DummyOperator(task_id="start")
    task_a = DummyOperator(task_id="task_a")
    task_b = DummyOperator(task_id="task_b")
    end = DummyOperator(task_id="end")

    start >> [task_a, task_b] >> end
```

This DAG does:

```
         â†’ task_a â†’
start â†’             â†’ end
         â†’ task_b â†’
```

---

### ğŸ› ï¸ **DAG in Airflow UI:**

<img width="1919" height="1047" alt="image" src="https://github.com/user-attachments/assets/8a7b647f-c2b9-4b1c-81e0-2bbea824c159" />

### ğŸ› ï¸ **Airflow DAG code for above:**

Iâ€™ll use **PythonOperator** for simplicity:

```python
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator

# ------------------------------
# Dummy task functions
# ------------------------------
def collect_data():
    print("Collecting raw data...")

def aggregate_data():
    print("Aggregating data...")

def transform_data():
    print("Transforming data...")

def store_data():
    print("Storing data to target system...")

# ------------------------------
# Default args
# ------------------------------
default_args = {
    "owner": "data_engineer",
    "depends_on_past": False,  # Can override in tasks
    "email_on_failure": False,
    "email_on_retry": False,
    "retries": 2,
    "retry_delay": timedelta(minutes=5),
}

# ------------------------------
# DAG Definition
# ------------------------------
with DAG(
    dag_id="etl_pipeline_dag",
    default_args=default_args,
    description="ETL pipeline with multiple tasks",
    schedule_interval=timedelta(days=1),
    start_date=datetime(2025, 9, 1),
    catchup=False,
    tags=["example", "etl"],
) as dag:

    # Task 1: Collect Data
    collect = PythonOperator(
        task_id="collect_data",
        python_callable=collect_data,
        depends_on_past=True,  # Will only run if previous run succeeded
        retries=3,
        retry_delay=timedelta(minutes=2),
        execution_timeout=timedelta(minutes=10),
    )

    # Task 2: Aggregate Data
    aggregate = PythonOperator(
        task_id="aggregate_data",
        python_callable=aggregate_data,
        depends_on_past=True,
        retries=2,
        retry_delay=timedelta(minutes=3),
        execution_timeout=timedelta(minutes=15),
    )

    # Task 3: Transform Data
    transform = PythonOperator(
        task_id="transform_data",
        python_callable=transform_data,
        depends_on_past=False,  # Can start fresh each day
        retries=1,
        retry_delay=timedelta(minutes=5),
        execution_timeout=timedelta(minutes=20),
    )

    # Task 4: Store Data
    store = PythonOperator(
        task_id="store_data",
        python_callable=store_data,
        depends_on_past=False,
        retries=2,
        retry_delay=timedelta(minutes=5),
        execution_timeout=timedelta(minutes=10),
    )

    # ------------------------------
    # Task Dependencies
    # ------------------------------
    collect >> aggregate >> transform >> store
```

---

### ğŸ”‘ Highlights:

* Each task has:

  * âœ… `task_id` â†’ unique within DAG
  * âœ… `depends_on_past` â†’ forces sequential daily dependencies
  * âœ… `retries` & `retry_delay` â†’ retry policy per task
  * âœ… `execution_timeout` â†’ fails if task runs too long
* DAG runs **daily** (`schedule_interval=timedelta(days=1)`)
* Task order: **collect â†’ aggregate â†’ transform â†’ store**
