## Hooks

In **Apache Airflow**, **Hooks** are **interfaces to external systems**. They abstract the logic for connecting to services like databases, cloud providers, APIs, and message queues.

---

### ðŸ§  **Simple Definition:**

> A **Hook** is a **Python class** in Airflow that helps you **connect to and interact with external systems**, such as MySQL, AWS S3, Google BigQuery, etc.

---

### ðŸ”— **Why Use Hooks?**

* Simplifies connection logic
* Reuses credentials stored in Airflow's **Connections**
* Avoids hardcoding usernames, passwords, tokens in your DAGs
* Ensures secure and maintainable integration

---

### ðŸ§© **Example Use Cases:**

| System          | Hook                  |
| --------------- | --------------------- |
| MySQL           | `MySqlHook`           |
| Postgres        | `PostgresHook`        |
| AWS S3          | `S3Hook`              |
| Google BigQuery | `BigQueryHook`        |
| REST APIs       | `HttpHook`            |
| FTP/SFTP        | `FtpHook`, `SFTPHook` |

---

### ðŸ“¦ **Using a Hook (Example with PostgresHook):**

```python
from airflow.hooks.postgres_hook import PostgresHook

def fetch_data_from_postgres():
    hook = PostgresHook(postgres_conn_id='my_postgres_conn')
    connection = hook.get_conn()
    cursor = connection.cursor()
    cursor.execute("SELECT * FROM users LIMIT 10")
    rows = cursor.fetchall()
    print(rows)
```

* `my_postgres_conn` is defined in **Airflow Connections UI**
* Hook handles authentication and connection reuse

---

### ðŸŽ¯ **Hooks vs Operators vs Tasks:**

| Component    | Role                                          |
| ------------ | --------------------------------------------- |
| **Hook**     | Connects to external systems (e.g., DB, API)  |
| **Operator** | Executes logic (often using hooks internally) |
| **Task**     | An instance of an operator within a DAG       |

> ðŸ” Operators often **call Hooks internally** to perform actions like running queries or uploading files.

---

### ðŸ“š **Custom Hooks:**

You can create your own hook by subclassing `BaseHook` if you need to connect to a custom API or service:

```python
from airflow.hooks.base import BaseHook

class MyCustomAPIHook(BaseHook):
    def __init__(self, conn_id):
        self.conn_id = conn_id
        self.conn = self.get_connection(conn_id)
    
    def call_api(self, endpoint):
        token = self.conn.password
        url = f"{self.conn.host}/{endpoint}"
        # logic to call the API using requests or httpx
```

---

### ðŸ“š **Using Hooks:**

Hereâ€™s a **sample DAG** that:

1. **Collects data** from an API (using `HttpHook`)
2. **Aggregates & Transforms** it (with Pandas)
3. **Stores** results in a Postgres table (using `PostgresHook`)

```python
from datetime import datetime, timedelta
import pandas as pd
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.http.hooks.http import HttpHook
from airflow.providers.postgres.hooks.postgres import PostgresHook

# ------------------------------
# Functions for tasks
# ------------------------------

def collect_data_from_api(**context):
    """Collect raw JSON data from API using HttpHook"""
    http_hook = HttpHook(method='GET', http_conn_id='my_api_conn')
    response = http_hook.run(endpoint='/data')
    data = response.json()
    context['ti'].xcom_push(key='raw_data', value=data)

def aggregate_data(**context):
    """Aggregate API data"""
    data = context['ti'].xcom_pull(key='raw_data', task_ids='collect_data')
    df = pd.DataFrame(data)
    agg = df.groupby("category").sum().reset_index()
    context['ti'].xcom_push(key='aggregated_data', value=agg.to_dict(orient="records"))

def transform_data(**context):
    """Transform aggregated data"""
    agg_data = context['ti'].xcom_pull(key='aggregated_data', task_ids='aggregate_data')
    df = pd.DataFrame(agg_data)
    df["processed_at"] = datetime.utcnow()
    context['ti'].xcom_push(key='transformed_data', value=df.to_dict(orient="records"))

def store_data_in_postgres(**context):
    """Store transformed data into Postgres using PostgresHook"""
    records = context['ti'].xcom_pull(key='transformed_data', task_ids='transform_data')
    df = pd.DataFrame(records)

    pg_hook = PostgresHook(postgres_conn_id='my_postgres_conn')
    engine = pg_hook.get_sqlalchemy_engine()

    # Store data (append mode)
    df.to_sql("etl_results", engine, if_exists="append", index=False)

# ------------------------------
# Default args
# ------------------------------
default_args = {
    "owner": "data_engineer",
    "depends_on_past": False,
    "retries": 2,
    "retry_delay": timedelta(minutes=5),
}

# ------------------------------
# DAG Definition
# ------------------------------
with DAG(
    dag_id="etl_pipeline_with_hooks",
    default_args=default_args,
    description="ETL pipeline using Airflow hooks",
    schedule_interval=timedelta(days=1),
    start_date=datetime(2025, 9, 1),
    catchup=False,
    tags=["etl", "hooks"],
) as dag:

    collect = PythonOperator(
        task_id="collect_data",
        python_callable=collect_data_from_api,
        retries=3,
        retry_delay=timedelta(minutes=2),
        execution_timeout=timedelta(minutes=10),
    )

    aggregate = PythonOperator(
        task_id="aggregate_data",
        python_callable=aggregate_data,
        depends_on_past=True,
        execution_timeout=timedelta(minutes=10),
    )

    transform = PythonOperator(
        task_id="transform_data",
        python_callable=transform_data,
        retries=1,
        execution_timeout=timedelta(minutes=15),
    )

    store = PythonOperator(
        task_id="store_data",
        python_callable=store_data_in_postgres,
        retries=2,
        retry_delay=timedelta(minutes=3),
        execution_timeout=timedelta(minutes=20),
    )

    collect >> aggregate >> transform >> store
```

### ðŸ”‘ Key Notes:

* **HttpHook**: connects to external APIs (needs a connection named `my_api_conn` in Airflow UI).
* **PostgresHook**: connects to Postgres (needs `my_postgres_conn` in Airflow).
* **XCom**: used to pass data between tasks.
* **Pandas**: used for aggregation & transformation.
