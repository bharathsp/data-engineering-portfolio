# ğŸ” `depends_on_past` in Apache Airflow

The **`depends_on_past`** parameter in Airflow is used to control whether a task should **wait for its own previous run to succeed** before running again.

---

### ğŸ§  **Simple Definition:**

> If `depends_on_past=True`, a task will **not run** for the current DAG run unless its **previous run was successful**.

---

### ğŸ“… **Use Case Example:**

Suppose your DAG runs **daily**, and you have a task `load_to_db`:

```python
PythonOperator(
    task_id='load_to_db',
    python_callable=load_data,
    depends_on_past=True
)
```

* âœ… On **July 10**, the task runs successfully.
* ğŸŸ¥ On **July 11**, it fails.
* ğŸ” On **July 12**, it **will not run** because **July 11 failed**.

---

### ğŸ§© **Why Use `depends_on_past`?**

| Scenario                                           | Benefit                                           |
| -------------------------------------------------- | ------------------------------------------------- |
| **Data pipelines** that must process days in order | Ensures previous data is complete                 |
| **Incremental loading**                            | Prevents loading Day 3 before Day 2 is successful |
| **Avoids data duplication**                        | Won't reprocess unless previous batch was good    |

---

### ğŸ”„ **Default Behavior:**

| Parameter         | Default |
| ----------------- | ------- |
| `depends_on_past` | `False` |

So by default, tasks do **not** wait on their own previous runs.

---

### ğŸ” **Combined with Other Parameters:**

You can use it alongside:

```python
PythonOperator(
    task_id='transform_data',
    depends_on_past=True,
    retries=2,
    retry_delay=timedelta(minutes=5)
)
```

---

### âš ï¸ Things to Note:

* `depends_on_past` only checks **the same task** in **previous DAG run**.
* If you **clear** a failed task in the UI and rerun it, the next one **can proceed**.

---

# ğŸ” `retries` in Apache Airflow

In **Apache Airflow**, the `retries` parameter controls **how many times a task should be retried** if it **fails**.

---

### ğŸ§  **Simple Definition:**

> `retries` defines **how many retry attempts** Airflow should make if a task fails, before marking it as `failed`.

---

### ğŸ”§ **Syntax Example:**

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta

def failing_task():
    raise Exception("This will fail!")

with DAG(
    dag_id="retry_example_dag",
    start_date=datetime(2025, 7, 1),
    schedule_interval="@daily",
    catchup=False
) as dag:

    task = PythonOperator(
        task_id="fail_and_retry",
        python_callable=failing_task,
        retries=3,  # Retry 3 times on failure
        retry_delay=timedelta(minutes=5)  # Wait 5 mins between retries
    )
```

---

### âš™ï¸ **Common Retry-Related Parameters:**

| Parameter                   | Purpose                                               |
| --------------------------- | ----------------------------------------------------- |
| `retries`                   | Number of retry attempts (default is `0`)             |
| `retry_delay`               | Time to wait between retries                          |
| `max_retry_delay`           | Maximum delay between retries                         |
| `retry_exponential_backoff` | Increases delay exponentially                         |
| `retry_on_failure`          | Retry on any failure (default behavior)               |
| `depends_on_past`           | If True, task wonâ€™t run unless previous run succeeded |

---

### ğŸ” **Retry Behavior:**

* If `retries=3` and a task fails:

  * Airflow will try to rerun it **3 more times**
  * After exhausting retries, it is marked as `failed`
* Task state transitions:

  ```
  â†’ running â†’ failed â†’ retrying â†’ success or failed
  ```

---

### ğŸ§© **Use Case:**

If a task connects to an external API that may temporarily fail, setting retries helps ensure:

* Transient errors donâ€™t break the DAG
* Workflow can self-heal without manual intervention

---

### ğŸ“Œ Note:

* `retry_delay` uses `datetime.timedelta`
* Tasks can be retried **automatically**, without restarting the whole DAG
* You can monitor retry attempts in the **Task Logs** and **Tree View**

---

# ğŸš¨ `on_failure_callback` in Apache Airflow

In **Apache Airflow**, the `on_failure_callback` is a **function** that gets triggered **automatically when a task fails**.

---

### ğŸ§  **Simple Definition:**

> `on_failure_callback` lets you **define custom actions** (like sending alerts, logging, or triggering fallback logic) when a **task fails**.

---

### ğŸ“¦ **Where Itâ€™s Used:**

* On **individual tasks**
* At the **DAG level** (applies to all tasks unless overridden)

---

### âœ… **Common Use Cases:**

| Use Case       | What You Can Do                                  |
| -------------- | ------------------------------------------------ |
| ğŸ”” Alerting    | Send Slack, Email, or Teams messages             |
| ğŸ“œ Logging     | Log failure details to a file or external system |
| ğŸ“‚ Recovery    | Trigger a cleanup or rollback DAG                |
| ğŸ” Retry Logic | Call APIs to reset states or open tickets        |

---

### ğŸ“Œ **Callback Function Requirements:**

* Must accept a single argument: `context` (dict)
* `context` contains info about the failed task, DAG, exception, etc.

---

### ğŸ§ª **Example: Custom Failure Callback**

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime
import logging

def my_failure_alert(context):
    task_id = context['task_instance'].task_id
    dag_id = context['dag'].dag_id
    exception = context['exception']
    logging.error(f"Task {task_id} in DAG {dag_id} failed. Reason: {exception}")

def always_fails():
    raise ValueError("Intentional failure")

with DAG(
    dag_id='failure_callback_dag',
    start_date=datetime(2025, 7, 1),
    schedule_interval="@daily",
    catchup=False
) as dag:

    failing_task = PythonOperator(
        task_id='failing_task',
        python_callable=always_fails,
        on_failure_callback=my_failure_alert  # <-- Callback here
    )
```

---

### ğŸ› ï¸ **DAG-level Failure Callback:**

Apply to **all tasks** in the DAG:

```python
with DAG(
    dag_id='dag_with_global_callback',
    start_date=datetime(2025, 7, 1),
    schedule_interval="@daily",
    default_args={
        'on_failure_callback': my_failure_alert
    }
) as dag:
    ...
```

---

### ğŸ” **`context` Dictionary Includes:**

| Key              | Description        |
| ---------------- | ------------------ |
| `dag`            | DAG object         |
| `task`           | Task object        |
| `ti`             | TaskInstance       |
| `execution_date` | Execution datetime |
| `log_url`        | Link to logs in UI |
| `exception`      | Exception raised   |
