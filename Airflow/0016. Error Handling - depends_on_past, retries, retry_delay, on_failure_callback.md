### 🔁 `depends_on_past` in Apache Airflow

The **`depends_on_past`** parameter in Airflow is used to control whether a task should **wait for its own previous run to succeed** before running again.

---

### 🧠 **Simple Definition:**

> If `depends_on_past=True`, a task will **not run** for the current DAG run unless its **previous run was successful**.

---

### 📅 **Use Case Example:**

Suppose your DAG runs **daily**, and you have a task `load_to_db`:

```python
PythonOperator(
    task_id='load_to_db',
    python_callable=load_data,
    depends_on_past=True
)
```

* ✅ On **July 10**, the task runs successfully.
* 🟥 On **July 11**, it fails.
* 🔁 On **July 12**, it **will not run** because **July 11 failed**.

---

### 🧩 **Why Use `depends_on_past`?**

| Scenario                                           | Benefit                                           |
| -------------------------------------------------- | ------------------------------------------------- |
| **Data pipelines** that must process days in order | Ensures previous data is complete                 |
| **Incremental loading**                            | Prevents loading Day 3 before Day 2 is successful |
| **Avoids data duplication**                        | Won't reprocess unless previous batch was good    |

---

### 🔄 **Default Behavior:**

| Parameter         | Default |
| ----------------- | ------- |
| `depends_on_past` | `False` |

So by default, tasks do **not** wait on their own previous runs.

---

### 🔐 **Combined with Other Parameters:**

You can use it alongside:

```python
PythonOperator(
    task_id='transform_data',
    depends_on_past=True,
    retries=2,
    retry_delay=timedelta(minutes=5)
)
```

---

### ⚠️ Things to Note:

* `depends_on_past` only checks **the same task** in **previous DAG run**.
* If you **clear** a failed task in the UI and rerun it, the next one **can proceed**.

---

### 🔁 `retries` in Apache Airflow

In **Apache Airflow**, the `retries` parameter controls **how many times a task should be retried** if it **fails**.

---

### 🧠 **Simple Definition:**

> `retries` defines **how many retry attempts** Airflow should make if a task fails, before marking it as `failed`.

---

### 🔧 **Syntax Example:**

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta

def failing_task():
    raise Exception("This will fail!")

with DAG(
    dag_id="retry_example_dag",
    start_date=datetime(2025, 7, 1),
    schedule_interval="@daily",
    catchup=False
) as dag:

    task = PythonOperator(
        task_id="fail_and_retry",
        python_callable=failing_task,
        retries=3,  # Retry 3 times on failure
        retry_delay=timedelta(minutes=5)  # Wait 5 mins between retries
    )
```

---

### ⚙️ **Common Retry-Related Parameters:**

| Parameter                   | Purpose                                               |
| --------------------------- | ----------------------------------------------------- |
| `retries`                   | Number of retry attempts (default is `0`)             |
| `retry_delay`               | Time to wait between retries                          |
| `max_retry_delay`           | Maximum delay between retries                         |
| `retry_exponential_backoff` | Increases delay exponentially                         |
| `retry_on_failure`          | Retry on any failure (default behavior)               |
| `depends_on_past`           | If True, task won’t run unless previous run succeeded |

---

### 🔁 **Retry Behavior:**

* If `retries=3` and a task fails:

  * Airflow will try to rerun it **3 more times**
  * After exhausting retries, it is marked as `failed`
* Task state transitions:

  ```
  → running → failed → retrying → success or failed
  ```

---

### 🧩 **Use Case:**

If a task connects to an external API that may temporarily fail, setting retries helps ensure:

* Transient errors don’t break the DAG
* Workflow can self-heal without manual intervention

---

### 📌 Note:

* `retry_delay` uses `datetime.timedelta`
* Tasks can be retried **automatically**, without restarting the whole DAG
* You can monitor retry attempts in the **Task Logs** and **Tree View**
