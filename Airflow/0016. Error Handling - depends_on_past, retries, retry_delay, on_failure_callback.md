# 🔁 `depends_on_past` in Apache Airflow

The **`depends_on_past`** parameter in Airflow is used to control whether a task should **wait for its own previous run to succeed** before running again.

---

### 🧠 **Simple Definition:**

> If `depends_on_past=True`, a task will **not run** for the current DAG run unless its **previous run was successful**.

---

### 📅 **Use Case Example:**

Suppose your DAG runs **daily**, and you have a task `load_to_db`:

```python
PythonOperator(
    task_id='load_to_db',
    python_callable=load_data,
    depends_on_past=True
)
```

* ✅ On **July 10**, the task runs successfully.
* 🟥 On **July 11**, it fails.
* 🔁 On **July 12**, it **will not run** because **July 11 failed**.

---

### 🧩 **Why Use `depends_on_past`?**

| Scenario                                           | Benefit                                           |
| -------------------------------------------------- | ------------------------------------------------- |
| **Data pipelines** that must process days in order | Ensures previous data is complete                 |
| **Incremental loading**                            | Prevents loading Day 3 before Day 2 is successful |
| **Avoids data duplication**                        | Won't reprocess unless previous batch was good    |

---

### 🔄 **Default Behavior:**

| Parameter         | Default |
| ----------------- | ------- |
| `depends_on_past` | `False` |

So by default, tasks do **not** wait on their own previous runs.

---

### 🔐 **Combined with Other Parameters:**

You can use it alongside:

```python
PythonOperator(
    task_id='transform_data',
    depends_on_past=True,
    retries=2,
    retry_delay=timedelta(minutes=5)
)
```

---

### ⚠️ Things to Note:

* `depends_on_past` only checks **the same task** in **previous DAG run**.
* If you **clear** a failed task in the UI and rerun it, the next one **can proceed**.

---

# 🔁 `retries` in Apache Airflow

In **Apache Airflow**, the `retries` parameter controls **how many times a task should be retried** if it **fails**.

---

### 🧠 **Simple Definition:**

> `retries` defines **how many retry attempts** Airflow should make if a task fails, before marking it as `failed`.

---

### 🔧 **Syntax Example:**

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta

def failing_task():
    raise Exception("This will fail!")

with DAG(
    dag_id="retry_example_dag",
    start_date=datetime(2025, 7, 1),
    schedule_interval="@daily",
    catchup=False
) as dag:

    task = PythonOperator(
        task_id="fail_and_retry",
        python_callable=failing_task,
        retries=3,  # Retry 3 times on failure
        retry_delay=timedelta(minutes=5)  # Wait 5 mins between retries
    )
```

---

### ⚙️ **Common Retry-Related Parameters:**

| Parameter                   | Purpose                                               |
| --------------------------- | ----------------------------------------------------- |
| `retries`                   | Number of retry attempts (default is `0`)             |
| `retry_delay`               | Time to wait between retries                          |
| `max_retry_delay`           | Maximum delay between retries                         |
| `retry_exponential_backoff` | Increases delay exponentially                         |
| `retry_on_failure`          | Retry on any failure (default behavior)               |
| `depends_on_past`           | If True, task won’t run unless previous run succeeded |

---

### 🔁 **Retry Behavior:**

* If `retries=3` and a task fails:

  * Airflow will try to rerun it **3 more times**
  * After exhausting retries, it is marked as `failed`
* Task state transitions:

  ```
  → running → failed → retrying → success or failed
  ```

---

### 🧩 **Use Case:**

If a task connects to an external API that may temporarily fail, setting retries helps ensure:

* Transient errors don’t break the DAG
* Workflow can self-heal without manual intervention

---

### 📌 Note:

* `retry_delay` uses `datetime.timedelta`
* Tasks can be retried **automatically**, without restarting the whole DAG
* You can monitor retry attempts in the **Task Logs** and **Tree View**

---

# 🚨 `on_failure_callback` in Apache Airflow

In **Apache Airflow**, the `on_failure_callback` is a **function** that gets triggered **automatically when a task fails**.

---

### 🧠 **Simple Definition:**

> `on_failure_callback` lets you **define custom actions** (like sending alerts, logging, or triggering fallback logic) when a **task fails**.

---

### 📦 **Where It’s Used:**

* On **individual tasks**
* At the **DAG level** (applies to all tasks unless overridden)

---

### ✅ **Common Use Cases:**

| Use Case       | What You Can Do                                  |
| -------------- | ------------------------------------------------ |
| 🔔 Alerting    | Send Slack, Email, or Teams messages             |
| 📜 Logging     | Log failure details to a file or external system |
| 📂 Recovery    | Trigger a cleanup or rollback DAG                |
| 🔁 Retry Logic | Call APIs to reset states or open tickets        |

---

### 📌 **Callback Function Requirements:**

* Must accept a single argument: `context` (dict)
* `context` contains info about the failed task, DAG, exception, etc.

---

### 🧪 **Example: Custom Failure Callback**

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime
import logging

def my_failure_alert(context):
    task_id = context['task_instance'].task_id
    dag_id = context['dag'].dag_id
    exception = context['exception']
    logging.error(f"Task {task_id} in DAG {dag_id} failed. Reason: {exception}")

def always_fails():
    raise ValueError("Intentional failure")

with DAG(
    dag_id='failure_callback_dag',
    start_date=datetime(2025, 7, 1),
    schedule_interval="@daily",
    catchup=False
) as dag:

    failing_task = PythonOperator(
        task_id='failing_task',
        python_callable=always_fails,
        on_failure_callback=my_failure_alert  # <-- Callback here
    )
```

---

### 🛠️ **DAG-level Failure Callback:**

Apply to **all tasks** in the DAG:

```python
with DAG(
    dag_id='dag_with_global_callback',
    start_date=datetime(2025, 7, 1),
    schedule_interval="@daily",
    default_args={
        'on_failure_callback': my_failure_alert
    }
) as dag:
    ...
```

---

### 🔍 **`context` Dictionary Includes:**

| Key              | Description        |
| ---------------- | ------------------ |
| `dag`            | DAG object         |
| `task`           | Task object        |
| `ti`             | TaskInstance       |
| `execution_date` | Execution datetime |
| `log_url`        | Link to logs in UI |
| `exception`      | Exception raised   |
