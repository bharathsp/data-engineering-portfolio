# ğŸ”‘ **sortByKey() in Spark**

## ğŸ§© What is it?

* `sortByKey()` is an **RDD transformation** in Apache Spark.
* It is used on **Pair RDDs (key-value RDDs)**.
* It sorts the **RDD by keys** in either **ascending (default)** or **descending** order.

ğŸ‘‰ It returns a **new RDD** where the data is sorted by the key.

---

## âš¡ Syntax

```python
rdd.sortByKey(ascending=True, numPartitions=None, keyfunc=lambda k: k)
```

### Parameters:

* `ascending`: `True` (default) for ascending, `False` for descending.
* `numPartitions`: Number of partitions in the output RDD.
* `keyfunc`: Function applied to keys before sorting (like a transformation on keys).

---

## ğŸ“Š Example 1: Basic Usage

```python
from pyspark import SparkContext
sc = SparkContext("local", "SortByKeyExample")

data = [(3, "apple"), (1, "banana"), (2, "cherry")]
rdd = sc.parallelize(data)

sorted_rdd = rdd.sortByKey()
print(sorted_rdd.collect())
```

**Output:**

```
[(1, 'banana'), (2, 'cherry'), (3, 'apple')]
```

---

## ğŸ“Š Example 2: Descending Order

```python
sorted_desc = rdd.sortByKey(ascending=False)
print(sorted_desc.collect())
```

**Output:**

```
[(3, 'apple'), (2, 'cherry'), (1, 'banana')]
```

---

## ğŸ“Š Example 3: Custom Key Function

Suppose keys are strings and you want to sort by **length of the key**:

```python
data = [("aa", 1), ("b", 2), ("ccc", 3)]
rdd = sc.parallelize(data)

sorted_custom = rdd.sortByKey(keyfunc=lambda k: len(k))
print(sorted_custom.collect())
```

**Output:**

```
[('b', 2), ('aa', 1), ('ccc', 3)]
```

---

# ğŸ”„ Difference vs Similar Functions

* âœ… `sortByKey()` â†’ Sorts by **key** (only for Pair RDDs).
* âœ… `sortBy()` â†’ Can sort by **any function** applied to RDD elements.
* âœ… `orderBy()` â†’ Used in **DataFrames**, not RDDs.

---

# ğŸ–¼ï¸ Analogy

Think of `sortByKey()` like **sorting a dictionary by its keys** ğŸ“’ â†’ you donâ€™t care about values while sorting.

Example:

```
Before: {3: "apple", 1: "banana", 2: "cherry"}
After:  {1: "banana", 2: "cherry", 3: "apple"}
```

---

âœ… **In short:**

* Use `sortByKey()` when working with **Pair RDDs**.
* It sorts based on the **keys** (ascending/descending).
* Useful in preprocessing, grouping, and ranking tasks in Spark.
