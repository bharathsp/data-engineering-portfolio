# ğŸ¼ Pandas API on Spark

* Spark 3.2+ introduced the **Pandas API on Spark** (`pyspark.pandas`, now `pyspark.pandas` â†’ `pyspark.pandas` / `pyspark.pandas.api`).
* It lets you write **Pandas-like code** that runs **on top of Spark**, giving scalability to huge datasets.
* Perfect for users familiar with Pandas but working with **big data**.

ğŸ‘‰ Think of it as **Pandas syntax + Spark power ğŸš€**.

---

# ğŸ”„ Conversion Between Pandas & Spark DataFrames

## 1ï¸âƒ£ **Pandas â†’ Spark DataFrame**

```python
import pandas as pd
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("PandasToSpark").getOrCreate()

# Create Pandas DataFrame
pdf = pd.DataFrame({
    "name": ["Alice", "Bob", "Cathy"],
    "age": [25, 30, 28]
})

# Convert to Spark DataFrame
spark_df = spark.createDataFrame(pdf)

spark_df.show()
```

**Output:**

```
+-----+---+
| name|age|
+-----+---+
|Alice| 25|
|  Bob| 30|
|Cathy| 28|
+-----+---+
```

---

## 2ï¸âƒ£ **Spark DataFrame â†’ Pandas DataFrame**

```python
# Convert back to Pandas DataFrame
pdf2 = spark_df.toPandas()
print(pdf2)
```

**Output:**

```
    name  age
0  Alice   25
1    Bob   30
2  Cathy   28
```

âš ï¸ **Caution:** `toPandas()` collects all data into **driver memory**. Not recommended for very large datasets.

---

# ğŸ—ï¸ Example: Pandas API on Spark

Instead of converting back and forth, you can just use the **Pandas API on Spark** directly:

```python
import pyspark.pandas as ps

# Create Pandas-on-Spark DataFrame
psdf = ps.DataFrame({
    "name": ["Alice", "Bob", "Cathy"],
    "age": [25, 30, 28]
})

print(psdf)
```

Now you can use familiar Pandas operations:

```python
print(psdf[psdf.age > 27])   # Filtering
print(psdf.describe())       # Stats
```

These run on **Spark cluster**, not just locally.

---

# ğŸ–¼ï¸ Analogy

* **Pandas ğŸ¼** = Your laptop Excel â†’ great for small files.
* **Spark DataFrame âš¡** = A distributed database table â†’ handles **huge files**.
* **Pandas API on Spark ğŸ¼âš¡** = Excel-like interface, but powered by a **distributed cluster**.

---

# âœ… Summary

* **Convert Pandas â†’ Spark** â†’ `spark.createDataFrame(pandas_df)`
* **Convert Spark â†’ Pandas** â†’ `spark_df.toPandas()` (use cautiously on big data).
* **Pandas API on Spark** â†’ Write Pandas-style code, execute with Spark scalability.
