# 🐼 Pandas API on Spark

* Spark 3.2+ introduced the **Pandas API on Spark** (`pyspark.pandas`, now `pyspark.pandas` → `pyspark.pandas` / `pyspark.pandas.api`).
* It lets you write **Pandas-like code** that runs **on top of Spark**, giving scalability to huge datasets.
* Perfect for users familiar with Pandas but working with **big data**.

👉 Think of it as **Pandas syntax + Spark power 🚀**.

---

# 🔄 Conversion Between Pandas & Spark DataFrames

## 1️⃣ **Pandas → Spark DataFrame**

```python
import pandas as pd
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("PandasToSpark").getOrCreate()

# Create Pandas DataFrame
pdf = pd.DataFrame({
    "name": ["Alice", "Bob", "Cathy"],
    "age": [25, 30, 28]
})

# Convert to Spark DataFrame
spark_df = spark.createDataFrame(pdf)

spark_df.show()
```

**Output:**

```
+-----+---+
| name|age|
+-----+---+
|Alice| 25|
|  Bob| 30|
|Cathy| 28|
+-----+---+
```

---

## 2️⃣ **Spark DataFrame → Pandas DataFrame**

```python
# Convert back to Pandas DataFrame
pdf2 = spark_df.toPandas()
print(pdf2)
```

**Output:**

```
    name  age
0  Alice   25
1    Bob   30
2  Cathy   28
```

⚠️ **Caution:** `toPandas()` collects all data into **driver memory**. Not recommended for very large datasets.

---

# 🏗️ Example: Pandas API on Spark

Instead of converting back and forth, you can just use the **Pandas API on Spark** directly:

```python
import pyspark.pandas as ps

# Create Pandas-on-Spark DataFrame
psdf = ps.DataFrame({
    "name": ["Alice", "Bob", "Cathy"],
    "age": [25, 30, 28]
})

print(psdf)
```

Now you can use familiar Pandas operations:

```python
print(psdf[psdf.age > 27])   # Filtering
print(psdf.describe())       # Stats
```

These run on **Spark cluster**, not just locally.

---

# 🖼️ Analogy

* **Pandas 🐼** = Your laptop Excel → great for small files.
* **Spark DataFrame ⚡** = A distributed database table → handles **huge files**.
* **Pandas API on Spark 🐼⚡** = Excel-like interface, but powered by a **distributed cluster**.

---

# ✅ Summary

* **Convert Pandas → Spark** → `spark.createDataFrame(pandas_df)`
* **Convert Spark → Pandas** → `spark_df.toPandas()` (use cautiously on big data).
* **Pandas API on Spark** → Write Pandas-style code, execute with Spark scalability.
