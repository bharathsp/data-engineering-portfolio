## **PySpark DataFrame Options & Methods**

---

### ðŸ§¾ **1. `header=True`**

**Definition**: Instructs Spark to treat the first line of the CSV file as column names.

**Syntax**:

```python
df = spark.read.csv("data.csv", header=True)
```

**Example Input File (`data.csv`)**:

```
Name,Age
Alice,30
Bob,25
```

**Output DataFrame**:

```text
+-----+---+
|Name |Age|
+-----+---+
|Alice| 30|
|Bob  | 25|
+-----+---+
```

---

### ðŸ§¾ **2. `inferSchema=True`**

**Definition**: Automatically infers the data types of columns based on the input data.

**Syntax**:

```python
df = spark.read.csv("data.csv", header=True, inferSchema=True)
```

**Example**:
Without `inferSchema`, all fields are strings.
With it, `Age` becomes `IntegerType`.

---

### ðŸ§¾ **3. `schema=...`**

**Definition**: Define a schema explicitly using `StructType` and `StructField`.

**Syntax**:

```python
from pyspark.sql.types import StructType, StructField, StringType, IntegerType

schema = StructType([
    StructField("Name", StringType(), True),
    StructField("Age", IntegerType(), True)
])

df = spark.read.csv("data.csv", header=True, schema=schema)
```

---

### ðŸ§¾ **3. `printSchema()`**

**Definition**:
Prints the **schema** (i.e., structure) of a DataFrame in a **tree format** showing column names, data types, and nullability.

**Syntax**:

```python
df.printSchema()
```

**Example Input DataFrame**:

```python
+-------+---+------------+
| Name  |Age| JoinDate   |
+-------+---+------------+
| Alice | 30| 2022-01-01 |
| Bob   | 25| 2021-05-10 |
+-------+---+------------+
```

**Code**:

```python
df.printSchema()
```

**Output**:

```text
root
 |-- Name: string (nullable = true)
 |-- Age: integer (nullable = true)
 |-- JoinDate: string (nullable = true)
```

---

### ðŸ§¾ **4. sep=...**

**Definition**:
Specifies the **delimiter** used in the CSV file (default is comma `,`). Useful for reading **TSV**, **pipe-separated**, or any custom-delimited files.

**Syntax**:

```python
df = spark.read.csv("data.tsv", header=True, sep="\t")
```

**Example**:
Assume a tab-separated file (`data.tsv`) with the following content:

```
Name	Age
Alice	30
Bob	25
```

**Code**:

```python
df = spark.read.csv("data.tsv", header=True, sep="\t")
df.show()
```

**Output**:

```text
+-----+---+
|Name |Age|
+-----+---+
|Alice| 30|
|Bob  | 25|
+-----+---+
```

You can replace `sep="\t"` with other delimiters as needed:

* For pipe `|` separated: `sep="|"`
* For semicolon separated: `sep=";"`
* For custom characters: `sep="#"`

---

### ðŸ§¾ **4. `show(n)`**

**Definition**: Displays the top `n` rows of the DataFrame (default: 20).

**Syntax**:

```python
df.show(2)
```

**Output**:

```text
+-----+---+
|Name |Age|
+-----+---+
|Alice| 30|
|Bob  | 25|
+-----+---+
```


---

### ðŸ§¾ **5. `withColumn()`**

**Definition**: Adds a new column or replaces an existing one.

**Syntax**:

```python
from pyspark.sql.functions import col
df.withColumn("AgePlus5", col("Age") + 5).show()
```

**Output**:

```text
+-----+---+--------+
|Name |Age|AgePlus5|
+-----+---+--------+
|Alice| 30|     35 |
|Bob  | 25|     30 |
+-----+---+--------+
```

---

### ðŸ§¾ **6. `select()`**

**Definition**: Selects one or more columns from the DataFrame.

**Syntax**:

```python
df.select("Name").show()
```

**Output**:

```text
+-----+
|Name |
+-----+
|Alice|
|Bob  |
+-----+
```

---

### ðŸ§¾ **7. `filter()` / `where()`**

**Definition**: Filters rows based on a boolean condition.

**Syntax**:

```python
from pyspark.sql.functions import col

df.filter(col("Age") > 25).show()
# OR
df.where("Age > 25").show()
```

**Output**:

```text
+-----+---+
|Name |Age|
+-----+---+
|Alice| 30|
+-----+---+
```

---

### ðŸ§¾ **8. `groupBy()`**

**Definition**: Groups rows by one or more columns for aggregation.

**Syntax**:

```python
df.groupBy("Name").count().show()
```

**Input**:

```text
+-----+---+
|Name |Age|
+-----+---+
|Alice| 30|
|Alice| 25|
|Bob  | 25|
+-----+---+
```

**Output**:

```text
+-----+-----+
|Name |count|
+-----+-----+
|Alice|  2  |
|Bob  |  1  |
+-----+-----+
```

---

### ðŸ§¾ **9. `agg()`**

**Definition**: Applies one or more aggregation functions after `groupBy`.

**Syntax**:

```python
from pyspark.sql.functions import sum, avg

df.groupBy("Name").agg(sum("Age").alias("TotalAge"), avg("Age").alias("AvgAge")).show()
```

**Output**:

```text
+-----+--------+-------+
|Name |TotalAge|AvgAge |
+-----+--------+-------+
|Alice|   55   | 27.5  |
|Bob  |   25   | 25.0  |
+-----+--------+-------+
```

---

### ðŸ§¾ **10. `orderBy()` / `sort()`**

**Definition**: Sorts rows by one or more columns.

**Syntax**:

```python
df.orderBy("Age").show()
df.orderBy(col("Age").desc()).show()
```

**Output** (Ascending):

```text
+-----+---+
|Name |Age|
+-----+---+
|Bob  | 25|
|Alice| 30|
+-----+---+
```

---

### ðŸ§¾ **11. `drop()`**

**Definition**: Drops a specified column from the DataFrame.

**Syntax**:

```python
df.drop("Age").show()
```

**Output**:

```text
+-----+
|Name |
+-----+
|Alice|
|Bob  |
+-----+
```

---

### ðŸ§¾ **12. `na.fill()` / `na.drop()` / `na.replace()`**

**Definition**: Handle missing (`null`) values.

#### âœ… `na.fill(value)`

```python
df.na.fill(0).show()
```

#### âœ… `na.drop()`

```python
df.na.drop().show()
```

#### âœ… `na.replace()`

```python
df.na.replace("NA", None).show()
```

**Input**:

```text
+-----+----+
|Name |Age |
+-----+----+
|John | 25 |
|Jane |null|
|NA   | 40 |
+-----+----+
```

**Output (using `fill`)**:

```text
+-----+----+
|Name |Age |
+-----+----+
|John | 25 |
|Jane |  0 |
|NA   | 40 |
+-----+----+
```

---
