# Important Information

**CPU Cores (Hardware level)**

* These are physical or virtual cores on the machines in your cluster.
* Example: If a worker node has 2 CPUs Ã— 8 cores each, that node has 16 CPU cores.
* This is the real hardware your jobs will run on.

**Spark Cores (Spark resource allocation)**

* In Spark, when we talk about â€œnumber of coresâ€, we usually mean:
* How many CPU threads Spark is allowed to use across the cluster.
* This is controlled via executor cores (--executor-cores) Ã— number of executors (--num-executors).
* Spark maps its â€œcoresâ€ to CPU threads, but itâ€™s a logical allocation, not a separate hardware entity.
* Spark canâ€™t use more Spark cores than CPU cores available in the cluster (unless you overcommit in virtualized environments, which can hurt performance).

**Waves**

* In Spark, "waves" refers to how many rounds (batches) of tasks need to run sequentially to process all partitions of your dataset, given the number of available executor cores.
* Spark splits your data into partitions.
* Each partition is processed by one task.
* The number of tasks that can run at the same time is determined by your total available cores (sum of all executor cores).
* If you have more partitions than cores, not all tasks can run at once â€” theyâ€™ll have to be executed in waves.
* If you want 1 wave, your total cores should equal your total partitions (so all partitions run at once).
* If youâ€™re okay with multiple waves, you can use fewer cores â€” but total job time may increase.

---

# Thumb Rules

Here are some common **thumb rules** that Spark engineers (including me) usually assume when doing cluster configuration sizing:

---

## **ðŸ“Œ Partitioning**

1. **Default partition size:** \~**128 MB** per partition (sometimes 256 MB for large clusters to reduce task scheduling overhead).
2. **#Partitions formula:**

   $$
   \text{Partitions} = \frac{\text{Data size (MB)}}{\text{Partition size (MB)}}
   $$
3. Have **2â€“4Ã— more partitions than total cores** for good load balancing.
4. Avoid too many small partitions â€” increases task scheduling overhead.

---

## **ðŸ“Œ Cores per Executor**

5. Recommended: **4â€“5 cores per executor** for a good balance between parallelism and GC overhead.
6. Never exceed **\~5 cores** unless you know your workload is CPU-bound and GC is tuned well.

---

## **ðŸ“Œ Memory per Executor**

7. **Memory per executor formula:**

Partition size x Cores per executor x Safety factor (2â€“3Ã—)

   * overhead (â‰ˆ 300â€“500 MB).
8. Keep **executor memory â‰¤ 32 GB** to avoid Spark switching from â€œcompressed oopsâ€ to normal object references (which use more memory).
9. **Heap + Overhead = Total executor memory** (YARN automatically adds overhead unless set manually).

---

## **ðŸ“Œ Total Cores**

10. **Cores = Executors Ã— Cores per executor**.
11. **1 core processes 1 partition at a time** â€” this is the key driver of waves.
12. For a **1-wave** execution:
* Total cores â‰ˆ Total partitions

For **multi-wave** execution:
* Total cores} â‰ˆ Partitions / waves

---

## **ðŸ“Œ Executor Count**

13. Executors = Total cores Ã· Cores per executor.
14. Keep a couple of cores free for the driver & application master (YARN mode).

---

## **ðŸ“Œ General**

15. **Data locality matters** â€” having more cores than available local data blocks can cause network shuffling.
16. Always consider **compression factor** â€” if data is compressed, memory needs post-decompression can be 2â€“4Ã— higher.
17. Shuffle-heavy jobs (joins, groupBy) require more memory per executor.
18. If caching data, size executors so the dataset fits in memory with safety buffer.

---

# Scenario 1
To process 25GB of data in spark

* a. How many CPU cores are required?
* b. How many executors are required?
* c. How much memory is required for each executor?
* d. What is the total memory required?

---

## **Step 1 â€“ Understanding Spark resource calculation**

When deciding Spark resources, you need:

* **Data size** (25 GB given)
* **Executor memory allocation rule** (leave \~1 GB overhead for Spark/YARN)
* **Number of cores per executor** (usually 4â€“5 for optimal parallelism)
* **Data-to-memory ratio** (rule of thumb: `2x data size` in memory for shuffle/processing overhead)

---

### **A. Number of CPU Cores**

**Rule:**

* Spark parallelism works best with **\~4 cores per executor**.
* **Number of cores total** â‰ˆ `(Data size / Data per core)` Ã— cores per executor.

We need to know **how much data one core should handle**.
A common guideline is:

> 1 core can comfortably handle **2â€“3 GB** of in-memory data without becoming a bottleneck.

Letâ€™s assume **2.5 GB per core**:

```
Total cores needed = 25 GB Ã· 2.5 GB = 10 cores
```

âœ… **Answer:** **10 CPU cores** (can be split across executors)

---

### **B. Number of Executors**

We donâ€™t want 1 giant executor (bad for fault tolerance), so:

* If each executor has **4 cores**:

```
Executors = Total cores Ã· Cores per executor
Executors = 10 Ã· 4 = 2.5 â‰ˆ 3 executors
```

Weâ€™ll slightly over-provision, so **3 executors** is fine.

---

### **C. Memory Required per Executor**

**Rule:**

* Memory per executor = `(Data per executor Ã— Overhead factor)`
* Overhead factor: \~1.2 to 2 (due to shuffle, caching, etc.)
* Also, leave **\~1 GB** for YARN/Mesos overhead.

First, **data per executor**:

```
Data per executor = Total data Ã· Number of executors
= 25 GB Ã· 3 â‰ˆ 8.33 GB
```

Add **2Ã— overhead** for Spark shuffles:

```
8.33 GB Ã— 2 = 16.66 GB
```

Add **1 GB YARN overhead**:

```
16.66 + 1 â‰ˆ 17.66 GB
```

Weâ€™ll round up to the nearest GB (and often allocate in even GBs):
âœ… **Answer:** **18 GB per executor**

---

### **D. Total Memory Required**

```
Total memory = Memory per executor Ã— Number of executors
= 18 GB Ã— 3 = 54 GB
```

âœ… **Answer:** **54 GB total cluster memory**

---

## **Final Spark Resource Plan**

| Parameter                  | Value     | Reason                          |
| -------------------------- | --------- | ------------------------------- |
| **a. CPU cores**           | **10**    | Based on \~2.5 GB/core          |
| **b. Executors**           | **3**     | 4 cores/executor rule           |
| **c. Memory per executor** | **18 GB** | 2Ã— shuffle overhead + 1 GB YARN |
| **d. Total memory**        | **54 GB** | 18 GB Ã— 3 executors             |

---

## **Extra Tip** ðŸš€

In Spark, this would translate to:

```bash
--num-executors 3 \
--executor-cores 4 \
--executor-memory 18G
```

(Since 3 Ã— 4 cores = 12 cores, a bit more than our minimum 10 cores, giving better parallelism.)
