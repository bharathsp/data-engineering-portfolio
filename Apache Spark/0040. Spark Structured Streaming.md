# ⚡ **What is Structured Streaming?**

* **Structured Streaming** is a **stream processing engine** built on top of Spark SQL.
* Processes data streams as **unbounded tables** (infinite DataFrames).
* Provides a **declarative API (SQL/DataFrames)** instead of low-level DStreams.
* Handles **event-time**, **watermarking**, **exactly-once guarantees**.
* It’s the **successor to Spark Streaming** (RDD-based).

👉 In short: it’s **real-time DataFrames**.

---

# 🏗️ How It Works

1. Incoming stream of data = treated as an **infinite table**.
2. Each new batch of data = **new rows appended**.
3. You write queries using **DataFrame API or SQL**.
4. Spark executes queries **incrementally** as new data arrives.

---

# 📊 Example: Structured Streaming Word Count

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("StructuredStreaming").getOrCreate()

# Read stream from socket
lines = spark.readStream.format("socket") \
    .option("host", "localhost") \
    .option("port", 9999) \
    .load()

# Split into words
from pyspark.sql.functions import explode, split
words = lines.select(explode(split(lines.value, " ")).alias("word"))

# Count words
word_counts = words.groupBy("word").count()

# Output to console
query = word_counts.writeStream \
    .outputMode("complete") \
    .format("console") \
    .start()

query.awaitTermination()
```

---

# ▶️ How to Run

1. Start a socket server:

   ```bash
   nc -lk 9999
   ```
2. Type messages (e.g., `hello structured streaming spark spark`).
3. Spark updates the **console output** with live word counts.

---

# 🔄 Output Modes

* **Append** → Only new rows are written.
* **Complete** → Entire result table updated each time.
* **Update** → Only updated rows are written.

---

# 📡 Sources

Structured Streaming supports:

* **Socket (TCP)**
* **Kafka**
* **Files (CSV, JSON, Parquet)**
* **Rate generator (testing)**

---

# 🛠️ Sinks (Where to Output)

* Console (debugging)
* Files (CSV, JSON, Parquet)
* Kafka
* Memory (for debugging SQL queries)

---

# 🖼️ Real-Life Use Cases

* 📊 **Real-time dashboards** → Monitoring web traffic/logs.
* 💳 **Fraud detection** → Flagging suspicious credit card transactions.
* 📦 **IoT streaming** → Analyzing sensor/telemetry data.
* 📡 **ETL pipelines** → Incrementally ingesting & transforming data.
* 🐦 **Social media analytics** → Processing tweets or comments in real-time.

---

# 🖼️ Analogy

Think of a **live spreadsheet 📊**:

* New rows keep getting added (stream).
* Formulas (queries) are **always running**.
* You see updated results continuously.

---

# ✅ Summary

* **Structured Streaming** = real-time DataFrames (not micro-batches like old Spark Streaming).
* Built on **Spark SQL** → easier, more powerful, supports event-time & exactly-once.
* Sources = Kafka, sockets, files; Sinks = console, Kafka, files.
* Best choice for **new streaming projects** in Spark.
