The **Spark UI** is a **web-based interface** that lets you monitor and debug your Spark applications in real time or after they finish running.

Think of it like Sparkâ€™s **â€œmission control dashboardâ€** â€” it shows you whatâ€™s happening inside your job:

* Which stages and tasks are running
* How much time each took
* Memory, CPU, and shuffle data usage
* Any errors or slowdowns

---

## **1ï¸âƒ£ Where to Access It**

* **Local mode:** By default at `http://localhost:4040` while your app runs.
* **Cluster mode:** Accessed through the Spark masterâ€™s URL or your cluster manager (YARN, Kubernetes, Databricks, etc.).
* After the job ends, itâ€™s available in **Spark History Server** (if enabled).

---

## **2ï¸âƒ£ Key Tabs in Spark UI**

| **Tab**         | **What It Shows**                                                                  |
| --------------- | ---------------------------------------------------------------------------------- |
| **Jobs**        | High-level list of Spark jobs with status, duration, and links to stages.          |
| **Stages**      | Breakdown of each job into stages; shows task counts, shuffle read/write, GC time. |
| **Tasks**       | Details per task (executor ID, duration, input size, errors).                      |
| **Storage**     | RDD/DataFrame caching info (size, partitions).                                     |
| **Environment** | Spark configuration, JVM settings, classpath info.                                 |
| **Executors**   | Memory and CPU usage per executor; task success/failure rates.                     |
| **SQL**         | (For Spark SQL) Logical and physical query plans, execution time, metrics.         |

---

## **3ï¸âƒ£ Why Itâ€™s Useful**

* **Debugging**: Find slow stages, skewed data, or failed tasks.
* **Performance tuning**: See if jobs are waiting on shuffle, spilling to disk, or running out of memory.
* **Memory monitoring**: Track GC time, storage usage, executor memory pressure.
* **Execution tracing**: View DAG (Directed Acyclic Graph) visualization of transformations.

---

## **4ï¸âƒ£ Example Workflow**

Letâ€™s say your Spark job is slow:

1. Open Spark UI â†’ **Jobs tab** â†’ Identify the slow job.
2. Click into **Stages** â†’ See if one stage is much slower.
3. Check **Tasks** â†’ Maybe one task took 2 minutes while others took 5 seconds (data skew!).
4. Look at **Executors** â†’ Some executors might be idle while others are overloaded.
5. Tune partitioning, memory, or parallelism accordingly.

---

ğŸ’¡ In short â€” **Spark UI is your real-time X-ray machine for Spark jobs**. Without it, tuning performance or finding bottlenecks is like driving at night with no headlights.

---

# Apache Spark Docs

https://spark.apache.org/docs/latest/web-ui.html

Apache Spark provides a suite of web user interfaces (UIs) that you can use to monitor the status and resource consumption of your Spark cluster.

Hereâ€™s your **Jobs Tab** section rewritten with icons for better readability:

---

## ğŸ“Š **Jobs Tab**

The **Jobs** tab provides two key views:

* **ğŸ“‹ Summary Page** â€“ Shows high-level info like status, duration, progress of all jobs, and an overall event timeline.
* **ğŸ” Details Page** â€“ Opens when you click a job, displaying the event timeline, DAG visualization, and all stages for that specific job.

---

### **ğŸ“Œ Information Displayed**

* ğŸ‘¤ **User** â€“ Current Spark user
* â° **Started At** â€“ Startup time of the Spark application
* â³ **Total Uptime** â€“ Time since the Spark application started
* ğŸ—‚ **Scheduling Mode** â€“ Job scheduling strategy used
* ğŸ“ˆ **Number of Jobs (by Status)**:

  * ğŸŸ¢ **Active**
  * âœ… **Completed**
  * ğŸ”´ **Failed**

<img width="230" height="191" alt="image" src="https://github.com/user-attachments/assets/6eb47f7d-9d47-4e8a-a5cf-336e58c65fa2" />



* Event timeline: Displays in chronological order the events related to the executors (added, removed) and the jobs

<img width="1920" height="521" alt="image" src="https://github.com/user-attachments/assets/80d6d0ab-945e-4b43-b0b7-41d4c85e618c" />



* Details of jobs grouped by status: Displays detailed information of the jobs including Job ID, description (with a link to detailed job page), submitted time, duration, stages summary and tasks progress bar

<img width="1328" height="496" alt="image" src="https://github.com/user-attachments/assets/c4f3671a-c291-48d6-997b-b5c5e83c7a97" />



* When you click on a specific job, you can see the detailed information of this job.

---

### ğŸ” **Jobs Detail**

This page shows details for a specific job identified by its **Job ID**.

---

### **ğŸ“Œ Job Information**

* ğŸš¦ **Job Status** â€“ `Running` | `Succeeded` | `Failed`
* ğŸ“Š **Number of Stages (by Status)** â€“ `Active` | `Pending` | `Completed` | `Skipped` | `Failed`
* ğŸ“ **Associated SQL Query** â€“ Link to the **SQL** tab for this job
* ğŸ•’ **Event Timeline** â€“ Chronological list of events related to:
  * âš™ **Executors** â€“ Added / Removed
  * ğŸ“¦ **Stages** â€“ Submitted / Completed / Failed

<img width="1431" height="479" alt="image" src="https://github.com/user-attachments/assets/1822036f-0ddf-4bd0-b0ec-24dff5cfc630" />

---

### **ğŸ“ˆ DAG Visualization**

* ğŸ—º **Representation** â€“ Directed Acyclic Graph where:

  * ğŸ”µ **Vertices** = RDDs or DataFrames
  * ğŸ”— **Edges** = Operations applied on RDDs
* ğŸ§® **Example** â€“ `sc.parallelize(1 to 100).toDF.count()`

<img width="569" height="522" alt="image" src="https://github.com/user-attachments/assets/4dbeb741-5a13-4e17-91db-1defbfa62efe" />

---

### **ğŸ“‹ List of Stages** *(Grouped by State)*

`Active` | `Pending` | `Completed` | `Skipped` | `Failed`

For each stage:

* ğŸ†” **Stage ID**
* ğŸ“ **Description**
* ğŸ“… **Submitted Timestamp**
* â³ **Duration**
* ğŸ“Š **Tasks Progress Bar**
* ğŸ“¥ **Input** â€“ Bytes read from storage in this stage
* ğŸ“¤ **Output** â€“ Bytes written to storage in this stage
* ğŸ”„ **Shuffle Read** â€“ Total shuffle bytes & records read (local + remote)
* ğŸ”€ **Shuffle Write** â€“ Bytes & records written to disk for a future shuffle stage

<img width="1356" height="235" alt="image" src="https://github.com/user-attachments/assets/d1801dd3-eb50-45ed-be46-a9102dc0d814" />

---

## ğŸ¯ **Stages Tab**

The **Stages** tab displays the current state of all stages from all jobs in the Spark application.

---

### **ğŸ“Œ Summary View**

* ğŸ“Š **Stage Counts (by Status)** â€“ `Active` | `Pending` | `Completed` | `Skipped` | `Failed`

<img width="332" height="159" alt="image" src="https://github.com/user-attachments/assets/99b59dfc-def4-4231-a77b-09c6152c7afd" />

* ğŸ—‚ **Fair Scheduling Mode** â€“ Displays **Pool Properties** table

<img width="1348" height="184" alt="image" src="https://github.com/user-attachments/assets/e66032ab-664c-4429-b61d-d7eb2337e7ad" />

* ğŸ” **Stages per Status** â€“ Detailed stage lists by state:
  * In **Active** stages â¡ï¸ Can **Kill** stage via link
  * In **Failed** stages â¡ï¸ Failure reason is displayed
  * Click stage **Description** â¡ï¸ Opens task details

<img width="1323" height="550" alt="image" src="https://github.com/user-attachments/assets/71578dc9-8d24-46cd-9366-871a6ca5ec52" />

---

### **ğŸ“„ Stage Detail View**

#### **ğŸ†” Stage Header**

* â³ **Total Time Across All Tasks**
* ğŸŒ **Locality Level Summary**
* ğŸ”„ **Shuffle Read Size / Records**
* ğŸ”— **Associated Job IDs**

<img width="441" height="178" alt="image" src="https://github.com/user-attachments/assets/13e4cf67-6cc6-4d48-8694-6c010655eb0a" />

---

#### **ğŸ“ˆ DAG Visualization**

* ğŸ”µ **Vertices** â€“ Represent RDDs or DataFrames
* ğŸ”— **Edges** â€“ Operations applied on RDDs
* ğŸ“¦ **Node Grouping** â€“ By operation scope (e.g., `BatchScan`, `WholeStageCodegen`, `Exchange`)
* ğŸ· **Annotations** â€“ Whole Stage Code Generation ops include **Codegen ID**
* ğŸ” **Cross-Reference** â€“ SQL/DataFrame stages link to SQL Tabâ€™s execution plans

<img width="562" height="661" alt="image" src="https://github.com/user-attachments/assets/ce9e297a-2255-4349-b784-1323b9faa0a2" />

---

### **ğŸ“Š Stage Metrics Summary**

* ğŸ•µ **Tasks Deserialization Time** â€“ Time to deserialize task
* â± **Duration** â€“ Total execution time of tasks
* ğŸ—‘ **GC Time** â€“ Total JVM Garbage Collection time
* ğŸ“¦ **Result Serialization Time** â€“ Time spent serializing task result before sending to driver
* ğŸ“¤ **Getting Result Time** â€“ Time driver spends fetching results from workers
* â³ **Scheduler Delay** â€“ Wait time before task execution
* ğŸ§  **Peak Execution Memory** â€“ Max memory used during shuffles, aggregations, joins
* ğŸ“¥ **Shuffle Read Size / Records** â€“ Includes local + remote reads
* â³ **Shuffle Read Fetch Wait Time** â€“ Time blocked waiting for shuffle data from remote executors
* ğŸŒ **Shuffle Remote Reads** â€“ Bytes read from remote executors
* ğŸ“¤ **Shuffle Write Time** â€“ Time spent writing shuffle data
* ğŸ’¾ **Shuffle Spill (Memory)** â€“ Deserialized shuffle data size in memory
* ğŸ’¿ **Shuffle Spill (Disk)** â€“ Serialized shuffle data size on disk

<img width="1907" height="865" alt="image" src="https://github.com/user-attachments/assets/cca04c1f-90ba-4f5e-a387-4a6209b5e97f" />

---

### **ğŸ“Š Aggregated Metrics by Executor**

Shows same metrics as above but aggregated **per executor**.

<img width="1332" height="176" alt="image" src="https://github.com/user-attachments/assets/90793eda-5d70-4eaa-98f6-012eac11a472" />

---

### **ğŸ“¦ Accumulators**

* ğŸ”„ Shared variables updated inside transformations
* ğŸ· Only **Named Accumulators** are displayed in UI

<img width="1334" height="140" alt="image" src="https://github.com/user-attachments/assets/44b738fb-9319-4735-8a97-4403f9f32dde" />

---

### **ğŸ“ Task Details**

* Displays same metrics as summary but **per task**
* ğŸ“œ Includes **Log Links** and **Task Attempt Number** (on failure)
* ğŸ“Š Shows **Accumulator Value** at end of each task (if named)

<img width="1328" height="300" alt="image" src="https://github.com/user-attachments/assets/18351c52-bfaa-45e6-bf0f-e3ba45265c2b" />

---

## ğŸ’¾ **Storage Tab**

The **Storage** tab displays the **persisted** `RDDs` and `DataFrames` in the Spark application.

---

### **ğŸ“Œ Summary View**

* ğŸ“¦ **Storage Level** â€“ e.g., `MEMORY_ONLY`, `MEMORY_ONLY_SER`, `DISK_ONLY`
* ğŸ§® **Number of Partitions**
* ğŸ“ **Size** â€“ Total memory/disk usage
* ğŸ’¡ **Note** â€“ Newly persisted RDDs/DataFrames **will not appear** until they are **materialized** via an **action** (e.g., `.count()`, `.collect()`)

---

### **ğŸ“œ Example**

```scala
scala> import org.apache.spark.storage.StorageLevel._
import org.apache.spark.storage.StorageLevel._

scala> val rdd = sc.range(0, 100, 1, 5).setName("rdd")
rdd: org.apache.spark.rdd.RDD[Long] = rdd MapPartitionsRDD[1] at range at <console>:27

scala> rdd.persist(MEMORY_ONLY_SER)
res0: rdd.type = rdd MapPartitionsRDD[1] at range at <console>:27

scala> rdd.count
res1: Long = 100

scala> val df = Seq((1, "andy"), (2, "bob"), (2, "andy")).toDF("count", "name")
df: org.apache.spark.sql.DataFrame = [count: int, name: string]

scala> df.persist(DISK_ONLY)
res2: df.type = [count: int, name: string]

scala> df.count
res3: Long = 3
```

ğŸ“Š **Result** â€“ After running this code, the Storage tab will list:

1. **RDD: `rdd`** â€“ Persisted in `MEMORY_ONLY_SER`
2. **DataFrame: `df`** â€“ Persisted in `DISK_ONLY`

<img width="1892" height="210" alt="image" src="https://github.com/user-attachments/assets/89eb0431-7f0c-4d56-89e6-cd78dd8e7b9c" />

---

### **ğŸ“„ Detail View**

When you click an RDD/DataFrame name (e.g., **`rdd`**), you see:

* ğŸ“ **Data Distribution** across the cluster
* âš™ **Executors** storing each partition
* ğŸ“ **Partition Sizes**
* ğŸ’¾ **Memory/Disk Overhead** per partition

<img width="1900" height="605" alt="image" src="https://github.com/user-attachments/assets/eb9af63f-41d3-4be9-9a3f-f80715a093f3" />

---

## âš™ **Environment Tab**

The **Environment** tab displays values for various **environment** and **configuration variables**, including **JVM**, **Spark**, and **system properties**. It is useful for verifying if your properties are set correctly.

---

<img width="1880" height="843" alt="image" src="https://github.com/user-attachments/assets/6dc50491-8b11-4014-aae5-840a84ab5a7b" />

### **ğŸ“Œ Sections in Environment Tab**

1. ğŸƒ **Runtime Information**

   * Versions of **Java**, **Scala**, and other runtime details

2. ğŸš€ **Spark Properties**

   * Application settings such as:

     * `spark.app.name`
     * `spark.driver.memory`
   * âš  Properties with prefix `spark.hadoop.*` are shown **here**, not in Hadoop Properties

3. ğŸ—‚ **Hadoop Properties** *(clickable link)*

   * Shows properties related to **Hadoop** and **YARN**

<img width="1228" height="389" alt="image" src="https://github.com/user-attachments/assets/f4a958c6-b7a9-499e-97c9-3abc79fd78ae" />

4. ğŸ–¥ **System Properties**

   * Detailed **JVM** properties

<img width="1167" height="330" alt="image" src="https://github.com/user-attachments/assets/81fc067e-a4ee-47e8-bd08-1b92fbd38702" />

5. ğŸ“š **Classpath Entries**

   * Lists all loaded classes and their sources
   * ğŸ” Useful for **debugging class conflicts**

<img width="1079" height="213" alt="image" src="https://github.com/user-attachments/assets/4e0165ab-48c2-49c2-86a4-93b12c52a56f" />

---

## ğŸ›  **Executors Tab**

The **Executors** tab displays **summary information** about all executors created for the Spark application, including **memory/disk usage**, **task statistics**, and **shuffle metrics**.

<img width="2466" height="1354" alt="image" src="https://github.com/user-attachments/assets/abf86b1b-06f2-480b-830b-d957b67a9dc4" />

---

### **ğŸ“Œ Information Displayed**

* ğŸ’¾ **Storage Memory** â€“ Memory used & reserved for caching data
* ğŸ§® **Resource Usage** â€“ Memory, disk, and cores used by each executor
* ğŸ“Š **Performance Metrics** â€“ GC time, shuffle read/write statistics

---

### **ğŸ” Additional Tools**

* ğŸ“œ **Stderr Log**

  * Clicking the **`stderr`** link for **executor 0** opens its **standard error log** in the console

<img width="2510" height="964" alt="image" src="https://github.com/user-attachments/assets/856b6ee6-7105-44f3-ac7f-44ad8e0b0296" />

* ğŸ§µ **Thread Dump**

  * Clicking the **`Thread Dump`** link for **executor 0** shows the **JVM thread dump** for that executor
  * ğŸ›  Useful for **performance troubleshooting** and identifying blocked threads

<img width="997" height="563" alt="image" src="https://github.com/user-attachments/assets/98489aba-ba15-4f72-ab6d-c31885aafd50" />

---

Hereâ€™s your **SQL Tab** section rewritten with icons and consistent formatting so it matches the previous Spark UI tab descriptions:

---

## ğŸ—„ **SQL Tab**

The **SQL** tab appears when the application executes **Spark SQL queries**. It provides details about **query execution time**, **jobs**, and **plans** (logical & physical) along with the **execution DAG**.

---

### **ğŸ“Œ Example**

```scala
val df = Seq((1, "andy"), (2, "bob"), (2, "andy")).toDF("count", "name")
df.count
df.createGlobalTempView("df")

spark.sql("select name, sum(count) from global_temp.df group by name").show
```

ğŸ“Š After running the above, the SQL tab lists the **three DataFrame/SQL operations**.
Clicking **`show at <console>: 24`** for the last query opens its **DAG** and **execution details**.

---

### **ğŸ“ˆ Query Details Page**

* â± **Execution Time & Duration**
* ğŸ”— **Associated Jobs**
* ğŸ—º **SQL Execution DAG** â€“ Blocks represent execution stages:

  * ğŸ§® **WholeStageCodegen (1)** â€“ Combines multiple operators (e.g., `LocalTableScan`, `HashAggregate`) into a single compiled Java function for performance

    * Metrics: **Number of Rows**, **Spill Size**
    * `(1)` = Code generation ID
  * ğŸ”€ **Exchange** â€“ Shows shuffle metrics such as written records & data size

---

### **ğŸ›  Logical & Physical Plans**

* ğŸ“œ **Logical Plan** â€“ Shows how Spark parses & optimizes the query
* âš™ **Physical Plan** â€“ Shows execution steps; whole-stage codegen steps are prefixed with `*(ID)`

  * Example: `*(1) LocalTableScan`

---

### **ğŸ“Š SQL Metrics Overview**

SQL metrics are shown in the **physical operator blocks** and help in analyzing execution:

| ğŸ“ Metric                                | ğŸ“ Meaning                                 | ğŸ›  Operators                                 |
| ---------------------------------------- | ------------------------------------------ | -------------------------------------------- |
| ğŸ“„ **Number of Output Rows**             | Rows produced by operator                  | Aggregate, Join, Filter, Scan, etc.          |
| ğŸ“¦ **Data Size**                         | Size of broadcast/shuffled/collected data  | BroadcastExchange, ShuffleExchange, Subquery |
| â± **Time to Collect**                    | Time to gather data                        | BroadcastExchange, Subquery                  |
| ğŸ” **Scan Time**                         | Time spent scanning data                   | ColumnarBatchScan, FileSourceScan            |
| ğŸ—‚ **Metadata Time**                     | Time fetching metadata (partitions, files) | FileSourceScan                               |
| ğŸ“¤ **Shuffle Bytes Written**             | Bytes written during shuffle               | CollectLimit, ShuffleExchange                |
| ğŸ“ **Shuffle Records Written**           | Records written during shuffle             | CollectLimit, ShuffleExchange                |
| â³ **Shuffle Write Time**                 | Time writing shuffle data                  | CollectLimit, ShuffleExchange                |
| ğŸ“¦ **Remote Blocks Read**                | Blocks read from remote                    | CollectLimit, ShuffleExchange                |
| ğŸŒ **Remote Bytes Read**                 | Bytes read from remote                     | CollectLimit, ShuffleExchange                |
| ğŸ’¾ **Remote Bytes Read to Disk**         | Bytes moved from remote to local disk      | CollectLimit, ShuffleExchange                |
| ğŸ“¥ **Local Blocks Read**                 | Blocks read locally                        | CollectLimit, ShuffleExchange                |
| ğŸ“ **Local Bytes Read**                  | Bytes read locally                         | CollectLimit, ShuffleExchange                |
| â³ **Fetch Wait Time**                    | Time waiting for data fetch                | CollectLimit, ShuffleExchange                |
| ğŸ“Š **Records Read**                      | Number of records read                     | CollectLimit, ShuffleExchange                |
| â± **Sort Time**                          | Time sorting data                          | Sort                                         |
| ğŸ§  **Peak Memory**                       | Max memory used                            | Sort, HashAggregate                          |
| ğŸ’¿ **Spill Size**                        | Data spilled to disk                       | Sort, HashAggregate                          |
| ğŸ›  **Time in Aggregation Build**         | Time building aggregation hash             | HashAggregate                                |
| ğŸ” **Avg Hash Probe Bucket List Iters**  | Avg bucket list iterations during probe    | HashAggregate                                |
| ğŸ“¦ **Data Size of Build Side**           | Size of built hash map                     | ShuffledHashJoin                             |
| â³ **Time to Build Hash Map**             | Time building hash map                     | ShuffledHashJoin                             |
| ğŸ“ **Task Commit Time**                  | Time committing task output                | File-based writes                            |
| ğŸ“œ **Job Commit Time**                   | Time committing job output                 | File-based writes                            |
| ğŸ“¤ **Data Sent to Python Workers**       | Bytes sent to Python worker                | Python UDFs, Pandas API                      |
| ğŸ“¥ **Data Returned from Python Workers** | Bytes returned from Python worker          | Python UDFs, Pandas API                      |

---
