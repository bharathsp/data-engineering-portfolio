## **PySpark DataFrame Options & Methods**

---

### üßæ **1. `header=True`**

**Definition**: Instructs Spark to treat the first line of the CSV file as column names.

**Syntax**:

```python
df = spark.read.csv("data.csv", header=True)
```

**Example Input File (`data.csv`)**:

```
Name,Age
Alice,30
Bob,25
```

**Output DataFrame**:

```text
+-----+---+
|Name |Age|
+-----+---+
|Alice| 30|
|Bob  | 25|
+-----+---+
```

---

### üßæ **2. `inferSchema=True`**

**Definition**: Automatically infers the data types of columns based on the input data.

**Syntax**:

```python
df = spark.read.csv("data.csv", header=True, inferSchema=True)
```

**Example**:
Without `inferSchema`, all fields are strings.
With it, `Age` becomes `IntegerType`.

---

### üßæ **3. `schema=...`**

**Definition**: Define a schema explicitly using `StructType` and `StructField`.

**Syntax**:

```python
from pyspark.sql.types import StructType, StructField, StringType, IntegerType

schema = StructType([
    StructField("Name", StringType(), True),
    StructField("Age", IntegerType(), True)
])

df = spark.read.csv("data.csv", header=True, schema=schema)
```

---

### üßæ **3. `printSchema()`**

**Definition**:
Prints the **schema** (i.e., structure) of a DataFrame in a **tree format** showing column names, data types, and nullability.

**Syntax**:

```python
df.printSchema()
```

**Example Input DataFrame**:

```python
+-------+---+------------+
| Name  |Age| JoinDate   |
+-------+---+------------+
| Alice | 30| 2022-01-01 |
| Bob   | 25| 2021-05-10 |
+-------+---+------------+
```

**Code**:

```python
df.printSchema()
```

**Output**:

```text
root
 |-- Name: string (nullable = true)
 |-- Age: integer (nullable = true)
 |-- JoinDate: string (nullable = true)
```

---

### üßæ **4. sep=...**

**Definition**:
Specifies the **delimiter** used in the CSV file (default is comma `,`). Useful for reading **TSV**, **pipe-separated**, or any custom-delimited files.

**Syntax**:

```python
df = spark.read.csv("data.tsv", header=True, sep="\t")
```

**Example**:
Assume a tab-separated file (`data.tsv`) with the following content:

```
Name	Age
Alice	30
Bob	25
```

**Code**:

```python
df = spark.read.csv("data.tsv", header=True, sep="\t")
df.show()
```

**Output**:

```text
+-----+---+
|Name |Age|
+-----+---+
|Alice| 30|
|Bob  | 25|
+-----+---+
```

You can replace `sep="\t"` with other delimiters as needed:

* For pipe `|` separated: `sep="|"`
* For semicolon separated: `sep=";"`
* For custom characters: `sep="#"`

---

### üßæ **4. `show(n)`**

**Definition**: Displays the top `n` rows of the DataFrame (default: 20).

**Syntax**:

```python
df.show(2)
```

**Output**:

```text
+-----+---+
|Name |Age|
+-----+---+
|Alice| 30|
|Bob  | 25|
+-----+---+
```


---

### üßæ **5. `withColumn()`**

**Definition**: Adds a new column or replaces an existing one.

**Syntax**:

```python
from pyspark.sql.functions import col
df.withColumn("AgePlus5", col("Age") + 5).show()
```

**Output**:

```text
+-----+---+--------+
|Name |Age|AgePlus5|
+-----+---+--------+
|Alice| 30|     35 |
|Bob  | 25|     30 |
+-----+---+--------+
```

---

### üßæ **5. `withColumnRenamed()`**

**Definition**:
Renames an **existing column** in a DataFrame.

**Syntax**:

```python
df.withColumnRenamed("oldColumnName", "newColumnName")
```

**Example Input DataFrame**:

```text
+-------+---+
| Name  |Age|
+-------+---+
| Alice | 30|
| Bob   | 25|
+-------+---+
```

**Code**:

```python
df_renamed = df.withColumnRenamed("Name", "EmployeeName")
df_renamed.show()
```

**Output**:

```text
+-------------+---+
|EmployeeName |Age|
+-------------+---+
|Alice        | 30|
|Bob          | 25|
+-------------+---+
```

---

### üßæ **6. `select()`**

**Definition**: Selects one or more columns from the DataFrame.

**Syntax**:

```python
df.select("Name").show()
```

**Output**:

```text
+-----+
|Name |
+-----+
|Alice|
|Bob  |
+-----+
```

---

### üßæ **13. `selectExpr()`**

**Definition**:
Allows you to **run SQL expressions** on DataFrame columns using string expressions ‚Äî combining the power of `select()` with SQL syntax.


**Syntax**:

```python
df.selectExpr("column1", "column2 as new_column", "expression as alias")
```

**Example Input DataFrame**:

```text
+-------+---+
| Name  |Age|
+-------+---+
| Alice | 30|
| Bob   | 25|
+-------+---+
```

**Code**:

```python
df.selectExpr("Name", "Age + 5 as AgePlus5").show()
```

**Output**:

```text
+-------+---------+
| Name  | AgePlus5|
+-------+---------+
| Alice |      35 |
| Bob   |      30 |
+-------+---------+
```

üîπ Use Cases:

* Apply transformations like arithmetic directly (`"Salary * 1.1 as UpdatedSalary"`)
* Rename columns (`"Age as age_in_years"`)
* Use string expressions in a concise, SQL-like format
* Avoid using `withColumn()` multiple times

---

### üßæ **7. `filter()` / `where()`**

**Definition**: Filters rows based on a boolean condition.

**Syntax**:

```python
from pyspark.sql.functions import col

df.filter(col("Age") > 25).show()
# OR
df.where("Age > 25").show()
```

**Output**:

```text
+-----+---+
|Name |Age|
+-----+---+
|Alice| 30|
+-----+---+
```

---

### üßæ **8. `groupBy()`**

**Definition**: Groups rows by one or more columns for aggregation.

**Syntax**:

```python
df.groupBy("Name").count().show()
```

**Input**:

```text
+-----+---+
|Name |Age|
+-----+---+
|Alice| 30|
|Alice| 25|
|Bob  | 25|
+-----+---+
```

**Output**:

```text
+-----+-----+
|Name |count|
+-----+-----+
|Alice|  2  |
|Bob  |  1  |
+-----+-----+
```

---

### üßæ **9. `agg()`**

**Definition**: Applies one or more aggregation functions after `groupBy`.

**Syntax**:

```python
from pyspark.sql.functions import sum, avg

df.groupBy("Name").agg(sum("Age").alias("TotalAge"), avg("Age").alias("AvgAge")).show()
```

**Output**:

```text
+-----+--------+-------+
|Name |TotalAge|AvgAge |
+-----+--------+-------+
|Alice|   55   | 27.5  |
|Bob  |   25   | 25.0  |
+-----+--------+-------+
```

---

### üßæ **10. `orderBy()` / `sort()`**

**Definition**: Sorts rows by one or more columns.

**Syntax**:

```python
df.orderBy("Age").show()
df.orderBy(col("Age").desc()).show()

# Sort by a single column (ascending by default)
df.sort("Age").show()
df.sort(df.Age).show()

# Sort by multiple columns
df.sort("Age", "Name").show()

# Sort by column in descending order
df.sort(col("Age").desc()).show()
df.sort(df.Age.desc()).show()

# Mixed ascending and descending
df.sort(col("Age").desc(), col("Name").asc()).show()
```

**Output** (Ascending):

```text
+-----+---+
|Name |Age|
+-----+---+
|Bob  | 25|
|Alice| 30|
+-----+---+
```

---

### üîó **10.`union()`**

**Definition**: Combines the rows of two DataFrames with the same schema (same number of columns and matching data types).


**Syntax**:

```python
df1 = spark.createDataFrame([("Alice", 25), ("Bob", 30)], ["Name", "Age"])
df2 = spark.createDataFrame([("Carol", 22), ("Dave", 28)], ["Name", "Age"])

# Union of df1 and df2
df1.union(df2).show()
```


**Output**:

```
+-----+---+
|Name |Age|
+-----+---+
|Alice|25 |
|Bob  |30 |
|Carol|22 |
|Dave |28 |
+-----+---+
```

Sure! Here's the explanation and syntax for **`distinct()`** in PySpark, following the same format:

---

### üßæ `distinct()`

**Definition**: Removes **duplicate rows** from a DataFrame.

**Syntax**:

```python
df = spark.createDataFrame([
    ("Alice", 25),
    ("Bob", 30),
    ("Alice", 25)
], ["Name", "Age"])

# Remove duplicate rows
df.distinct().show()
```

**Output**:

```
+-----+---+
|Name |Age|
+-----+---+
|Alice|25 |
|Bob  |30 |
+-----+---+
```

---

### üßæ **11. `drop()`**

**Definition**: Drops a specified column from the DataFrame.

**Syntax**:

```python
df.drop("Age").show()
```

**Output**:

```text
+-----+
|Name |
+-----+
|Alice|
|Bob  |
+-----+
```

---

### üßæ `dropDuplicates()`

**Definition**: Removes duplicate rows based on **specific column(s)**. Unlike `distinct()`, which removes full-row duplicates, `dropDuplicates()` allows you to specify which columns to consider for identifying duplicates.

**Syntax**:

```python
df = spark.createDataFrame([
    ("Alice", 25),
    ("Bob", 30),
    ("Alice", 30),
    ("Alice", 25)
], ["Name", "Age"])

# Drop duplicates based only on the "Name" column
df.dropDuplicates(["Name"]).show()
```

**Output**:

```
+-----+---+
|Name |Age|
+-----+---+
|Alice|25 |
|Bob  |30 |
+-----+---+
```

> Only the first occurrence of each unique `Name` is retained.

---

### üßæ **12. `na.fill()` / `na.drop()` / `na.replace()`**

**Definition**: Handle missing (`null`) values.

#### ‚úÖ `na.fill(value)`

```python
df.na.fill(0).show()
```

#### ‚úÖ `na.drop()`

```python
df.na.drop().show()
```

#### ‚úÖ `na.replace()`

```python
df.na.replace("NA", None).show()
```

**Input**:

```text
+-----+----+
|Name |Age |
+-----+----+
|John | 25 |
|Jane |null|
|NA   | 40 |
+-----+----+
```

**Output (using `fill`)**:

```text
+-----+----+
|Name |Age |
+-----+----+
|John | 25 |
|Jane |  0 |
|NA   | 40 |
+-----+----+
```

---

### üßæ `join()`

**Definition**: Combines two DataFrames based on a **common column or condition** (similar to SQL joins).

**Syntax**:

```python
df1 = spark.createDataFrame([
    (1, "Alice"), 
    (2, "Bob")
], ["id", "name"])

df2 = spark.createDataFrame([
    (1, "HR"), 
    (2, "Finance")
], ["id", "department"])

# Inner join on "id"
df1.join(df2, on="id", how="inner").show()
```

**üì§ Output**:

```
+---+-----+---------+
|id |name |department|
+---+-----+---------+
|1  |Alice|HR       |
|2  |Bob  |Finance  |
+---+-----+---------+
```

**üîÅ Join Types**:

| Type       | `how` value   | Description                                      |
| ---------- | ------------- | ------------------------------------------------ |
| Inner Join | `"inner"`     | Rows with matching keys in both DataFrames       |
| Left Join  | `"left"`      | All rows from left, with matching right          |
| Right Join | `"right"`     | All rows from right, with matching left          |
| Full Outer | `"outer"`     | All rows from both, matched where possible       |
| Left Semi  | `"left_semi"` | Only rows from left that have matches in right   |
| Left Anti  | `"left_anti"` | Only rows from left **without** matches in right |

**üß† Notes**:

* Use `on="column"` or `on=["col1", "col2"]` for multiple join keys.
* You can also specify join conditions using expressions:

  ```python
  df1.join(df2, df1.id == df2.id, "inner")
  ```

---

## **PySpark SQL functions**

---

### üìå 1. `col()`

**Description**: Refers to a column in a DataFrame; used for transformations.

**Syntax**:

```python
from pyspark.sql.functions import col
df.select(col("column_name"))
```

**Example**:
Input:

```python
+-------+-----+
| Name  | Age |
+-------+-----+
| Alice | 30  |
| Bob   | 25  |
+-------+-----+
```

Code:

```python
df.select(col("Name")).show()
```

Output:

```python
+-------+
|  Name |
+-------+
| Alice |
|  Bob  |
+-------+
```

---

### üìå 2. `lit()`

**Description**: Adds a constant/literal value as a new column.

**Syntax**:

```python
from pyspark.sql.functions import lit
df.withColumn("Country", lit("India"))
```

**Example**:
Input:

```python
+-----+
| Name|
+-----+
| John|
+-----+
```

Output:

```python
+-----+--------+
| Name| Country|
+-----+--------+
| John|  India |
+-----+--------+
```

---

### üìå 3. `when()`, `otherwise()`

**Description**: Used for conditional logic similar to if-else.

**Syntax**:

```python
from pyspark.sql.functions import when, col
df.withColumn("Status", when(col("Age") >= 18, "Adult").otherwise("Minor"))
```

**Example**:
Input:

```python
+-------+-----+
| Name  | Age |
+-------+-----+
| Alice | 30  |
| Bob   | 15  |
+-------+-----+
```

Output:

```python
+-------+-----+--------+
| Name  | Age | Status |
+-------+-----+--------+
| Alice | 30  | Adult  |
| Bob   | 15  | Minor  |
+-------+-----+--------+
```

---

### üìå 4. `sum()`, `avg()`, `count()`, `max()`, `min()`

**Description**: Aggregation functions.

**Syntax**:

```python
from pyspark.sql.functions import sum, avg, count, max, min
df.groupBy("Department").agg(sum("Salary"), avg("Salary"))
```

**Example**:
Input:

```python
+----------+------+
|Department|Salary|
+----------+------+
|   IT     | 5000 |
|   IT     | 7000 |
+----------+------+
```

Output:

```python
+----------+-----------+-----------+
|Department|sum(Salary)|avg(Salary)|
+----------+-----------+-----------+
|   IT     | 12000     | 6000.0    |
+----------+-----------+-----------+
```

---

### üìå 5. `lower()`, `upper()`

**Description**: Converts string column values to lowercase or uppercase.

**Syntax**:

```python
from pyspark.sql.functions import lower, upper
df.withColumn("LowerName", lower(col("Name")))
```

**Example**:
Input:

```python
+-------+
| Name  |
+-------+
| Alice |
+-------+
```

Output:

```python
+-------+----------+
| Name  |LowerName |
+-------+----------+
| Alice |  alice   |
+-------+----------+
```

---

### üìå 6. `regexp_replace()`

**Description**: Replaces substrings in a column using a regex.

**Syntax**:

```python
from pyspark.sql.functions import regexp_replace
df.withColumn("Cleaned", regexp_replace(col("Text"), "[^a-zA-Z ]", ""))
```

**Example**:
Input:

```python
+---------------+
|     Text      |
+---------------+
| Hello, World! |
+---------------+
```

Output:

```python
+---------------+-----------+
|     Text      |  Cleaned  |
+---------------+-----------+
| Hello, World! | Hello World |
+---------------+-----------+
```

---

### üìå 7. `concat()`, `concat_ws()`

**Description**: Concatenates multiple columns (with or without separator).

**Syntax**:

```python
from pyspark.sql.functions import concat, concat_ws
df.withColumn("FullName", concat_ws(" ", col("First"), col("Last")))
```

**Example**:
Input:

```python
+-------+--------+
| First |  Last  |
+-------+--------+
| John  |  Doe   |
+-------+--------+
```

Output:

```python
+-------+--------+-----------+
| First |  Last  | FullName  |
+-------+--------+-----------+
| John  |  Doe   | John Doe  |
+-------+--------+-----------+
```

---

### üìå 8. `year()`, `month()`, `dayofmonth()`

**Description**: Extract year, month, or day from date column.

**Syntax**:

```python
from pyspark.sql.functions import year, month, dayofmonth
df.withColumn("Year", year(col("Date")))
```

**Example**:
Input:

```python
+----------+
|   Date   |
+----------+
|2023-05-12|
+----------+
```

Output:

```python
+----------+------+
|   Date   | Year |
+----------+------+
|2023-05-12| 2023 |
+----------+------+
```

---

### üìå 9. `to_date()`, `date_format()`

**Description**:

* `to_date()`: Converts string to DateType.
* `date_format()`: Formats a date column into a string.

**Syntax**:

```python
from pyspark.sql.functions import to_date, date_format
df.withColumn("Formatted", date_format(to_date(col("Date")), "MMM-yyyy"))
```

**Example**:
Input:

```python
+-------------+
|     Date    |
+-------------+
| 12-05-2023  |
+-------------+
```

Output:

```python
+-------------+-----------+
|     Date    | Formatted |
+-------------+-----------+
| 12-05-2023  | May-2023  |
+-------------+-----------+
```

---

### üìå 10. `isnull()`, `isnotnull()`

**Description**: Used to filter or identify null values.

**Syntax**:

```python
from pyspark.sql.functions import col
df.filter(col("Age").isNull())
```

**Example**:
Input:

```python
+-----+-----+
|Name | Age |
+-----+-----+
|John | 25  |
|Jane |null |
+-----+-----+
```

Output:

```python
+-----+-----+
|Name | Age |
+-----+-----+
|Jane |null |
+-----+-----+
```

---

### üìå 11. `explode()`

**Description**: Converts array or map column into multiple rows (flattens the array).

**Syntax**:

```python
from pyspark.sql.functions import explode
df.withColumn("Hobby", explode(col("Hobbies")))
```

**Example**:
Input:

```python
+--------+--------------------+
|  Name  |      Hobbies       |
+--------+--------------------+
| Alice  | ["Reading", "Swim"]|
+--------+--------------------+
```

Output:

```python
+--------+--------+
|  Name  | Hobby  |
+--------+--------+
| Alice  |Reading |
| Alice  | Swim   |
+--------+--------+
```

---

## **Spark SQL functions**

---

### üßæ **1. `createOrReplaceTempView("view_name")`**

**Definition**:
Registers a DataFrame as a temporary SQL table (view), which allows SQL queries to be run using `spark.sql()`.

**Syntax**:

```python
df.createOrReplaceTempView("employee_view")
```

> ‚ö†Ô∏è The view exists only during the lifetime of the Spark session.

**Example Input DataFrame**:

```python
+-------+--------+------+
| Name  | Dept   | Age  |
+-------+--------+------+
| Alice | HR     | 30   |
| Bob   | IT     | 25   |
+-------+--------+------+
```

**Usage**:

```python
df.createOrReplaceTempView("employee_view")
result = spark.sql("SELECT * FROM employee_view WHERE Age > 26")
result.show()
```

**Output**:

```text
+-------+-----+---+
| Name  |Dept |Age|
+-------+-----+---+
| Alice | HR  |30 |
+-------+-----+---+
```

---

### üßæ **2. `spark.sql("SQL_QUERY")`**

**Definition**:
Executes SQL queries on registered temporary views or Hive tables.

**Syntax**:

```python
result = spark.sql("SELECT Dept, COUNT(*) as count FROM employee_view GROUP BY Dept")
```

**Example Input View (`employee_view`)**:

```text
+-------+--------+------+
| Name  | Dept   | Age  |
+-------+--------+------+
| Alice | HR     | 30   |
| Bob   | IT     | 25   |
| John  | HR     | 28   |
+-------+--------+------+
```

**Query**:

```python
spark.sql("SELECT Dept, COUNT(*) as count FROM employee_view GROUP BY Dept").show()
```

**Output**:

```text
+-------+-----+
| Dept  |count|
+-------+-----+
| HR    |  2  |
| IT    |  1  |
+-------+-----+
```

---

### üßæ **3. `saveAsTable("table_name")`**

**Definition**:
Saves a DataFrame as a persistent Hive or Spark table, which can be queried using SQL even after the session ends (if Hive support is enabled).

**Syntax**:

```python
df.write.mode("overwrite").saveAsTable("employee_table")
```

> ‚ö†Ô∏è Requires Hive support to be enabled (`enableHiveSupport()` in `SparkSession`)

**Example Input DataFrame**:

```text
+-------+--------+------+
| Name  | Dept   | Age  |
+-------+--------+------+
| Alice | HR     | 30   |
| Bob   | IT     | 25   |
+-------+--------+------+
```

**Usage**:

```python
df.write.mode("overwrite").saveAsTable("employee_table")
```

**Querying it later**:

```python
spark.sql("SELECT * FROM employee_table").show()
```

**Output**:

```text
+-------+--------+-----+
| Name  | Dept   | Age |
+-------+--------+-----+
| Alice | HR     | 30  |
| Bob   | IT     | 25  |
+-------+--------+-----+
```

---

### üì• `spark.read.csv()`

**Definition**: Reads a CSV file into a **DataFrame**.

**Syntax**:

```python
df = spark.read.csv("data.csv", header=True, inferSchema=True)
df.show()
```

**‚öôÔ∏è Common Options**:

| Option             | Description                           |
| ------------------ | ------------------------------------- |
| `header=True`      | Uses the first row as column names    |
| `inferSchema=True` | Automatically infers data types       |
| `sep=","`          | Sets the delimiter (default is comma) |

**Example Output**:

```
+-----+---+
|Name |Age|
+-----+---+
|Alice|25 |
|Bob  |30 |
+-----+---+
```

---

### üì§ `DataFrame.write.csv()`

**Definition**: Writes a **DataFrame** to a CSV file.

**Syntax**:

```python
df.write.csv("output_path", header=True, mode="overwrite")
```

**‚öôÔ∏è Common Options**:

| Option             | Description                       |
| ------------------ | --------------------------------- |
| `header=True`      | Writes column names as header row |
| `mode="overwrite"` | Overwrites existing output        |
| `sep=","`          | Sets delimiter (default is comma) |

* Writes to folder `output_path/` with files like:

---

### ü™ü **Window Functions**

**Definition**: Perform calculations across a set of rows **related to the current row**, without collapsing the result like a `groupBy` does. Commonly used for ranking, cumulative totals, moving averages, etc.

**‚úÖ Setup Syntax**:

```python
from pyspark.sql.window import Window
from pyspark.sql.functions import row_number, rank, dense_rank, sum

# Define a window partitioned by "Department" and ordered by "Salary"
window_spec = Window.partitionBy("Department").orderBy("Salary")
```

**üìå Common Window Functions**:

| Function       | Description                               | Syntax Example                    |
| -------------- | ----------------------------------------- | --------------------------------- |
| `row_number()` | Assigns a unique row number per partition | `row_number().over(window_spec)`  |
| `rank()`       | Gives rank with gaps                      | `rank().over(window_spec)`        |
| `dense_rank()` | Gives rank without gaps                   | `dense_rank().over(window_spec)`  |
| `sum()`        | Running total or partition sum            | `sum("Salary").over(window_spec)` |

**üß™ Example**:

```python
data = [
    ("Alice", "HR", 3000),
    ("Bob", "HR", 4000),
    ("Charlie", "IT", 5000),
    ("David", "IT", 4500),
    ("Eve", "HR", 3500)
]
df = spark.createDataFrame(data, ["Name", "Department", "Salary"])

from pyspark.sql.functions import row_number
window_spec = Window.partitionBy("Department").orderBy("Salary")

df.withColumn("row_num", row_number().over(window_spec)).show()
```

---

**üì§ Output**:

```
+-------+----------+------+--------+
| Name  |Department|Salary|row_num|
+-------+----------+------+--------+
| Alice | HR       | 3000 |   1    |
| Eve   | HR       | 3500 |   2    |
| Bob   | HR       | 4000 |   3    |
| David | IT       | 4500 |   1    |
| Charlie| IT      | 5000 |   2    |
+-------+----------+------+--------+
```

---
