## **PySpark DataFrame Options & Methods**

---

### 🧾 **1. `header=True`**

**Definition**: Instructs Spark to treat the first line of the CSV file as column names.

**Syntax**:

```python
df = spark.read.csv("data.csv", header=True)
```

**Example Input File (`data.csv`)**:

```
Name,Age
Alice,30
Bob,25
```

**Output DataFrame**:

```text
+-----+---+
|Name |Age|
+-----+---+
|Alice| 30|
|Bob  | 25|
+-----+---+
```

---

### 🧾 **2. `inferSchema=True`**

**Definition**: Automatically infers the data types of columns based on the input data.

**Syntax**:

```python
df = spark.read.csv("data.csv", header=True, inferSchema=True)
```

**Example**:
Without `inferSchema`, all fields are strings.
With it, `Age` becomes `IntegerType`.

---

### 🧾 **3. `schema=...`**

**Definition**: Define a schema explicitly using `StructType` and `StructField`.

**Syntax**:

```python
from pyspark.sql.types import StructType, StructField, StringType, IntegerType

schema = StructType([
    StructField("Name", StringType(), True),
    StructField("Age", IntegerType(), True)
])

df = spark.read.csv("data.csv", header=True, schema=schema)
```

---

### 🧾 **3. `printSchema()`**

**Definition**:
Prints the **schema** (i.e., structure) of a DataFrame in a **tree format** showing column names, data types, and nullability.

**Syntax**:

```python
df.printSchema()
```

**Example Input DataFrame**:

```python
+-------+---+------------+
| Name  |Age| JoinDate   |
+-------+---+------------+
| Alice | 30| 2022-01-01 |
| Bob   | 25| 2021-05-10 |
+-------+---+------------+
```

**Code**:

```python
df.printSchema()
```

**Output**:

```text
root
 |-- Name: string (nullable = true)
 |-- Age: integer (nullable = true)
 |-- JoinDate: string (nullable = true)
```

---

### 🧾 **4. sep=...**

**Definition**:
Specifies the **delimiter** used in the CSV file (default is comma `,`). Useful for reading **TSV**, **pipe-separated**, or any custom-delimited files.

**Syntax**:

```python
df = spark.read.csv("data.tsv", header=True, sep="\t")
```

**Example**:
Assume a tab-separated file (`data.tsv`) with the following content:

```
Name	Age
Alice	30
Bob	25
```

**Code**:

```python
df = spark.read.csv("data.tsv", header=True, sep="\t")
df.show()
```

**Output**:

```text
+-----+---+
|Name |Age|
+-----+---+
|Alice| 30|
|Bob  | 25|
+-----+---+
```

You can replace `sep="\t"` with other delimiters as needed:

* For pipe `|` separated: `sep="|"`
* For semicolon separated: `sep=";"`
* For custom characters: `sep="#"`

---

### 🧾 **4. `show(n)`**

**Definition**: Displays the top `n` rows of the DataFrame (default: 20).

**Syntax**:

```python
df.show(2)
```

**Output**:

```text
+-----+---+
|Name |Age|
+-----+---+
|Alice| 30|
|Bob  | 25|
+-----+---+
```


---

### 🧾 **5. `withColumn()`**

**Definition**: Adds a new column or replaces an existing one.

**Syntax**:

```python
from pyspark.sql.functions import col
df.withColumn("AgePlus5", col("Age") + 5).show()
```

**Output**:

```text
+-----+---+--------+
|Name |Age|AgePlus5|
+-----+---+--------+
|Alice| 30|     35 |
|Bob  | 25|     30 |
+-----+---+--------+
```

---

### 🧾 **6. `select()`**

**Definition**: Selects one or more columns from the DataFrame.

**Syntax**:

```python
df.select("Name").show()
```

**Output**:

```text
+-----+
|Name |
+-----+
|Alice|
|Bob  |
+-----+
```

---

### 🧾 **7. `filter()` / `where()`**

**Definition**: Filters rows based on a boolean condition.

**Syntax**:

```python
from pyspark.sql.functions import col

df.filter(col("Age") > 25).show()
# OR
df.where("Age > 25").show()
```

**Output**:

```text
+-----+---+
|Name |Age|
+-----+---+
|Alice| 30|
+-----+---+
```

---

### 🧾 **8. `groupBy()`**

**Definition**: Groups rows by one or more columns for aggregation.

**Syntax**:

```python
df.groupBy("Name").count().show()
```

**Input**:

```text
+-----+---+
|Name |Age|
+-----+---+
|Alice| 30|
|Alice| 25|
|Bob  | 25|
+-----+---+
```

**Output**:

```text
+-----+-----+
|Name |count|
+-----+-----+
|Alice|  2  |
|Bob  |  1  |
+-----+-----+
```

---

### 🧾 **9. `agg()`**

**Definition**: Applies one or more aggregation functions after `groupBy`.

**Syntax**:

```python
from pyspark.sql.functions import sum, avg

df.groupBy("Name").agg(sum("Age").alias("TotalAge"), avg("Age").alias("AvgAge")).show()
```

**Output**:

```text
+-----+--------+-------+
|Name |TotalAge|AvgAge |
+-----+--------+-------+
|Alice|   55   | 27.5  |
|Bob  |   25   | 25.0  |
+-----+--------+-------+
```

---

### 🧾 **10. `orderBy()` / `sort()`**

**Definition**: Sorts rows by one or more columns.

**Syntax**:

```python
df.orderBy("Age").show()
df.orderBy(col("Age").desc()).show()
```

**Output** (Ascending):

```text
+-----+---+
|Name |Age|
+-----+---+
|Bob  | 25|
|Alice| 30|
+-----+---+
```

---

### 🧾 **11. `drop()`**

**Definition**: Drops a specified column from the DataFrame.

**Syntax**:

```python
df.drop("Age").show()
```

**Output**:

```text
+-----+
|Name |
+-----+
|Alice|
|Bob  |
+-----+
```

---

### 🧾 **12. `na.fill()` / `na.drop()` / `na.replace()`**

**Definition**: Handle missing (`null`) values.

#### ✅ `na.fill(value)`

```python
df.na.fill(0).show()
```

#### ✅ `na.drop()`

```python
df.na.drop().show()
```

#### ✅ `na.replace()`

```python
df.na.replace("NA", None).show()
```

**Input**:

```text
+-----+----+
|Name |Age |
+-----+----+
|John | 25 |
|Jane |null|
|NA   | 40 |
+-----+----+
```

**Output (using `fill`)**:

```text
+-----+----+
|Name |Age |
+-----+----+
|John | 25 |
|Jane |  0 |
|NA   | 40 |
+-----+----+
```

---

## **PySpark SQL functions**

---

### 📌 1. `col()`

**Description**: Refers to a column in a DataFrame; used for transformations.

**Syntax**:

```python
from pyspark.sql.functions import col
df.select(col("column_name"))
```

**Example**:
Input:

```python
+-------+-----+
| Name  | Age |
+-------+-----+
| Alice | 30  |
| Bob   | 25  |
+-------+-----+
```

Code:

```python
df.select(col("Name")).show()
```

Output:

```python
+-------+
|  Name |
+-------+
| Alice |
|  Bob  |
+-------+
```

---

### 📌 2. `lit()`

**Description**: Adds a constant/literal value as a new column.

**Syntax**:

```python
from pyspark.sql.functions import lit
df.withColumn("Country", lit("India"))
```

**Example**:
Input:

```python
+-----+
| Name|
+-----+
| John|
+-----+
```

Output:

```python
+-----+--------+
| Name| Country|
+-----+--------+
| John|  India |
+-----+--------+
```

---

### 📌 3. `when()`, `otherwise()`

**Description**: Used for conditional logic similar to if-else.

**Syntax**:

```python
from pyspark.sql.functions import when, col
df.withColumn("Status", when(col("Age") >= 18, "Adult").otherwise("Minor"))
```

**Example**:
Input:

```python
+-------+-----+
| Name  | Age |
+-------+-----+
| Alice | 30  |
| Bob   | 15  |
+-------+-----+
```

Output:

```python
+-------+-----+--------+
| Name  | Age | Status |
+-------+-----+--------+
| Alice | 30  | Adult  |
| Bob   | 15  | Minor  |
+-------+-----+--------+
```

---

### 📌 4. `sum()`, `avg()`, `count()`, `max()`, `min()`

**Description**: Aggregation functions.

**Syntax**:

```python
from pyspark.sql.functions import sum, avg, count, max, min
df.groupBy("Department").agg(sum("Salary"), avg("Salary"))
```

**Example**:
Input:

```python
+----------+------+
|Department|Salary|
+----------+------+
|   IT     | 5000 |
|   IT     | 7000 |
+----------+------+
```

Output:

```python
+----------+-----------+-----------+
|Department|sum(Salary)|avg(Salary)|
+----------+-----------+-----------+
|   IT     | 12000     | 6000.0    |
+----------+-----------+-----------+
```

---

### 📌 5. `lower()`, `upper()`

**Description**: Converts string column values to lowercase or uppercase.

**Syntax**:

```python
from pyspark.sql.functions import lower, upper
df.withColumn("LowerName", lower(col("Name")))
```

**Example**:
Input:

```python
+-------+
| Name  |
+-------+
| Alice |
+-------+
```

Output:

```python
+-------+----------+
| Name  |LowerName |
+-------+----------+
| Alice |  alice   |
+-------+----------+
```

---

### 📌 6. `regexp_replace()`

**Description**: Replaces substrings in a column using a regex.

**Syntax**:

```python
from pyspark.sql.functions import regexp_replace
df.withColumn("Cleaned", regexp_replace(col("Text"), "[^a-zA-Z ]", ""))
```

**Example**:
Input:

```python
+---------------+
|     Text      |
+---------------+
| Hello, World! |
+---------------+
```

Output:

```python
+---------------+-----------+
|     Text      |  Cleaned  |
+---------------+-----------+
| Hello, World! | Hello World |
+---------------+-----------+
```

---

### 📌 7. `concat()`, `concat_ws()`

**Description**: Concatenates multiple columns (with or without separator).

**Syntax**:

```python
from pyspark.sql.functions import concat, concat_ws
df.withColumn("FullName", concat_ws(" ", col("First"), col("Last")))
```

**Example**:
Input:

```python
+-------+--------+
| First |  Last  |
+-------+--------+
| John  |  Doe   |
+-------+--------+
```

Output:

```python
+-------+--------+-----------+
| First |  Last  | FullName  |
+-------+--------+-----------+
| John  |  Doe   | John Doe  |
+-------+--------+-----------+
```

---

### 📌 8. `year()`, `month()`, `dayofmonth()`

**Description**: Extract year, month, or day from date column.

**Syntax**:

```python
from pyspark.sql.functions import year, month, dayofmonth
df.withColumn("Year", year(col("Date")))
```

**Example**:
Input:

```python
+----------+
|   Date   |
+----------+
|2023-05-12|
+----------+
```

Output:

```python
+----------+------+
|   Date   | Year |
+----------+------+
|2023-05-12| 2023 |
+----------+------+
```

---

### 📌 9. `to_date()`, `date_format()`

**Description**:

* `to_date()`: Converts string to DateType.
* `date_format()`: Formats a date column into a string.

**Syntax**:

```python
from pyspark.sql.functions import to_date, date_format
df.withColumn("Formatted", date_format(to_date(col("Date")), "MMM-yyyy"))
```

**Example**:
Input:

```python
+-------------+
|     Date    |
+-------------+
| 12-05-2023  |
+-------------+
```

Output:

```python
+-------------+-----------+
|     Date    | Formatted |
+-------------+-----------+
| 12-05-2023  | May-2023  |
+-------------+-----------+
```

---

### 📌 10. `isnull()`, `isnotnull()`

**Description**: Used to filter or identify null values.

**Syntax**:

```python
from pyspark.sql.functions import col
df.filter(col("Age").isNull())
```

**Example**:
Input:

```python
+-----+-----+
|Name | Age |
+-----+-----+
|John | 25  |
|Jane |null |
+-----+-----+
```

Output:

```python
+-----+-----+
|Name | Age |
+-----+-----+
|Jane |null |
+-----+-----+
```

---

### 📌 11. `explode()`

**Description**: Converts array or map column into multiple rows (flattens the array).

**Syntax**:

```python
from pyspark.sql.functions import explode
df.withColumn("Hobby", explode(col("Hobbies")))
```

**Example**:
Input:

```python
+--------+--------------------+
|  Name  |      Hobbies       |
+--------+--------------------+
| Alice  | ["Reading", "Swim"]|
+--------+--------------------+
```

Output:

```python
+--------+--------+
|  Name  | Hobby  |
+--------+--------+
| Alice  |Reading |
| Alice  | Swim   |
+--------+--------+
```

---

## **Spark SQL functions**

---

### 🧾 **1. `createOrReplaceTempView("view_name")`**

**Definition**:
Registers a DataFrame as a temporary SQL table (view), which allows SQL queries to be run using `spark.sql()`.

**Syntax**:

```python
df.createOrReplaceTempView("employee_view")
```

> ⚠️ The view exists only during the lifetime of the Spark session.

**Example Input DataFrame**:

```python
+-------+--------+------+
| Name  | Dept   | Age  |
+-------+--------+------+
| Alice | HR     | 30   |
| Bob   | IT     | 25   |
+-------+--------+------+
```

**Usage**:

```python
df.createOrReplaceTempView("employee_view")
result = spark.sql("SELECT * FROM employee_view WHERE Age > 26")
result.show()
```

**Output**:

```text
+-------+-----+---+
| Name  |Dept |Age|
+-------+-----+---+
| Alice | HR  |30 |
+-------+-----+---+
```

---

### 🧾 **2. `spark.sql("SQL_QUERY")`**

**Definition**:
Executes SQL queries on registered temporary views or Hive tables.

**Syntax**:

```python
result = spark.sql("SELECT Dept, COUNT(*) as count FROM employee_view GROUP BY Dept")
```

**Example Input View (`employee_view`)**:

```text
+-------+--------+------+
| Name  | Dept   | Age  |
+-------+--------+------+
| Alice | HR     | 30   |
| Bob   | IT     | 25   |
| John  | HR     | 28   |
+-------+--------+------+
```

**Query**:

```python
spark.sql("SELECT Dept, COUNT(*) as count FROM employee_view GROUP BY Dept").show()
```

**Output**:

```text
+-------+-----+
| Dept  |count|
+-------+-----+
| HR    |  2  |
| IT    |  1  |
+-------+-----+
```

---

### 🧾 **3. `saveAsTable("table_name")`**

**Definition**:
Saves a DataFrame as a persistent Hive or Spark table, which can be queried using SQL even after the session ends (if Hive support is enabled).

**Syntax**:

```python
df.write.mode("overwrite").saveAsTable("employee_table")
```

> ⚠️ Requires Hive support to be enabled (`enableHiveSupport()` in `SparkSession`)

**Example Input DataFrame**:

```text
+-------+--------+------+
| Name  | Dept   | Age  |
+-------+--------+------+
| Alice | HR     | 30   |
| Bob   | IT     | 25   |
+-------+--------+------+
```

**Usage**:

```python
df.write.mode("overwrite").saveAsTable("employee_table")
```

**Querying it later**:

```python
spark.sql("SELECT * FROM employee_table").show()
```

**Output**:

```text
+-------+--------+-----+
| Name  | Dept   | Age |
+-------+--------+-----+
| Alice | HR     | 30  |
| Bob   | IT     | 25  |
+-------+--------+-----+
```

---

