# 🤖 **Spark MLlib (Machine Learning Library)**

## 🧩 What is it?

* **MLlib** is Apache Spark’s **machine learning library**.
* Provides scalable implementations of **machine learning algorithms** and **utilities**.
* Runs **distributed** on large datasets, unlike scikit-learn which is **single-node**.

---

# ⚡ Capabilities of MLlib

### 1️⃣ **Supervised Learning**

* 📈 **Regression**

  * `LinearRegression`
  * `DecisionTreeRegressor`
  * `RandomForestRegressor`
  * `GBTRegressor` (Gradient Boosted Trees)

* ✅ **Classification**

  * `LogisticRegression`
  * `NaiveBayes`
  * `DecisionTreeClassifier`
  * `RandomForestClassifier`
  * `GBTClassifier`
  * `LinearSVC` (Support Vector Machines)

---

### 2️⃣ **Unsupervised Learning**

* 📊 **Clustering**

  * `KMeans`
  * `GaussianMixture`
  * `LDA` (Latent Dirichlet Allocation – topic modeling)

* 🔍 **Dimensionality Reduction**

  * `PCA` (Principal Component Analysis)
  * `SVD` (Singular Value Decomposition)

---

### 3️⃣ **Recommendations**

* 🎬 **Collaborative Filtering**

  * `ALS` (Alternating Least Squares) → used for **recommendation systems** (like Netflix, Amazon).

---

### 4️⃣ **Utilities**

* Feature extraction & transformation (e.g., `VectorAssembler`, `StringIndexer`, `OneHotEncoder`).
* Pipelines for building end-to-end ML workflows.
* Model evaluation metrics (`RegressionEvaluator`, `MulticlassClassificationEvaluator`, etc.).

---

# 📚 Library for MLlib

All ML algorithms are in the **`pyspark.ml`** package.

```python
# Regression
from pyspark.ml.regression import LinearRegression, DecisionTreeRegressor

# Classification
from pyspark.ml.classification import LogisticRegression, NaiveBayes, DecisionTreeClassifier

# Clustering
from pyspark.ml.clustering import KMeans

# Dimensionality Reduction
from pyspark.ml.feature import PCA

# Recommendation
from pyspark.ml.recommendation import ALS
```

---

# 🏗️ Example: Linear Regression

```python
from pyspark.sql import SparkSession
from pyspark.ml.regression import LinearRegression

spark = SparkSession.builder.appName("MLlibExample").getOrCreate()

# Sample training data
data = [(1, 2.0, 1.0), (2, 3.0, 2.0), (3, 4.0, 3.0)]
columns = ["label", "feature1", "feature2"]

df = spark.createDataFrame(data, columns)

# Assemble features into a vector
from pyspark.ml.feature import VectorAssembler
assembler = VectorAssembler(inputCols=["feature1", "feature2"], outputCol="features")
train_df = assembler.transform(df)

# Train model
lr = LinearRegression(featuresCol="features", labelCol="label")
model = lr.fit(train_df)

# Print coefficients
print("Coefficients:", model.coefficients)
print("Intercept:", model.intercept)
```

---

# 🖼️ Analogy

Think of MLlib as a **gym 🏋️‍♂️**:

* **Trainers** = Algorithms (regression, clustering, classification).
* **Workout tools** = Transformers & Feature Extractors.
* **Workout plan** = Pipelines.
* **Fitness test** = Evaluators.

---

# ✅ Summary

* **Spark MLlib** = Scalable ML library in Spark.
* Supports **regression, classification, clustering, dimensionality reduction, recommendations**.
* Use **`pyspark.ml`** package.
* Provides **pipelines, transformers, evaluators** for complete ML workflows.
