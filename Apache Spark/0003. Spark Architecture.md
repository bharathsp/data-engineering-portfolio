Apache Spark follows a **master-slave distributed architecture** that allows it to process large-scale data in a parallel and fault-tolerant manner. 

---

## üîß **Spark Architecture Core Components**

### **1. Spark Driver**

* **Role**: Orchestrator / Brain of the Spark job.
* **Responsibilities**:

  * Creates the **SparkContext**.
  * Converts user code into a **DAG (Directed Acyclic Graph)** of stages.
  * Coordinates job execution and keeps track of metadata.

---

### **2. SparkSession**

* Entry point to Spark (since Spark 2.0).
* Used to create the **SparkContext** and access all Spark functionalities.

---

### **3. SparkContext**

* Low-level engine connecting the Spark application to the **Cluster Manager**.
* Sends DAGs to the **DAG Scheduler**.

---

### **4. DAG Scheduler**

* Converts **RDD transformations** into **stages**.
* Defines **stage boundaries** based on **shuffle dependencies**.
* Sends stages to the **Task Scheduler**.

---

### **5. Task Scheduler**

* Breaks **stages** into **tasks** (smallest units of execution).
* Sends tasks to **executors** via **Cluster Manager**.

---

### **6. Cluster Manager**

* Allocates resources across applications.
* Spark supports:

  * **Standalone** (built-in)
  * **YARN**
  * **Mesos**
  * **Kubernetes**

---

### **7. Worker Nodes**

* Nodes in the cluster that run the tasks.
* Host **executors**.

---

### **8. Executors**

* JVM processes on worker nodes.
* Each application has **its own executors**.
* Responsibilities:

  * Execute tasks.
  * Store RDD partitions in memory/cache.
  * Report status to the **driver**.

---

### **9. Cores**

* Threads available inside executors to run multiple tasks in parallel.

---

## üìä Spark Execution Flow Diagram

```
+-----------------------------+
|       Spark Driver          |
|  (SparkSession & Context)   |
+-------------+---------------+
              |
              v
    +-------------------+
    |   DAG Scheduler   |
    +-------------------+
              |
              v
    +-------------------+
    |  Task Scheduler   |
    +-------------------+
              |
              v
    +----------------------+
    |   Cluster Manager    |
    +----------------------+
              |
              v
    +---------------------+         +---------------------+
    |   Worker Node 1     |         |   Worker Node 2     |
    |  +--------------+   |         |  +--------------+   |
    |  |  Executor 1  |   |  <--->  |  |  Executor 2  |   |
    |  | Tasks + Core |   |         |  | Tasks + Core |   |
    +---------------------+         +---------------------+
```

---

## üîÅ **Spark Execution Cycle (Job ‚Üí Stage ‚Üí Task)**

1. **User Code (Driver)**:

   * Invokes transformations/actions on RDD/DataFrame.
   * Triggers job execution.

2. **DAG Scheduler**:

   * Splits the job into **stages** (based on shuffles).

3. **Task Scheduler**:

   * Divides each stage into **tasks** (per partition).
   * Schedules them on executors via **Cluster Manager**.

4. **Executors**:

   * Run tasks and return results to **Driver**.

---

## ‚öôÔ∏è **Resource Allocation: Executors, Cores, Memory**

### üß† Executors

* Number of executors = number of JVMs running on worker nodes.
* Each executor:

  * Has multiple **cores**.
  * Allocated a fixed **memory** chunk (e.g., `--executor-memory 4G`).

### üîÑ Cores

* Each core = one concurrent **task** execution thread.
* E.g., 5 cores per executor = 5 tasks can run in parallel.

### üíæ Memory

* Split into:

  * **Storage Memory**: for caching RDDs/dataframes.
  * **Execution Memory**: for shuffle, joins, aggregations.
  * **User Memory**: for custom user data.
  * **Reserved Memory**: Spark internal overhead.

---

## ‚öñÔ∏è Example Resource Layout

**Cluster with 3 Worker Nodes**

* Each node has: 16 cores, 64 GB RAM.

### You request:

* `--num-executors 6`
* `--executor-cores 4`
* `--executor-memory 16G`

Then:

* 2 executors per worker node.
* Each executor:

  * Runs 4 parallel tasks.
  * Uses 16 GB RAM.

---

## ‚úÖ Summary

| Component       | Role                                 |
| --------------- | ------------------------------------ |
| SparkSession    | Entry point                          |
| SparkContext    | Connects to Cluster Manager          |
| DAG Scheduler   | Builds execution plan                |
| Task Scheduler  | Sends tasks to executors             |
| Cluster Manager | Allocates resources                  |
| Worker Nodes    | Run executors                        |
| Executors       | Run tasks and cache data             |
| Cores           | Enable task parallelism in executors |
