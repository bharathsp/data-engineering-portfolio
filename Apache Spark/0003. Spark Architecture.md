Apache Spark follows a **master-slave distributed architecture** that allows it to process large-scale data in a parallel and fault-tolerant manner. 

<img width="600" height="350" alt="image" src="https://github.com/user-attachments/assets/9d533362-6dbd-482d-959a-d542feef15d4" />

---

# 🔧 **Spark Architecture Core Components**

## **1. Spark Driver**

The **Driver** in Apache Spark is like the **“Brain 🧠”** of a Spark application.
It runs the main function of your program and is responsible for coordinating the entire job execution.

### 📋 Roles & Responsibilities of Spark Driver

1. **Program Execution** 📝

   * Runs the **main()** method of your Spark application.
   * Defines RDDs, DataFrames, transformations, and actions.

2. **Cluster Communication** 🌐

   * Connects with the **Cluster Manager** (YARN, Mesos, or Standalone).
   * Requests resources for Executors.

3. **Task Scheduling** 📅

   * Breaks the job into **stages** and **tasks**.
   * Assigns these tasks to **executors**.

4. **Metadata Management** 📂

   * Keeps track of RDD lineage, DAG (Directed Acyclic Graph), and execution plans.

5. **Result Collection** 📬

   * Collects results from executors.
   * Returns final output to the user or writes to storage.

### 💾 Driver Memory

* **Driver memory** = Memory allocated to the Driver process.
* Used for:

  * Storing metadata (RDD lineage, DAG).
  * Holding small results from executors.
  * Keeping variables and SparkSession context.
* Controlled by:

  ```
  --driver-memory 4g
  ```

  (This allocates 4 GB RAM to the driver).

⚠️ If driver memory is too small → job may fail with **OutOfMemoryError**.

### 🏎️ Real-life Analogy

Imagine **a race**:

* **Driver (Car Driver 🧑‍✈️)** = Spark Driver

  * Plans the route (DAG).
  * Decides pit stops (stages).
  * Communicates with team.
  * Monitors the race and ensures car (executors) is performing.

* **Car 🚗** = Cluster resources (executors).

* **Pit Crew 👨‍🔧** = Cluster Manager (allocates resources).

* **Fuel & Tires ⛽** = Data & Tasks.

If the **driver (brain)** doesn’t have enough **focus/energy (memory)**, the whole race collapses.

---

## **2. SparkSession**

* **SparkSession** is the **entry point** to any Spark application.
* Think of it as the **“Gateway 🚪”** to interact with Spark.
* It allows you to use **RDDs, DataFrames, Datasets, and SQL** all in one place.

👉 Introduced in **Spark 2.0** to unify `SQLContext`, `HiveContext`, and `SparkContext`.

### 📋 Roles & Responsibilities of SparkSession

1. **Application Entry Point** 🔑

   * Provides a single point to connect with Spark.

2. **Resource Manager** ⚙️

   * Manages the Spark context (driver + executors).
   * Handles configuration settings.

3. **DataFrame & Dataset API** 📊

   * Allows you to create DataFrames and Datasets.

4. **SQL Queries** 🗂️

   * Lets you run SQL queries on structured data using `.sql()`.

5. **Catalog Access** 📚

   * Keeps track of tables, databases, and temporary views.

### 🖥️ Example in Python (PySpark)

```python
from pyspark.sql import SparkSession

# Create SparkSession
spark = SparkSession.builder \
    .appName("MyApp") \
    .config("spark.some.config.option", "config-value") \
    .getOrCreate()

# Example: Create DataFrame
data = [("Bharath", 28), ("Anu", 25)]
df = spark.createDataFrame(data, ["Name", "Age"])

df.show()

# Example: SQL Query
df.createOrReplaceTempView("people")
spark.sql("SELECT * FROM people WHERE Age > 26").show()
```

### 🏎️ Real-life Analogy

Think of Spark as a **big airport ✈️**:

* **SparkSession = Airport Terminal 🛫**

  * Main entry point to the airport.
  * Provides access to flights (RDDs, DataFrames, SQL).
  * Has gates to different services (SQL, Hive, Catalog).

* **Flights ✈️** = DataFrames / RDDs.

* **Air Traffic Control 🛰️** = Driver + Cluster Manager.

Without the **terminal (SparkSession)**, you cannot reach flights (Spark features).

---

## **3. SparkContext**

* **SparkContext** is the **entry point** to the **low-level Spark Core API**.
* It represents a **connection between your Spark application and the Spark cluster**.
* Before Spark 2.0, every application **had to create a SparkContext** to talk to the cluster.
* After Spark 2.0 → you usually use **SparkSession**, which internally creates a SparkContext for you.

### 📋 Roles & Responsibilities of SparkContext

1. **Cluster Connection** 🌐

   * Establishes connection with the cluster manager (YARN, Mesos, Standalone).

2. **Resource Allocation** ⚙️

   * Requests resources (executors, CPUs, memory) from the cluster manager.

3. **RDD Creation** 📊

   * Used to create **RDDs** (Resilient Distributed Datasets), the core data structure of Spark.

4. **Task Distribution** 📅

   * Sends tasks (jobs broken into stages) to executors.

5. **Job Monitoring** 📈

   * Monitors the execution of jobs, tracks progress, and handles failures.

### 🖥️ Example in Python (PySpark)

```python
from pyspark import SparkContext

# Create SparkContext
sc = SparkContext("local", "MyApp")

# Create RDD
data = [1, 2, 3, 4, 5]
rdd = sc.parallelize(data)

# Perform operation
result = rdd.map(lambda x: x * 2).collect()
print(result)  # [2, 4, 6, 8, 10]
```

👉 In modern Spark, you usually do:

```python
sc = spark.sparkContext
```

because `SparkSession` manages it for you.

### 🏎️ Real-life Analogy

Think of Spark as a **railway network 🚆**:

* **SparkContext = Station Master 🚉**

  * Connects the station (application) to the railway network (cluster).
  * Assigns tracks (executors) for trains (tasks).
  * Ensures smooth operation of trains (job execution).

* **Trains 🚂** = Jobs & Tasks.

* **Passengers 👨‍👩‍👧** = Data (RDD elements).

* **Railway HQ 🏢** = Cluster Manager (allocates tracks/resources).

Without the **Station Master (SparkContext)**, the trains cannot run.

✨ In short:
**SparkContext = The “engine connector” that lets your Spark application talk to the cluster and work with RDDs.**
👉 After Spark 2.0, it’s wrapped inside **SparkSession**, so you don’t create it directly in most cases.

---

## **4. DAG Scheduler**

Great one 🚀 — the **DAG Scheduler** is an important piece in Spark’s internal execution flow. Let’s break it down step by step with icons, roles, and a real-life analogy.

### 🧩 What is DAG Scheduler?

* **DAG Scheduler** is the component in Spark that converts a **logical execution plan** into a **physical execution plan**.
* It **organizes jobs into stages**, and each stage into **tasks**.
* DAG stands for **Directed Acyclic Graph** → a flow of operations with no cycles.

👉 It sits between the **user code** and the **Task Scheduler**.

### 📋 Roles & Responsibilities of DAG Scheduler

1. **Job Division** 🗂️

   * Splits a Spark job into **stages** based on shuffle boundaries.

2. **Stage Creation** 📑

   * Creates **ResultStage** (for final actions like `collect()`) and **ShuffleMapStage** (for shuffle operations).

3. **Task Set Preparation** 📦

   * Breaks each stage into **tasks**, one task per partition.

4. **Task Submission** 🚚

   * Sends task sets to the **Task Scheduler**, which then distributes them to executors.

5. **Failure Handling** 🔄

   * If a stage fails, retries it (default 4 times).

### ⚙️ Example Flow

Suppose you run:

```python
rdd.map(lambda x: x+1).filter(lambda x: x>5).reduceByKey(lambda a,b: a+b).collect()
```

* `map` & `filter` → narrow transformations (can be in one stage).
* `reduceByKey` → shuffle needed → new stage.
* `collect` → triggers a **job**.

**DAG Scheduler** will:

* Break into **two stages** (before and after shuffle).
* Create tasks for each partition.
* Send tasks to **Task Scheduler → Executors**.

### 🏎️ Real-life Analogy

Think of a **food delivery system 🍔🚚**:

* **Customer Order (Action like collect)** = Job

* **DAG Scheduler** = **Restaurant Manager 👨‍🍳**

  * Splits the order into **stages**: cooking, packing, delivery.
  * Assigns tasks (cut veggies, cook patty, pack meal).
  * Makes sure tasks flow logically (cook before pack, pack before delivery).

* **Task Scheduler** = **Waiters/Delivery Dispatch 🚴**

  * Takes each task set and sends them to the right worker (executors).

Without the **manager (DAG Scheduler)**, the workflow would be chaotic.

✨ In short:
**DAG Scheduler = The planner 🧠 that converts your logical operations into a stage-wise execution plan and hands over tasks to the Task Scheduler.**

---

## **5. Task Scheduler**

* Breaks **stages** into **tasks** (smallest units of execution).
* Sends tasks to **executors** via **Cluster Manager**.

---

## **6. Cluster Manager**

* Allocates resources across applications.
* Spark supports:

  * **Standalone** (built-in)
  * **YARN**
  * **Mesos**
  * **Kubernetes**

---

## **7. Worker Nodes**

* Nodes in the cluster that run the tasks.
* Host **executors**.

---

## **8. Executors**

* JVM processes on worker nodes.
* Each application has **its own executors**.
* Responsibilities:

  * Execute tasks.
  * Store RDD partitions in memory/cache.
  * Report status to the **driver**.

---

## **9. Cores**

* Threads available inside executors to run multiple tasks in parallel.

---

## 📊 Spark Execution Flow Diagram

```
+-----------------------------+
|       Spark Driver          |
|  (SparkSession & Context)   |
+-------------+---------------+
              |
              v
    +-------------------+
    |   DAG Scheduler   |
    +-------------------+
              |
              v
    +-------------------+
    |  Task Scheduler   |
    +-------------------+
              |
              v
    +----------------------+
    |   Cluster Manager    |
    +----------------------+
              |
              v
    +---------------------+         +---------------------+
    |   Worker Node 1     |         |   Worker Node 2     |
    |  +--------------+   |         |  +--------------+   |
    |  |  Executor 1  |   |  <--->  |  |  Executor 2  |   |
    |  | Tasks + Core |   |         |  | Tasks + Core |   |
    +---------------------+         +---------------------+
```

---

## 🔁 **Spark Execution Cycle (Job → Stage → Task)**

1. **User Code (Driver)**:

   * Invokes transformations/actions on RDD/DataFrame.
   * Triggers job execution.

2. **DAG Scheduler**:

   * Splits the job into **stages** (based on shuffles).

3. **Task Scheduler**:

   * Divides each stage into **tasks** (per partition).
   * Schedules them on executors via **Cluster Manager**.

4. **Executors**:

   * Run tasks and return results to **Driver**.

---

## ⚙️ **Resource Allocation: Executors, Cores, Memory**

### 🧠 Executors

* Number of executors = number of JVMs running on worker nodes.
* Each executor:

  * Has multiple **cores**.
  * Allocated a fixed **memory** chunk (e.g., `--executor-memory 4G`).

### 🔄 Cores

* Each core = one concurrent **task** execution thread.
* E.g., 5 cores per executor = 5 tasks can run in parallel.

### 💾 Memory

* Split into:

  * **Storage Memory**: for caching RDDs/dataframes.
  * **Execution Memory**: for shuffle, joins, aggregations.
  * **User Memory**: for custom user data.
  * **Reserved Memory**: Spark internal overhead.

---

## ⚖️ Example Resource Layout

**Cluster with 3 Worker Nodes**

* Each node has: 16 cores, 64 GB RAM.

### You request:

* `--num-executors 6`
* `--executor-cores 4`
* `--executor-memory 16G`

Then:

* 2 executors per worker node.
* Each executor:

  * Runs 4 parallel tasks.
  * Uses 16 GB RAM.

---

## ✅ Summary

| Component       | Role                                 |
| --------------- | ------------------------------------ |
| SparkSession    | Entry point                          |
| SparkContext    | Connects to Cluster Manager          |
| DAG Scheduler   | Builds execution plan                |
| Task Scheduler  | Sends tasks to executors             |
| Cluster Manager | Allocates resources                  |
| Worker Nodes    | Run executors                        |
| Executors       | Run tasks and cache data             |
| Cores           | Enable task parallelism in executors |
