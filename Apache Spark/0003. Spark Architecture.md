Apache Spark follows a **master-slave distributed architecture** that allows it to process large-scale data in a parallel and fault-tolerant manner. 

<img width="600" height="350" alt="image" src="https://github.com/user-attachments/assets/9d533362-6dbd-482d-959a-d542feef15d4" />

---

# ğŸ”§ **Spark Architecture Core Components**

## **1. Spark Driver**

The **Driver** in Apache Spark is like the **â€œBrain ğŸ§ â€** of a Spark application.
It runs the main function of your program and is responsible for coordinating the entire job execution.

### ğŸ“‹ Roles & Responsibilities of Spark Driver

1. **Program Execution** ğŸ“

   * Runs the **main()** method of your Spark application.
   * Defines RDDs, DataFrames, transformations, and actions.

2. **Cluster Communication** ğŸŒ

   * Connects with the **Cluster Manager** (YARN, Mesos, or Standalone).
   * Requests resources for Executors.

3. **Task Scheduling** ğŸ“…

   * Breaks the job into **stages** and **tasks**.
   * Assigns these tasks to **executors**.

4. **Metadata Management** ğŸ“‚

   * Keeps track of RDD lineage, DAG (Directed Acyclic Graph), and execution plans.

5. **Result Collection** ğŸ“¬

   * Collects results from executors.
   * Returns final output to the user or writes to storage.

### ğŸ’¾ Driver Memory

* **Driver memory** = Memory allocated to the Driver process.
* Used for:

  * Storing metadata (RDD lineage, DAG).
  * Holding small results from executors.
  * Keeping variables and SparkSession context.
* Controlled by:

  ```
  --driver-memory 4g
  ```

  (This allocates 4 GB RAM to the driver).

âš ï¸ If driver memory is too small â†’ job may fail with **OutOfMemoryError**.

### ğŸï¸ Real-life Analogy

Imagine **a race**:

* **Driver (Car Driver ğŸ§‘â€âœˆï¸)** = Spark Driver

  * Plans the route (DAG).
  * Decides pit stops (stages).
  * Communicates with team.
  * Monitors the race and ensures car (executors) is performing.

* **Car ğŸš—** = Cluster resources (executors).

* **Pit Crew ğŸ‘¨â€ğŸ”§** = Cluster Manager (allocates resources).

* **Fuel & Tires â›½** = Data & Tasks.

If the **driver (brain)** doesnâ€™t have enough **focus/energy (memory)**, the whole race collapses.

---

## **2. SparkSession**

* **SparkSession** is the **entry point** to any Spark application.
* Think of it as the **â€œGateway ğŸšªâ€** to interact with Spark.
* It allows you to use **RDDs, DataFrames, Datasets, and SQL** all in one place.

ğŸ‘‰ Introduced in **Spark 2.0** to unify `SQLContext`, `HiveContext`, and `SparkContext`.

### ğŸ“‹ Roles & Responsibilities of SparkSession

1. **Application Entry Point** ğŸ”‘

   * Provides a single point to connect with Spark.

2. **Resource Manager** âš™ï¸

   * Manages the Spark context (driver + executors).
   * Handles configuration settings.

3. **DataFrame & Dataset API** ğŸ“Š

   * Allows you to create DataFrames and Datasets.

4. **SQL Queries** ğŸ—‚ï¸

   * Lets you run SQL queries on structured data using `.sql()`.

5. **Catalog Access** ğŸ“š

   * Keeps track of tables, databases, and temporary views.

### ğŸ–¥ï¸ Example in Python (PySpark)

```python
from pyspark.sql import SparkSession

# Create SparkSession
spark = SparkSession.builder \
    .appName("MyApp") \
    .config("spark.some.config.option", "config-value") \
    .getOrCreate()

# Example: Create DataFrame
data = [("Bharath", 28), ("Anu", 25)]
df = spark.createDataFrame(data, ["Name", "Age"])

df.show()

# Example: SQL Query
df.createOrReplaceTempView("people")
spark.sql("SELECT * FROM people WHERE Age > 26").show()
```

### ğŸï¸ Real-life Analogy

Think of Spark as a **big airport âœˆï¸**:

* **SparkSession = Airport Terminal ğŸ›«**

  * Main entry point to the airport.
  * Provides access to flights (RDDs, DataFrames, SQL).
  * Has gates to different services (SQL, Hive, Catalog).

* **Flights âœˆï¸** = DataFrames / RDDs.

* **Air Traffic Control ğŸ›°ï¸** = Driver + Cluster Manager.

Without the **terminal (SparkSession)**, you cannot reach flights (Spark features).

---

## **3. SparkContext**

* **SparkContext** is the **entry point** to the **low-level Spark Core API**.
* It represents a **connection between your Spark application and the Spark cluster**.
* Before Spark 2.0, every application **had to create a SparkContext** to talk to the cluster.
* After Spark 2.0 â†’ you usually use **SparkSession**, which internally creates a SparkContext for you.

### ğŸ“‹ Roles & Responsibilities of SparkContext

1. **Cluster Connection** ğŸŒ

   * Establishes connection with the cluster manager (YARN, Mesos, Standalone).

2. **Resource Allocation** âš™ï¸

   * Requests resources (executors, CPUs, memory) from the cluster manager.

3. **RDD Creation** ğŸ“Š

   * Used to create **RDDs** (Resilient Distributed Datasets), the core data structure of Spark.

4. **Task Distribution** ğŸ“…

   * Sends tasks (jobs broken into stages) to executors.

5. **Job Monitoring** ğŸ“ˆ

   * Monitors the execution of jobs, tracks progress, and handles failures.

### ğŸ–¥ï¸ Example in Python (PySpark)

```python
from pyspark import SparkContext

# Create SparkContext
sc = SparkContext("local", "MyApp")

# Create RDD
data = [1, 2, 3, 4, 5]
rdd = sc.parallelize(data)

# Perform operation
result = rdd.map(lambda x: x * 2).collect()
print(result)  # [2, 4, 6, 8, 10]
```

ğŸ‘‰ In modern Spark, you usually do:

```python
sc = spark.sparkContext
```

because `SparkSession` manages it for you.

### ğŸï¸ Real-life Analogy

Think of Spark as a **railway network ğŸš†**:

* **SparkContext = Station Master ğŸš‰**

  * Connects the station (application) to the railway network (cluster).
  * Assigns tracks (executors) for trains (tasks).
  * Ensures smooth operation of trains (job execution).

* **Trains ğŸš‚** = Jobs & Tasks.

* **Passengers ğŸ‘¨â€ğŸ‘©â€ğŸ‘§** = Data (RDD elements).

* **Railway HQ ğŸ¢** = Cluster Manager (allocates tracks/resources).

Without the **Station Master (SparkContext)**, the trains cannot run.

âœ¨ In short:
**SparkContext = The â€œengine connectorâ€ that lets your Spark application talk to the cluster and work with RDDs.**
ğŸ‘‰ After Spark 2.0, itâ€™s wrapped inside **SparkSession**, so you donâ€™t create it directly in most cases.

---

## **4. DAG Scheduler**

Great one ğŸš€ â€” the **DAG Scheduler** is an important piece in Sparkâ€™s internal execution flow. Letâ€™s break it down step by step with icons, roles, and a real-life analogy.

### ğŸ§© What is DAG Scheduler?

* **DAG Scheduler** is the component in Spark that converts a **logical execution plan** into a **physical execution plan**.
* It **organizes jobs into stages**, and each stage into **tasks**.
* DAG stands for **Directed Acyclic Graph** â†’ a flow of operations with no cycles.

ğŸ‘‰ It sits between the **user code** and the **Task Scheduler**.

### ğŸ“‹ Roles & Responsibilities of DAG Scheduler

1. **Job Division** ğŸ—‚ï¸

   * Splits a Spark job into **stages** based on shuffle boundaries.

2. **Stage Creation** ğŸ“‘

   * Creates **ResultStage** (for final actions like `collect()`) and **ShuffleMapStage** (for shuffle operations).

3. **Task Set Preparation** ğŸ“¦

   * Breaks each stage into **tasks**, one task per partition.

4. **Task Submission** ğŸšš

   * Sends task sets to the **Task Scheduler**, which then distributes them to executors.

5. **Failure Handling** ğŸ”„

   * If a stage fails, retries it (default 4 times).

### âš™ï¸ Example Flow

Suppose you run:

```python
rdd.map(lambda x: x+1).filter(lambda x: x>5).reduceByKey(lambda a,b: a+b).collect()
```

* `map` & `filter` â†’ narrow transformations (can be in one stage).
* `reduceByKey` â†’ shuffle needed â†’ new stage.
* `collect` â†’ triggers a **job**.

**DAG Scheduler** will:

* Break into **two stages** (before and after shuffle).
* Create tasks for each partition.
* Send tasks to **Task Scheduler â†’ Executors**.

### ğŸï¸ Real-life Analogy

Think of a **food delivery system ğŸ”ğŸšš**:

* **Customer Order (Action like collect)** = Job

* **DAG Scheduler** = **Restaurant Manager ğŸ‘¨â€ğŸ³**

  * Splits the order into **stages**: cooking, packing, delivery.
  * Assigns tasks (cut veggies, cook patty, pack meal).
  * Makes sure tasks flow logically (cook before pack, pack before delivery).

* **Task Scheduler** = **Waiters/Delivery Dispatch ğŸš´**

  * Takes each task set and sends them to the right worker (executors).

Without the **manager (DAG Scheduler)**, the workflow would be chaotic.

âœ¨ In short:
**DAG Scheduler = The planner ğŸ§  that converts your logical operations into a stage-wise execution plan and hands over tasks to the Task Scheduler.**

---

## **5. Task Scheduler**

* The **Task Scheduler** is the Spark component that takes **tasks prepared by the DAG Scheduler** and actually **schedules them on cluster executors** for execution.
* It works at a **lower level** than DAG Scheduler.
* It does **not understand stages, RDDs, or DAGs** â†’ it only knows **tasks** and executors.

### ğŸ“‹ Roles & Responsibilities of Task Scheduler

1. **Task Dispatching** ğŸšš

   * Takes **TaskSets** from DAG Scheduler and assigns tasks to executors.

2. **Resource Allocation** âš™ï¸

   * Chooses the best executor (based on data locality & availability).

3. **Execution Monitoring** ğŸ“¡

   * Tracks task status (running, completed, failed).
   * Collects task results from executors.

4. **Retry & Failover** ğŸ”„

   * If a task fails, reassigns it to another executor (up to 4 retries by default).

5. **Communication with Cluster Manager** ğŸŒ

   * Works with YARN, Mesos, or Spark Standalone to launch executors.

### âš™ï¸ Example Flow

Suppose DAG Scheduler sends a **TaskSet** (say, 100 tasks for 100 partitions).

* **Task Scheduler**:

  * Chooses executors close to the data.
  * Assigns each task to an executor.
  * Collects results & informs DAG Scheduler.

### ğŸï¸ Real-life Analogy

Think of a **restaurant again ğŸ´**:

* **DAG Scheduler = Head Chef ğŸ‘¨â€ğŸ³**

  * Breaks big order into tasks: cut veggies, cook rice, grill chicken.

* **Task Scheduler = Kitchen Dispatcher ğŸ‘¨â€ğŸ”§**

  * Assigns each task to the right cook.
  * Ensures no cook is idle and tasks are retried if a cook messes up.
  * Tracks completion and reports back to the chef.

So **Task Scheduler is like the dispatcher who makes sure each worker executes their piece of work properly.**

âœ¨ In short:
**Task Scheduler = The dispatcher that actually sends tasks to executors, monitors execution, and retries on failure.**
ğŸ‘‰ It doesnâ€™t know about DAGs/stages â€” only tasks.

---

## **6. Cluster Manager**

Nice ğŸ‘ â€” letâ€™s wrap the Spark execution flow with the **Cluster Manager**.

---

## ğŸ§© What is Cluster Manager?

* **Cluster Manager** is the **external service** that manages cluster resources (CPU, memory) across multiple machines.
* Spark doesnâ€™t manage resources by itself â†’ it asks the **Cluster Manager** to provide them.
* Executors (workers that run tasks) are launched by the Cluster Manager.

ğŸ‘‰ Without a cluster manager, Spark canâ€™t run in a distributed way.

### ğŸ“‹ Roles & Responsibilities of Cluster Manager

1. **Resource Management** âš™ï¸

   * Allocates CPUs, memory, and nodes for Spark applications.

2. **Executor Management** ğŸ§‘â€ğŸ’»

   * Starts and stops executors as requested by the Spark Driver.

3. **Multi-Application Support** ğŸ—‚ï¸

   * Runs multiple applications on the same cluster and shares resources.

4. **Communication with Driver** ğŸŒ

   * Reports back about available resources.
   * Executes commands given by Spark Driver.

### âš™ï¸ Types of Cluster Managers in Spark

1. **Standalone** ğŸ–¥ï¸

   * Sparkâ€™s built-in simple cluster manager.

2. **YARN** ğŸ§µ (Yet Another Resource Negotiator)

   * Hadoop ecosystemâ€™s resource manager.
   * Common in big data pipelines.

3. **Mesos** ğŸ§

   * General-purpose cluster manager.

4. **Kubernetes** â˜¸ï¸

   * Container-based cluster manager (modern choice).

### âš™ï¸ Example Flow (Simplified)

1. **Driver** asks Cluster Manager â†’ â€œI need 5 executors with 4 cores each.â€
2. **Cluster Manager** checks available resources.
3. **Cluster Manager** launches executors on worker nodes.
4. **Task Scheduler** assigns tasks to these executors.

### ğŸï¸ Real-life Analogy

Imagine a **ride-hailing service ğŸš–** (like Uber):

* **Cluster Manager = Dispatcher Office ğŸ¢**

  * Keeps track of all drivers (resources).
  * Assigns a driver (executor) when a passenger (task) requests a ride.
  * Ensures multiple customers (applications) can be served.

* **Spark Driver = Customer App ğŸ“±** (requests a ride).

* **Executors = Taxi Drivers ğŸš•** (do the actual driving = task execution).

Without the **dispatcher office (Cluster Manager)**, cars (executors) cannot be coordinated properly.

âœ¨ In short:
**Cluster Manager = The â€œresource allocatorâ€ that provides executors to Spark and manages resources across applications.**

---

## **7. Worker Nodes**

* **Worker Node = Machine (physical or VM) in the cluster** that actually executes the tasks assigned by the Spark Driver.
* Each worker runs **executors** (JVM processes) where the tasks execute.
* Workers are managed by the **Cluster Manager** (YARN, Kubernetes, Standalone, etc.).

ğŸ‘‰ Without worker nodes, Spark cannot process any data â€” the driver just does planning, but workers do the heavy lifting.

### ğŸ“‹ Roles & Responsibilities of Worker Nodes

1. **Run Executors** âš™ï¸

   * Launch executors as instructed by the Cluster Manager.

2. **Task Execution** ğŸ§‘â€ğŸ’»

   * Execute tasks assigned by the Task Scheduler.
   * Perform computations (transformations, actions).

3. **Data Storage (optional)** ğŸ’¾

   * Store cached data (RDD/DataFrame persist).
   * Sometimes co-located with HDFS or other storage.

4. **Communication** ğŸŒ

   * Send results back to the Driver.
   * Communicate with other executors during shuffle operations.

### âš™ï¸ Example Flow

1. Driver â†’ asks Cluster Manager for resources.
2. Cluster Manager â†’ assigns Worker Nodes.
3. Worker Nodes â†’ launch Executors.
4. Executors â†’ run tasks on data partitions.

### ğŸï¸ Real-life Analogy

Imagine a **factory ğŸ­**:

* **Spark Driver = Factory Manager ğŸ‘¨â€ğŸ’¼** (plans what needs to be produced).
* **Cluster Manager = HR/Admin Dept ğŸ—‚ï¸** (assigns workers to production lines).
* **Worker Nodes = Factory Workers ğŸ‘·** (do the actual manufacturing).
* **Executors = Workerâ€™s tools ğŸ› ï¸** (machines they use to produce results).

Without the **workers (Worker Nodes)**, no actual product (data processing) happens.

âœ¨ In short:
**Worker Nodes = The machines in the cluster that run executors to perform the actual computation and data storage in Spark.**

---

## **8. Executors**

* **Executors** are **JVM processes** launched on **Worker Nodes**.
* They are responsible for **executing tasks** and **storing data**.
* Executors live for the lifetime of a Spark application (unless dynamic allocation is enabled).

ğŸ‘‰ Think of executors as **workers inside a worker node**.

### ğŸ“‹ Roles & Responsibilities of Executors

1. **Task Execution** ğŸ§‘â€ğŸ’»

   * Run the tasks assigned by the **Task Scheduler**.

2. **Data Storage** ğŸ’¾

   * Hold in-memory data for RDD/DataFrame caching.
   * Store shuffle data for intermediate stages.

3. **Communication** ğŸŒ

   * Report back the task status/results to the Driver.
   * Exchange data with other executors during shuffle.

### ğŸ’¾ Executor Memory

* **Executor Memory** = Amount of memory allocated to each executor process.
* Used for:

  * **Storage memory** (caching RDDs/DataFrames).
  * **Execution memory** (sorting, shuffling, joins, aggregations).
  * **User memory** (custom objects, UDFs).

ğŸ‘‰ Configured using:

```bash
--executor-memory 4g
```

(allocates 4 GB RAM per executor).

âš ï¸ If executor memory is too small â†’ jobs may fail with **OutOfMemoryError**.

### âš™ï¸ Executor Cores

* **Executor Cores** = Number of **parallel tasks** an executor can run at the same time.
* Each task requires **1 core**.
* Example:

  * If an executor has **4 cores**, it can run **4 tasks simultaneously**.

ğŸ‘‰ Configured using:

```bash
--executor-cores 4
```

âš–ï¸ Rule of thumb: More cores â†’ more parallelism, but each needs memory.

### ğŸï¸ Real-life Analogy

Think of a **restaurant kitchen ğŸ´**:

* **Worker Node = Kitchen ğŸ‘¨â€ğŸ³**
* **Executor = Cook ğŸ‘©â€ğŸ³** (a worker in the kitchen).
* **Executor Memory = Ingredients & workspace ğŸ¥•ğŸ³** each cook has for preparing dishes.
* **Executor Cores = Number of hands/utensils âœ‹ğŸ´** the cook has â†’ determines how many dishes they can prepare at once.

If the cook (executor) has **too little memory (ingredients)**, theyâ€™ll fail to complete the dish.
If they have **more cores (utensils)**, they can cook multiple dishes in parallel.

âœ¨ In short:

* **Executors = JVM processes that do actual computation & caching**.
* **Executor Memory = RAM allocated to each executor**.
* **Executor Cores = Number of tasks each executor can run in parallel**.

---

## ğŸ“Š Spark Execution Flow Diagram

```
+-----------------------------+
|       Spark Driver          |
|  (SparkSession & Context)   |
+-------------+---------------+
              |
              v
    +-------------------+
    |   DAG Scheduler   |
    +-------------------+
              |
              v
    +-------------------+
    |  Task Scheduler   |
    +-------------------+
              |
              v
    +----------------------+
    |   Cluster Manager    |
    +----------------------+
              |
              v
    +---------------------+         +---------------------+
    |   Worker Node 1     |         |   Worker Node 2     |
    |  +--------------+   |         |  +--------------+   |
    |  |  Executor 1  |   |  <--->  |  |  Executor 2  |   |
    |  | Tasks + Core |   |         |  | Tasks + Core |   |
    +---------------------+         +---------------------+
```

---

## ğŸ” **Spark Execution Cycle and Data Movement**

### **Data Movement**

* **Transformations (lazy) âš¡**

  * `map`, `filter`, `flatMap`, etc. â†’ only build lineage in **Driver Memory**.
  * No actual execution until an **Action** is called.

* **Actions (trigger execution) ğŸš¦**

  * `collect()`, `count()`, `saveAsTextFile()` â†’ send execution plan to Executors.
  * Executors run tasks, store intermediate data in **Executor Memory**.
  * Shuffle operations (e.g., `reduceByKey`, `groupBy`) â†’ data moves between Executors across Worker Nodes.

* **Result Return ğŸ“¨**

ğŸ”¹ 1. Results Returned to the Driver

Executors return results to the **Driver** when:

* The **Action** requires the **final output** to be available in the Driver process.
  Examples:
* `collect()` â†’ Brings the **entire dataset** back to the Driver. âš ï¸ Risky for large data (can cause OOM).
* `take(n)` â†’ Brings the **first n rows** to the Driver.
* `count()` â†’ Sends just a **number (aggregate)** back to Driver.
* `reduce()` â†’ Sends the **final reduced value**.
* `first()` â†’ Sends the **first element**.

ğŸ“Œ **Analogy**:
Imagine the Driver as a **manager in the office ğŸ§‘â€ğŸ’¼**. If they ask *â€œGive me the total sales numberâ€*, the workers (executors) will compute and just **send back the number**. If the manager asks *â€œBring me all sales invoicesâ€* (`collect()`), the workers will **dump all papers on the managerâ€™s desk** (dangerous if itâ€™s millions of papers).

ğŸ”¹ 2. Results Stored in Executor Memory / Storage

Executors keep results **locally** in memory/disk when:

* The data is **cached** (`df.cache()` or `rdd.persist()`) â†’ Stored in executor memory/disk for reuse.
* **Intermediate shuffle data** â†’ During wide transformations (like `groupBy`, `join`), executors write shuffle files on disk and exchange data with other executors (not returned to driver).
* Iterative algorithms (like MLlib training) â†’ Executors hold partitions in memory for repeated access.

ğŸ“Œ **Analogy**:
Executors are like **team members ğŸ§‘â€ğŸ”§**. If they know they will need some results again, they **keep copies of their work in their drawers (executor memory)** instead of giving everything to the manager (driver).

ğŸ”¹ 3. Results Stored in HDFS, S3, DB

Executors donâ€™t normally send results back to the Driver when the end goal is writing output to a database. Instead, they write directly from Executors â†’ Database, in parallel.

This happens in situations like:

* â€ Saving Transformed Data into a Database
  * Example: df.write.jdbc(...) or df.write.format("jdbc").save()
  * Executors partition the data and each Executor writes its partition directly to the database table.
  * ğŸ“Œ Use case: You clean and enrich customer data and then save results into a PostgreSQL/MySQL/SQL Server database.
 
* â Streaming Output to a Database
  * In Structured Streaming, Executors continuously process micro-batches.
  * For each batch, Executors directly push processed rows into a database sink.
  * Example: Writing streaming results to Cassandra, HBase, MongoDB, or JDBC.
  * ğŸ“Œ Use case: Real-time IoT sensor data â†’ aggregated â†’ stored in Cassandra.
 
* â‚ ETL Workflows
  * In ETL pipelines, Spark Executors read raw data, transform it, and write final curated data to OLTP/OLAP databases.
  * Executors handle the parallel writes, ensuring data is distributed evenly (or based on partitioning).
  * ğŸ“Œ Use case: ETL from raw logs â†’ Spark transformation â†’ Store summary in Redshift/BigQuery.
 
* âƒ Batch Jobs that Persist Results
  * Jobs like daily aggregation, customer segmentation, or financial reports.
  * Executors compute partitions â†’ directly insert/update database rows.
  * Driver never collects full results in memory (avoids OOM).
  * ğŸ“Œ Use case: Banking transactions â†’ daily rollups â†’ written back to Oracle DB.

---

# ğŸ”„ Data Flow (Simplified Path)

1. **User â†’ Driver** ğŸ§‘â€ğŸ’»: You write Spark code.
2. **Driver â†’ Driver Memory** ğŸ§ ğŸ’¾: Stores DAG (lineage, plan).
3. **Driver â†’ DAG Scheduler â†’ Task Scheduler** ğŸ“‘ğŸšš: Splits into stages & tasks.
4. **Task Scheduler â†’ Cluster Manager** ğŸŒ: Requests resources.
5. **Cluster Manager â†’ Worker Node** ğŸ­: Launches Executors.
6. **Worker Node â†’ Executor** ğŸ‘·: Runs tasks.
7. **Executor â†’ Executor Memory & Cores** ğŸ’¾ğŸ”¢: Stores partitions in memory, uses cores for parallelism.
8. **Executors â†” Executors** ğŸ”„: Shuffle data exchange (joins, reduce, groupBy).
9. **Executor â†’ Driver** ğŸ“¨: Sends results back (for actions like collect).

---

# ğŸï¸ Real-life Analogy

Imagine **online food delivery ğŸ”ğŸšš**:

* **User** = You place an order (Spark code).
* **Driver (Restaurant Manager ğŸ‘¨â€ğŸ³)** = Plans what needs to be cooked.
* **Driver Memory** = Recipe book ğŸ“– (stores plan, lineage).
* **DAG Scheduler** = Splits into steps: chopping, cooking, packing.
* **Task Scheduler** = Assigns tasks to specific cooks.
* **Cluster Manager** = HR assigns workers and kitchens.
* **Worker Node (Kitchen ğŸ­)** = Place where cooking happens.
* **Executor (Cook ğŸ‘©â€ğŸ³)** = Cook who prepares dish.
* **Executor Memory (Ingredients & workspace ğŸ³)** = Storage for preparation.
* **Executor Cores (Hands/utensils âœ‹ğŸ´)** = How many dishes a cook can make in parallel.
* **Shuffle (Passing ingredients between cooks)** = Sharing between executors.
* **Action (Deliver order)** = Food finally reaches you.

---

âœ¨ In short:

* **Transformations** â†’ build lineage in Driver Memory.
* **Actions** â†’ trigger execution, tasks sent to Executors.
* **Executors** â†’ process partitions using cores & memory.
* **Data** â†’ moves between executors (shuffle) and back to Driver.
