Apache Spark follows a **master-slave distributed architecture** that allows it to process large-scale data in a parallel and fault-tolerant manner. 

<img width="600" height="350" alt="image" src="https://github.com/user-attachments/assets/9d533362-6dbd-482d-959a-d542feef15d4" />

---

# üîß **Spark Architecture Core Components**

## **1. Spark Driver**

The **Driver** in Apache Spark is like the **‚ÄúBrain üß†‚Äù** of a Spark application.
It runs the main function of your program and is responsible for coordinating the entire job execution.

### üìã Roles & Responsibilities of Spark Driver

1. **Program Execution** üìù

   * Runs the **main()** method of your Spark application.
   * Defines RDDs, DataFrames, transformations, and actions.

2. **Cluster Communication** üåê

   * Connects with the **Cluster Manager** (YARN, Mesos, or Standalone).
   * Requests resources for Executors.

3. **Task Scheduling** üìÖ

   * Breaks the job into **stages** and **tasks**.
   * Assigns these tasks to **executors**.

4. **Metadata Management** üìÇ

   * Keeps track of RDD lineage, DAG (Directed Acyclic Graph), and execution plans.

5. **Result Collection** üì¨

   * Collects results from executors.
   * Returns final output to the user or writes to storage.

### üíæ Driver Memory

* **Driver memory** = Memory allocated to the Driver process.
* Used for:

  * Storing metadata (RDD lineage, DAG).
  * Holding small results from executors.
  * Keeping variables and SparkSession context.
* Controlled by:

  ```
  --driver-memory 4g
  ```

  (This allocates 4 GB RAM to the driver).

‚ö†Ô∏è If driver memory is too small ‚Üí job may fail with **OutOfMemoryError**.

### üèéÔ∏è Real-life Analogy

Imagine **a race**:

* **Driver (Car Driver üßë‚Äç‚úàÔ∏è)** = Spark Driver

  * Plans the route (DAG).
  * Decides pit stops (stages).
  * Communicates with team.
  * Monitors the race and ensures car (executors) is performing.

* **Car üöó** = Cluster resources (executors).

* **Pit Crew üë®‚Äçüîß** = Cluster Manager (allocates resources).

* **Fuel & Tires ‚õΩ** = Data & Tasks.

If the **driver (brain)** doesn‚Äôt have enough **focus/energy (memory)**, the whole race collapses.

---

## **2. SparkSession**

* **SparkSession** is the **entry point** to any Spark application.
* Think of it as the **‚ÄúGateway üö™‚Äù** to interact with Spark.
* It allows you to use **RDDs, DataFrames, Datasets, and SQL** all in one place.

üëâ Introduced in **Spark 2.0** to unify `SQLContext`, `HiveContext`, and `SparkContext`.

### üìã Roles & Responsibilities of SparkSession

1. **Application Entry Point** üîë

   * Provides a single point to connect with Spark.

2. **Resource Manager** ‚öôÔ∏è

   * Manages the Spark context (driver + executors).
   * Handles configuration settings.

3. **DataFrame & Dataset API** üìä

   * Allows you to create DataFrames and Datasets.

4. **SQL Queries** üóÇÔ∏è

   * Lets you run SQL queries on structured data using `.sql()`.

5. **Catalog Access** üìö

   * Keeps track of tables, databases, and temporary views.

### üñ•Ô∏è Example in Python (PySpark)

```python
from pyspark.sql import SparkSession

# Create SparkSession
spark = SparkSession.builder \
    .appName("MyApp") \
    .config("spark.some.config.option", "config-value") \
    .getOrCreate()

# Example: Create DataFrame
data = [("Bharath", 28), ("Anu", 25)]
df = spark.createDataFrame(data, ["Name", "Age"])

df.show()

# Example: SQL Query
df.createOrReplaceTempView("people")
spark.sql("SELECT * FROM people WHERE Age > 26").show()
```

### üèéÔ∏è Real-life Analogy

Think of Spark as a **big airport ‚úàÔ∏è**:

* **SparkSession = Airport Terminal üõ´**

  * Main entry point to the airport.
  * Provides access to flights (RDDs, DataFrames, SQL).
  * Has gates to different services (SQL, Hive, Catalog).

* **Flights ‚úàÔ∏è** = DataFrames / RDDs.

* **Air Traffic Control üõ∞Ô∏è** = Driver + Cluster Manager.

Without the **terminal (SparkSession)**, you cannot reach flights (Spark features).

---

## **3. SparkContext**

* **SparkContext** is the **entry point** to the **low-level Spark Core API**.
* It represents a **connection between your Spark application and the Spark cluster**.
* Before Spark 2.0, every application **had to create a SparkContext** to talk to the cluster.
* After Spark 2.0 ‚Üí you usually use **SparkSession**, which internally creates a SparkContext for you.

### üìã Roles & Responsibilities of SparkContext

1. **Cluster Connection** üåê

   * Establishes connection with the cluster manager (YARN, Mesos, Standalone).

2. **Resource Allocation** ‚öôÔ∏è

   * Requests resources (executors, CPUs, memory) from the cluster manager.

3. **RDD Creation** üìä

   * Used to create **RDDs** (Resilient Distributed Datasets), the core data structure of Spark.

4. **Task Distribution** üìÖ

   * Sends tasks (jobs broken into stages) to executors.

5. **Job Monitoring** üìà

   * Monitors the execution of jobs, tracks progress, and handles failures.

### üñ•Ô∏è Example in Python (PySpark)

```python
from pyspark import SparkContext

# Create SparkContext
sc = SparkContext("local", "MyApp")

# Create RDD
data = [1, 2, 3, 4, 5]
rdd = sc.parallelize(data)

# Perform operation
result = rdd.map(lambda x: x * 2).collect()
print(result)  # [2, 4, 6, 8, 10]
```

üëâ In modern Spark, you usually do:

```python
sc = spark.sparkContext
```

because `SparkSession` manages it for you.

### üèéÔ∏è Real-life Analogy

Think of Spark as a **railway network üöÜ**:

* **SparkContext = Station Master üöâ**

  * Connects the station (application) to the railway network (cluster).
  * Assigns tracks (executors) for trains (tasks).
  * Ensures smooth operation of trains (job execution).

* **Trains üöÇ** = Jobs & Tasks.

* **Passengers üë®‚Äçüë©‚Äçüëß** = Data (RDD elements).

* **Railway HQ üè¢** = Cluster Manager (allocates tracks/resources).

Without the **Station Master (SparkContext)**, the trains cannot run.

‚ú® In short:
**SparkContext = The ‚Äúengine connector‚Äù that lets your Spark application talk to the cluster and work with RDDs.**
üëâ After Spark 2.0, it‚Äôs wrapped inside **SparkSession**, so you don‚Äôt create it directly in most cases.

---

## **4. DAG Scheduler**

Great one üöÄ ‚Äî the **DAG Scheduler** is an important piece in Spark‚Äôs internal execution flow. Let‚Äôs break it down step by step with icons, roles, and a real-life analogy.

### üß© What is DAG Scheduler?

* **DAG Scheduler** is the component in Spark that converts a **logical execution plan** into a **physical execution plan**.
* It **organizes jobs into stages**, and each stage into **tasks**.
* DAG stands for **Directed Acyclic Graph** ‚Üí a flow of operations with no cycles.

üëâ It sits between the **user code** and the **Task Scheduler**.

### üìã Roles & Responsibilities of DAG Scheduler

1. **Job Division** üóÇÔ∏è

   * Splits a Spark job into **stages** based on shuffle boundaries.

2. **Stage Creation** üìë

   * Creates **ResultStage** (for final actions like `collect()`) and **ShuffleMapStage** (for shuffle operations).

3. **Task Set Preparation** üì¶

   * Breaks each stage into **tasks**, one task per partition.

4. **Task Submission** üöö

   * Sends task sets to the **Task Scheduler**, which then distributes them to executors.

5. **Failure Handling** üîÑ

   * If a stage fails, retries it (default 4 times).

### ‚öôÔ∏è Example Flow

Suppose you run:

```python
rdd.map(lambda x: x+1).filter(lambda x: x>5).reduceByKey(lambda a,b: a+b).collect()
```

* `map` & `filter` ‚Üí narrow transformations (can be in one stage).
* `reduceByKey` ‚Üí shuffle needed ‚Üí new stage.
* `collect` ‚Üí triggers a **job**.

**DAG Scheduler** will:

* Break into **two stages** (before and after shuffle).
* Create tasks for each partition.
* Send tasks to **Task Scheduler ‚Üí Executors**.

### üèéÔ∏è Real-life Analogy

Think of a **food delivery system üçîüöö**:

* **Customer Order (Action like collect)** = Job

* **DAG Scheduler** = **Restaurant Manager üë®‚Äçüç≥**

  * Splits the order into **stages**: cooking, packing, delivery.
  * Assigns tasks (cut veggies, cook patty, pack meal).
  * Makes sure tasks flow logically (cook before pack, pack before delivery).

* **Task Scheduler** = **Waiters/Delivery Dispatch üö¥**

  * Takes each task set and sends them to the right worker (executors).

Without the **manager (DAG Scheduler)**, the workflow would be chaotic.

‚ú® In short:
**DAG Scheduler = The planner üß† that converts your logical operations into a stage-wise execution plan and hands over tasks to the Task Scheduler.**

---

## **5. Task Scheduler**

* Breaks **stages** into **tasks** (smallest units of execution).
* Sends tasks to **executors** via **Cluster Manager**.

---

## **6. Cluster Manager**

* Allocates resources across applications.
* Spark supports:

  * **Standalone** (built-in)
  * **YARN**
  * **Mesos**
  * **Kubernetes**

---

## **7. Worker Nodes**

* Nodes in the cluster that run the tasks.
* Host **executors**.

---

## **8. Executors**

* JVM processes on worker nodes.
* Each application has **its own executors**.
* Responsibilities:

  * Execute tasks.
  * Store RDD partitions in memory/cache.
  * Report status to the **driver**.

---

## **9. Cores**

* Threads available inside executors to run multiple tasks in parallel.

---

## üìä Spark Execution Flow Diagram

```
+-----------------------------+
|       Spark Driver          |
|  (SparkSession & Context)   |
+-------------+---------------+
              |
              v
    +-------------------+
    |   DAG Scheduler   |
    +-------------------+
              |
              v
    +-------------------+
    |  Task Scheduler   |
    +-------------------+
              |
              v
    +----------------------+
    |   Cluster Manager    |
    +----------------------+
              |
              v
    +---------------------+         +---------------------+
    |   Worker Node 1     |         |   Worker Node 2     |
    |  +--------------+   |         |  +--------------+   |
    |  |  Executor 1  |   |  <--->  |  |  Executor 2  |   |
    |  | Tasks + Core |   |         |  | Tasks + Core |   |
    +---------------------+         +---------------------+
```

---

## üîÅ **Spark Execution Cycle (Job ‚Üí Stage ‚Üí Task)**

1. **User Code (Driver)**:

   * Invokes transformations/actions on RDD/DataFrame.
   * Triggers job execution.

2. **DAG Scheduler**:

   * Splits the job into **stages** (based on shuffles).

3. **Task Scheduler**:

   * Divides each stage into **tasks** (per partition).
   * Schedules them on executors via **Cluster Manager**.

4. **Executors**:

   * Run tasks and return results to **Driver**.

---

## ‚öôÔ∏è **Resource Allocation: Executors, Cores, Memory**

### üß† Executors

* Number of executors = number of JVMs running on worker nodes.
* Each executor:

  * Has multiple **cores**.
  * Allocated a fixed **memory** chunk (e.g., `--executor-memory 4G`).

### üîÑ Cores

* Each core = one concurrent **task** execution thread.
* E.g., 5 cores per executor = 5 tasks can run in parallel.

### üíæ Memory

* Split into:

  * **Storage Memory**: for caching RDDs/dataframes.
  * **Execution Memory**: for shuffle, joins, aggregations.
  * **User Memory**: for custom user data.
  * **Reserved Memory**: Spark internal overhead.

---

## ‚öñÔ∏è Example Resource Layout

**Cluster with 3 Worker Nodes**

* Each node has: 16 cores, 64 GB RAM.

### You request:

* `--num-executors 6`
* `--executor-cores 4`
* `--executor-memory 16G`

Then:

* 2 executors per worker node.
* Each executor:

  * Runs 4 parallel tasks.
  * Uses 16 GB RAM.

---

## ‚úÖ Summary

| Component       | Role                                 |
| --------------- | ------------------------------------ |
| SparkSession    | Entry point                          |
| SparkContext    | Connects to Cluster Manager          |
| DAG Scheduler   | Builds execution plan                |
| Task Scheduler  | Sends tasks to executors             |
| Cluster Manager | Allocates resources                  |
| Worker Nodes    | Run executors                        |
| Executors       | Run tasks and cache data             |
| Cores           | Enable task parallelism in executors |
