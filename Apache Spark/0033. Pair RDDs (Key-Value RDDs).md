# 🔑 **Pair RDDs (Key-Value RDDs)**

## 🧩 What is a Pair RDD?

* A **Pair RDD** is just an RDD where each element is a **key-value pair (K, V)**.
* Keys are used for grouping, reducing, or joining data.
* Values are the associated data items.

👉 Formally:

```
RDD[(K, V)]
```

where `K = key`, `V = value`.

---

## ⚡ Why Pair RDDs?

* They enable **distributed key-based operations**, like in **MapReduce**.
* Spark provides special transformations/actions optimized for Pair RDDs.

Examples:

* `reduceByKey()` → Combine values with the same key.
* `groupByKey()` → Group values by key.
* `join()` → Join two RDDs by key.
* `sortByKey()` → Sort by key.

---

## 📊 Example: Creating a Pair RDD

```python
from pyspark import SparkContext
sc = SparkContext("local", "PairRDDExample")

data = [("apple", 2), ("banana", 3), ("apple", 4)]
rdd = sc.parallelize(data)

print(rdd.collect())
```

**Output:**

```
[('apple', 2), ('banana', 3), ('apple', 4)]
```

Here, `rdd` is a **Pair RDD** with key = fruit, value = number.

---

## 📊 Example: `reduceByKey()`

Sum values for each key:

```python
result = rdd.reduceByKey(lambda a, b: a + b)
print(result.collect())
```

**Output:**

```
[('banana', 3), ('apple', 6)]
```

---

## 📊 Example: `groupByKey()`

Group values by key:

```python
grouped = rdd.groupByKey().mapValues(list)
print(grouped.collect())
```

**Output:**

```
[('banana', [3]), ('apple', [2, 4])]
```

---

## 📊 Example: `sortByKey()`

```python
sorted_rdd = rdd.sortByKey()
print(sorted_rdd.collect())
```

**Output:**

```
[('apple', 2), ('apple', 4), ('banana', 3)]
```

---

# 🖼️ Analogy

Think of a Pair RDD as a **phonebook** 📖:

* **Key** → Person’s name.
* **Value** → Phone number(s).

Operations like `reduceByKey()` = merging multiple phone numbers for the same person.

---

# 🔄 Summary

* Pair RDD = RDD of **(key, value)** pairs.
* Enables **special transformations** like `reduceByKey`, `groupByKey`, `join`, `sortByKey`.
* Widely used in **aggregation, joins, and distributed processing**.

---

# 🔑 **keys() and values() in Spark**

## 🧩 What are they?

* **`keys()`** → Returns an RDD containing only the keys from a Pair RDD.
* **`values()`** → Returns an RDD containing only the values from a Pair RDD.

👉 They are **transformations** (lazy operations) and produce a new RDD.

---

## ⚡ Syntax

```python
rdd.keys()
rdd.values()
```

---

## 📊 Example: Using `keys()` and `values()`

```python
from pyspark import SparkContext
sc = SparkContext("local", "KeysValuesExample")

data = [("apple", 2), ("banana", 3), ("apple", 4)]
rdd = sc.parallelize(data)

keys_rdd = rdd.keys()
values_rdd = rdd.values()

print("Keys:", keys_rdd.collect())
print("Values:", values_rdd.collect())
```

**Output:**

```
Keys: ['apple', 'banana', 'apple']
Values: [2, 3, 4]
```

---

## 📊 Example: Counting Keys

You can use `keys()` with transformations like `distinct()`:

```python
unique_keys = rdd.keys().distinct()
print(unique_keys.collect())
```

**Output:**

```
['apple', 'banana']
```

---

## 📊 Example: Summing Values

Instead of manually extracting values, you can just call:

```python
total = rdd.values().sum()
print(total)
```

**Output:**

```
9
```

---

# 🖼️ Analogy

Imagine a **dictionary** 📖 of `word → meaning`:

* `keys()` = just get the **words**.
* `values()` = just get the **meanings**.

Example:

```
{'apple': 2, 'banana': 3, 'apple': 4}

keys() → ['apple', 'banana', 'apple']
values() → [2, 3, 4]
```

---

# 🔄 Summary

* `keys()` → Extracts **all keys** from a Pair RDD.
* `values()` → Extracts **all values** from a Pair RDD.
* Both return new RDDs, often used before `distinct()`, `countByValue()`, or aggregations.
