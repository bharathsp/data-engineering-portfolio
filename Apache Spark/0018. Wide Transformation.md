## 🌐 What is a **Wide Transformation** in Spark?

A **Wide Transformation** in Spark is a transformation where **data from multiple partitions is required to generate a single output partition**. This means data **must be shuffled** across the cluster.

> 🔁 Wide transformations trigger **shuffle operations**, involving **disk I/O, network I/O**, and **repartitioning**.

---

## 🔄 What Happens During a Wide Transformation?

1. Spark **marks the transformation as a shuffle dependency**.
2. **Data is redistributed** (shuffled) across the cluster based on a key.
3. Spark **creates a new stage** in the DAG, separating narrow and wide transformations.
4. The shuffle involves:

   * Writing to disk,
   * Moving data over the network,
   * Sorting & merging data by key.

This makes wide transformations **more expensive** than narrow ones.

---

## 📋 Common Wide Transformation Functions

| Function        | Description                                             |
| --------------- | ------------------------------------------------------- |
| `groupBy()`     | Groups records by key, causes shuffle                   |
| `join()`        | Combines datasets based on key (if not broadcasted)     |
| `distinct()`    | Removes duplicates (shuffles data)                      |
| `reduceByKey()` | Reduces values by key across partitions                 |
| `repartition()` | Evenly distributes data across new number of partitions |
| `coalesce()`    | Reduces the number of partitions (less shuffling)       |
| `sortBy()`      | Sorts data globally across partitions                   |

---

## 🔧 PySpark Code Example – Wide Transformation

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("WideTransformationExample").getOrCreate()

# Load sample data
data = [("A", 10), ("B", 20), ("A", 30), ("C", 40), ("B", 50)]
df = spark.createDataFrame(data, ["category", "value"])

# Wide transformation: groupBy (causes shuffle)
grouped_df = df.groupBy("category").sum("value")

# Action to trigger execution
grouped_df.show()
```

---

### 🧠 DAG Explanation (Internally)

* `groupBy()` creates a **shuffle dependency**.
* Spark **writes intermediate data to disk**.
* Executors **read and combine** data by key from all partitions.

```text
Stage 1: mapPartitions --> SHUFFLE --> Stage 2: reduceByKey (group)
```

---

## ✅ Advantages of Wide Transformations

| Advantage            | Description                                  |
| -------------------- | -------------------------------------------- |
| 🔗 Essential         | Needed for aggregation, joins, and analytics |
| 🧠 Distributed logic | Combines and computes across large datasets  |
| 💡 Supports features | Enables ML prep, summaries, joins, and more  |

---

## ❌ Disadvantages of Wide Transformations

| Disadvantage           | Description                                         |
| ---------------------- | --------------------------------------------------- |
| ⚠️ Expensive           | Involves **disk I/O**, **network I/O**, and **CPU** |
| 🐢 Slower              | Requires stage boundaries and data reshuffling      |
| 🔥 Susceptible to skew | Can fail or slow down due to **data skew**          |

---

## 📈 Performance Tip

To improve performance with wide transformations:

* Use **`reduceByKey`** instead of `groupByKey`
* Broadcast small tables when possible (`broadcast()` in joins)
* Handle data skew (e.g., **salting**)
* Cache reused datasets before shuffles
* Avoid unnecessary `repartition()`

---

## ✅ Summary

| Feature              | Wide Transformation                            |
| -------------------- | ---------------------------------------------- |
| Involves Shuffle?    | ✅ Yes                                          |
| Stage Breaks in DAG? | ✅ Yes (creates new stage)                      |
| Performance Cost     | ❌ High (due to I/O & network shuffle)          |
| Use Cases            | `groupBy`, `join`, `distinct`, `reduceByKey`   |
| Best Practice        | Minimize shuffle; use broadcast for small data |
