# Consider a scenario where your spark job has 5 lines of code. Out of these 5 lines of code, 3 lines are narrow transformation, 1 line is wide transformation, and 1 line is action. How many jobs and stages will be created in this case?

## **Step 1: Understand the definitions**

* **Narrow Transformation** 🔹: Data from each partition is used to create data for **only one partition** in the next stage (e.g., `map`, `filter`). These do **not** cause a shuffle, so they remain in the same stage.
* **Wide Transformation** 🔹: Data from one partition may be sent to **multiple partitions** in the next stage (e.g., `groupByKey`, `reduceByKey`). These **do cause** a shuffle and therefore create a **stage boundary**.
* **Action** 🎯: Triggers the execution of the DAG, producing **at least one job**.

---

## **Step 2: Apply it to the scenario**

You have:

1. **Line 1:** Narrow transformation
2. **Line 2:** Narrow transformation
3. **Line 3:** Narrow transformation
4. **Line 4:** Wide transformation (**shuffle**)
5. **Line 5:** Action

---

## **Step 3: Determine Jobs**

* An **action** triggers a **job**.
* You have **only 1 action**, so **only 1 job** is created.

**✅ Number of Jobs = 1**

---

## **Step 4: Determine Stages**

* All narrow transformations before a wide transformation belong to the **same stage**.
* The wide transformation creates a **shuffle boundary**, starting a new stage.
* After the wide transformation, there may be further narrow transformations before the action — they remain in the same stage as the wide transformation’s output stage.

**In your case:**

* **Stage 1:** Narrow → Narrow → Narrow (before shuffle)
* **Stage 2:** Wide transformation → Action

**✅ Number of Stages = 2**

---

### **Final Answer**

* **Jobs:** **1**
* **Stages:** **2**

---

# **Multiple Actions Triggering Multiple Jobs**

> You have a Spark application with the following steps:
>
> * Line 1: Read a DataFrame from a CSV file.
> * Line 2: Apply `filter()` (narrow transformation).
> * Line 3: Apply `groupBy()` followed by `agg()` (wide transformation).
> * Line 4: Call `count()` (action).
> * Line 5: Call `collect()` (action) on the original filtered DataFrame.
>
> **Question:** How many jobs and stages will be created, and why?

1️⃣ **Read CSV** → *creates initial DataFrame*
2️⃣ **filter()** → *narrow transformation*
3️⃣ **groupBy().agg()** → *wide transformation (shuffle)*
4️⃣ **count()** → *action*
5️⃣ **collect()** → *action* (but on the **filtered DataFrame**, not aggregated one)

---

## **Step-by-Step Execution Flow**

### **Step 1 & 2 — Read + Filter**

* **Read CSV** is not an action; Spark just builds the logical plan.
* **filter()** is **narrow** (no shuffle).

💡 At this stage → **still no job triggered**.

---

### **Step 3 — groupBy + agg**

* **groupBy().agg()** is a **wide transformation** → introduces **shuffle boundary**.
* This will split execution into **two stages** when an action is triggered later.

---

### **Step 4 — count()**

* First **action** → **triggers Job 1**.
* Execution plan:

  * **Stage 1:** Read CSV → Filter → Shuffle Write (groupBy).
  * **Stage 2:** Shuffle Read → Aggregate → Count.

📌 **Job 1 → 2 stages**.

---

### **Step 5 — collect() on filtered DataFrame**

* This uses the DataFrame **before groupBy()**.
* No shuffle needed here — only **narrow transformations** from Read → Filter → Collect.

📌 **Job 2 → 1 stage**.

---

## **Final Answer**

* **Total Jobs:** **2**

  * Job 1 → triggered by `count()` on aggregated data.
  * Job 2 → triggered by `collect()` on filtered data.

* **Total Stages:** **3**

  * Job 1 → **Stage 1** (narrow ops before shuffle) + **Stage 2** (shuffle read + agg).
  * Job 2 → **Stage 3** (narrow ops only).

---

## **Icon Visualization**

```
          ┌─────────────┐
          │  Read CSV   │
          └─────┬───────┘
                │ Narrow
          ┌─────▼───────┐
          │   filter()  │
          └─────┬───────┘
                │ Wide (Shuffle)
                ▼
          ┌─────────────┐
          │ groupBy+agg │
          └─────┬───────┘
                │
          ┌─────▼───────┐
          │   count()   │  ← **Job 1**  
          └─────────────┘
          Stage 1 → Stage 2

   (Parallel branch for filtered DataFrame)
          ┌─────────────┐
          │  filter()   │
          └─────┬───────┘
                ▼
          ┌─────────────┐
          │  collect()  │  ← **Job 2**  
          └─────────────┘
          Stage 3
```

---

✅ **Final Table:**

| Action      | Job | Stages |
| ----------- | --- | ------ |
| `count()`   | 1   | 2      |
| `collect()` | 2   | 1      |
| **Total**   | 2   | 3      |

---

# **Caching Between Actions**

> You have a DataFrame and perform these operations:
>
> * Line 1: Read data from Parquet.
> * Line 2: `filter()` (narrow transformation).
> * Line 3: `map()` (narrow transformation).
> * Line 4: `cache()` the result.
> * Line 5: `count()` (action).
> * Line 6: `collect()` (action) on the cached DataFrame.
>
> **Question:** How many jobs and stages will be created, and what difference does caching make here?

## **Step-by-Step Execution Flow**

### **Step 1 — Read Parquet**

* Reading Parquet is a **lazy** operation.
* No job is triggered yet — Spark just builds the logical plan.

---

### **Step 2 & 3 — filter() + map()**

* Both are **narrow transformations** (no shuffle).
* Still **no job triggered** yet — plan just keeps extending.

---

### **Step 4 — cache()**

* `cache()` is also **lazy** — it only marks the DataFrame to be persisted **when an action runs**.
* No execution happens yet.

---

### **Step 5 — count()**

* First **action** → triggers **Job 1**.
* Job execution:

  * **Stage 1:** Read Parquet → Filter → Map → Store in cache → Count.
  * Since no wide transformation is involved, only **1 stage** is created.

📌 **Job 1 → 1 stage**.

---

### **Step 6 — collect() on cached DataFrame**

* Second **action** → triggers **Job 2**.
* Because the DataFrame is **cached**, Spark will **reuse the cached data** instead of recomputing from source.
* Only **1 stage** is executed — data is fetched directly from cache into driver.

📌 **Job 2 → 1 stage**.

---

## **Without Caching**

If there was **no cache()**, the second action (`collect()`) would have to **recompute** from the source:

* Read Parquet → Filter → Map again.
* Still 1 stage per job, but **data would be read & processed twice**.

With caching:

* Data is computed once (Job 1) and stored in memory/disk.
* Job 2 simply reads from cache — **faster execution**.

---

## **Final Answer**

| Action      | Job | Stages | Notes                                                       |
| ----------- | --- | ------ | ----------------------------------------------------------- |
| `count()`   | 1   | 1      | Reads from source, applies transformations, stores in cache |
| `collect()` | 2   | 1      | Reads from cache                                            |
| **Total**   | 2   | 2      | Caching prevents recomputation                              |

---

## **Icon Visualization**

```
         ┌───────────────┐
         │  Read Parquet │
         └───────┬───────┘
                 │ Narrow
         ┌───────▼───────┐
         │   filter()    │
         └───────┬───────┘
                 │ Narrow
         ┌───────▼───────┐
         │    map()      │
         └───────┬───────┘
                 │ Cache
         ┌───────▼───────┐
         │   count()     │ ← **Job 1** (Stage 1)
         └───────────────┘

         ┌───────────────┐
         │  collect()    │ ← **Job 2** (Stage 2 - Reads from cache)
         └───────────────┘
```

---

✅ **Key point interviewers look for here:**
Caching **doesn’t reduce job count**, but it **avoids recomputation** in later jobs, improving performance for iterative or repeated actions.

---

# **Multiple Wide Transformations Before Action**

> You have:
>
> * Line 1: Read from JSON.
> * Line 2: `filter()` (narrow transformation).
> * Line 3: `join()` with another DataFrame (wide transformation).
> * Line 4: `groupBy()` and `agg()` (wide transformation).
> * Line 5: `show()` (action).
>
> **Question:** How many stages are created? Will there be one or two wide shuffles?

Let’s break down this **Multiple Wide Transformations Before Action** case carefully,
because this is one of those interview questions where people often assume **“wide transformations always cause separate stages”** — but that’s not always true.

---

## **Step-by-Step Execution Flow**

### **Step 1 — Read from JSON**

* Reading JSON is **lazy**, no execution yet.

---

### **Step 2 — filter()**

* **Narrow transformation** — no shuffle boundary introduced.
* Still **part of the same stage** when action runs.

---

### **Step 3 — join()**

* **Wide transformation** — introduces **shuffle boundary**.
* This means Spark will create:

  * **Stage 1:** Read + Filter + Shuffle Write.
  * **Stage 2:** Shuffle Read + Join output.

---

### **Step 4 — groupBy().agg()**

* This is **another wide transformation**.
* Whether it creates a **new shuffle** depends on **whether Spark can pipeline it with the previous stage**:

💡 In this case:

* `join()` → produces output that **needs to be grouped again** (keys may be different from join shuffle keys).
* So Spark must **reshuffle** for `groupBy()`.
* This creates **another stage**.

---

### **Step 5 — show()**

* First **action** triggers execution of all pending transformations.
* Execution plan now has **two shuffle boundaries**, so:

📌 **Stage 1:** Read → Filter → Shuffle Write (for join).
📌 **Stage 2:** Shuffle Read (join) → Shuffle Write (for groupBy).
📌 **Stage 3:** Shuffle Read (groupBy) → Aggregation → Show.

---

## **Final Answer**

* **Number of stages:** **3**

  * Stage 1: Before join shuffle.
  * Stage 2: After join, before groupBy shuffle.
  * Stage 3: After groupBy shuffle.

* **Number of wide shuffles:** **2**

  * First: For `join()`
  * Second: For `groupBy()`.

---

## **Icon Visualization**

```
Stage 1 (Narrow Ops)
┌────────────┐ → filter() → shuffle write (join)
│ Read JSON  │
└────────────┘

Stage 2 (First Shuffle Read)
shuffle read (join) → shuffle write (groupBy)

Stage 3 (Second Shuffle Read)
shuffle read (groupBy) → aggregation → show()
```

---

✅ **Key Interview Insight:**
Not every sequence of wide transformations can be **combined** —
if the second wide transformation uses a **different partitioning key** than the first shuffle, Spark will **reshuffle** and create another stage.

---

# **Action Inside a Loop**

> You have:
>
> * Read a DataFrame from a CSV file.
> * Apply `filter()` (narrow transformation).
> * Inside a for loop (runs 3 times), you call `count()` each time.
>
> **Question:** How many jobs will run in total? How many stages are created?

---

## **Step-by-Step Reasoning**

### **Step 1 — Read CSV**

* Lazily creates the DataFrame — **no job yet**.

---

### **Step 2 — filter()**

* **Narrow transformation**, still **lazy**.
* Plan so far: `Read → Filter` (no shuffle needed).

---

### **Step 3 — for loop with count() 3 times**

* `count()` is an **action** — each call triggers a **new job**.
* Since **no caching** is used, Spark recomputes the entire lineage each time.
* For each job:

  * **Stage 1:** Read CSV → Filter → Count (only narrow transformations, so just 1 stage).

---

## **Final Answer**

* **Number of jobs:** **3** (one per `count()` call).
* **Number of stages:** **3 total** (1 stage per job × 3 jobs).
* **Reason:** No caching → full recomputation for each action.

---

## **Icon Visualization**

```
for i in range(3):
    count()  ← Action → New Job Every Time
```

**Job 1:**
📄 Read CSV → Filter → Count (Stage 1)

**Job 2:**
📄 Read CSV → Filter → Count (Stage 2)

**Job 3:**
📄 Read CSV → Filter → Count (Stage 3)

---

💡 **With cache():**

* If you had called `.cache()` after filter(),

  * First `count()` → Job 1 (Stage 1) would compute & store in cache.
  * Next two `count()` calls → Job 2 & Job 3 would simply read from cache (still separate jobs, but no recomputation).

---

✅ **Key Interview Insight:**
Spark does **not** automatically cache results between actions.
Multiple actions on the same DataFrame without caching → multiple jobs + full recomputation each time.

---

# **Persist with Different Storage Levels**

> You have:
>
> * Read data.
> * Apply a series of narrow transformations.
> * `persist(StorageLevel.DISK_ONLY)`.
> * Call `count()` (action).
> * Apply another transformation and call `collect()` (action).
>
> **Question:** How many jobs and stages will be created? How would the behavior differ if you used `MEMORY_ONLY` instead?

Let’s unpack this **Persist with Different Storage Levels** case step by step,
because interviewers often want you to reason about **persistence + storage level effects**.

---

## **Step-by-Step Execution Flow**

### **Step 1 — Read Data**

* Lazily creates the DataFrame — no job yet.

---

### **Step 2 — Narrow Transformations**

* Still lazy — no job triggered yet.
* No shuffle introduced.

---

### **Step 3 — persist(StorageLevel.DISK\_ONLY)**

* Marks the DataFrame to be stored **on disk only** when computed for the first time.
* No execution yet — persist itself doesn’t trigger a job.

---

### **Step 4 — count()**

* First **action** → triggers **Job 1**.
* Execution plan:

  * **Stage 1:** Read → Narrow transformations → Write to Disk (persist) → Count.
  * Only **1 stage** because all transformations are narrow.

📌 **Job 1 → 1 stage**.

---

### **Step 5 — Another Transformation + collect()**

* You now have: **Persisted DataFrame → Another transformation → collect()**.
* This triggers **Job 2**:

  * Reads persisted data **from disk**.
  * Applies the additional transformation.
  * Collects to driver.
  * Again only **1 stage** because transformations are narrow.

📌 **Job 2 → 1 stage**.

---

## **Final Answer**

| Action      | Job | Stages | Notes                                             |
| ----------- | --- | ------ | ------------------------------------------------- |
| `count()`   | 1   | 1      | Reads from source, persists to disk, counts       |
| `collect()` | 2   | 1      | Reads from disk cache, applies new transformation |
| **Total**   | 2   | 2      | Persistence avoids recomputation from source      |

---

## **Behavior Change with MEMORY\_ONLY**

| Storage Level    | Effect                                                                                                                                                                                         |
| ---------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **DISK\_ONLY**   | Data is stored only on disk. Second job must **read from disk** (slower).                                                                                                                      |
| **MEMORY\_ONLY** | Data is stored in memory (RAM). Second job reads **directly from memory** (much faster). If memory is insufficient, Spark recomputes partitions on demand (can cause performance variability). |

💡 **Key point:** Persist/caching **does not reduce the number of jobs** — it reduces recomputation **within jobs triggered by subsequent actions**.

---

## **Icon Visualization**

```
Stage 1 (Job 1)
Read → Narrow ops → Persist(DISK_ONLY) → count()

Stage 2 (Job 2)
Read persisted data → New transformation → collect()
```

---

# **Chained Narrow and Wide Transformations**

> You have:
>
> * Line 1: Read data.
> * Line 2: `map()` (narrow).
> * Line 3: `flatMap()` (narrow).
> * Line 4: `reduceByKey()` (wide).
> * Line 5: `mapValues()` (narrow).
> * Line 6: `collect()` (action).
>
> **Question:** How many stages will be created, and where is the shuffle happening?

Let’s break down this **Chained Narrow and Wide Transformations** case step-by-step,
because here the key is to spot **exactly where the shuffle happens**.

---

## **Step-by-Step Reasoning**

### **Line 1 — Read data**

* Lazy operation — no job yet.

---

### **Line 2 — map()**

* **Narrow transformation** — stays within the same partition.
* Still part of the same stage.

---

### **Line 3 — flatMap()**

* **Narrow transformation** — no shuffle.
* Still in same stage.

---

### **Line 4 — reduceByKey()**

* **Wide transformation** — requires grouping all values with the same key.
* Causes a **shuffle boundary**:

  * Spark writes intermediate data (shuffle write) and then redistributes partitions.
  * This **ends Stage 1** and starts **Stage 2**.

---

### **Line 5 — mapValues()**

* **Narrow transformation** — operates on data within each partition.
* Stays in **Stage 2**.

---

### **Line 6 — collect()**

* **Action** — triggers the whole computation.

---

## **Final Stage Breakdown**

* **Stage 1:**
  Read → map() → flatMap() → **shuffle write** (reduceByKey).

* **Stage 2:**
  **shuffle read** (reduceByKey output) → mapValues() → collect().

---

## **Final Answer**

* **Number of stages:** **2**
* **Where shuffle happens:** At `reduceByKey()` — Spark must move data across the cluster based on keys.

---

## **Icon Visualization**

```
Stage 1
┌─────────────┐
│   Read      │
└─────┬───────┘
      ▼
   map()   (narrow)
      ▼
 flatMap() (narrow)
      ▼
reduceByKey() (wide - shuffle write)
───────────────────────────── shuffle boundary
Stage 2
shuffle read → mapValues() (narrow) → collect()
```

---

💡 **Key interview takeaway:**

* Narrow transformations can be chained in the same stage.
* Wide transformations **always** cause a shuffle boundary → new stage.
* Here, only **one wide transformation** → **two stages** total.

---

# **Two Actions After Wide Transformation**

> You have:
>
> * Read from Parquet.
> * Apply `reduceByKey()` (wide transformation).
> * Call `count()` (action).
> * Call `saveAsTextFile()` (action).
>
> **Question:** How many jobs will be triggered? Will the shuffle happen twice or be reused?

This **Two Actions After Wide Transformation** scenario is a great one for testing if someone understands **Spark’s lazy evaluation** *and* **shuffle reuse**.

---

## **Step-by-Step Execution Flow**

### **Step 1 — Read from Parquet**

* Lazy operation — no job yet.

---

### **Step 2 — reduceByKey()**

* **Wide transformation** — will cause a **shuffle boundary** when an action is triggered.
* Data must be **grouped by key** across partitions.

---

### **Step 3 — count()**

* First action → **triggers Job 1**:

  * **Stage 1:** Read Parquet → Shuffle Write (reduceByKey).
  * **Stage 2:** Shuffle Read → Count.

📌 **Job 1 → 2 stages**.

---

### **Step 4 — saveAsTextFile()**

* Second action → **triggers Job 2**:

  * **Here’s the tricky part:** Spark **does not** automatically cache shuffle results.
  * So `reduceByKey()` will be recomputed → **shuffle happens again**.
  * **Stage 1:** Read Parquet → Shuffle Write (reduceByKey).
  * **Stage 2:** Shuffle Read → Save as text file.

📌 **Job 2 → 2 stages**.

---

## **Final Answer**

* **Number of jobs:** **2** (one per action).
* **Shuffle happens twice** (because no caching is applied).
* **Total stages:** **4** (2 stages per job).

---

## **Optimization Insight**

If you did:

```python
rdd = rdd.reduceByKey(...).cache()
```

Then:

* First action (`count()`) would trigger computation & store shuffle output in memory/disk.
* Second action (`saveAsTextFile()`) would read from the cached result — **no second shuffle**.

---

## **Icon Visualization (Without Cache)**

```
Job 1:
Stage 1: Read → reduceByKey() → shuffle write
Stage 2: shuffle read → count()

Job 2:
Stage 3: Read → reduceByKey() → shuffle write
Stage 4: shuffle read → saveAsTextFile()
```

---

💡 **Key Interview Takeaway:**
Without caching, wide transformations **will be recomputed** for every action — leading to multiple shuffles. Caching can save both compute and network overhead.

---
