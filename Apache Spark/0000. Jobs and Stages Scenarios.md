# Consider a scenario where your spark job has 5 lines of code. Out of these 5 lines of code, 3 lines are narrow transformation, 1 line is wide transformation, and 1 line is action. How many jobs and stages will be created in this case?

## **Step 1: Understand the definitions**

* **Narrow Transformation** ðŸ”¹: Data from each partition is used to create data for **only one partition** in the next stage (e.g., `map`, `filter`). These do **not** cause a shuffle, so they remain in the same stage.
* **Wide Transformation** ðŸ”¹: Data from one partition may be sent to **multiple partitions** in the next stage (e.g., `groupByKey`, `reduceByKey`). These **do cause** a shuffle and therefore create a **stage boundary**.
* **Action** ðŸŽ¯: Triggers the execution of the DAG, producing **at least one job**.

---

## **Step 2: Apply it to the scenario**

You have:

1. **Line 1:** Narrow transformation
2. **Line 2:** Narrow transformation
3. **Line 3:** Narrow transformation
4. **Line 4:** Wide transformation (**shuffle**)
5. **Line 5:** Action

---

## **Step 3: Determine Jobs**

* An **action** triggers a **job**.
* You have **only 1 action**, so **only 1 job** is created.

**âœ… Number of Jobs = 1**

---

## **Step 4: Determine Stages**

* All narrow transformations before a wide transformation belong to the **same stage**.
* The wide transformation creates a **shuffle boundary**, starting a new stage.
* After the wide transformation, there may be further narrow transformations before the action â€” they remain in the same stage as the wide transformationâ€™s output stage.

**In your case:**

* **Stage 1:** Narrow â†’ Narrow â†’ Narrow (before shuffle)
* **Stage 2:** Wide transformation â†’ Action

**âœ… Number of Stages = 2**

---

### **Final Answer**

* **Jobs:** **1**
* **Stages:** **2**

---

# **Multiple Actions Triggering Multiple Jobs**

> You have a Spark application with the following steps:
>
> * Line 1: Read a DataFrame from a CSV file.
> * Line 2: Apply `filter()` (narrow transformation).
> * Line 3: Apply `groupBy()` followed by `agg()` (wide transformation).
> * Line 4: Call `count()` (action).
> * Line 5: Call `collect()` (action) on the original filtered DataFrame.
>
> **Question:** How many jobs and stages will be created, and why?

1ï¸âƒ£ **Read CSV** â†’ *creates initial DataFrame*
2ï¸âƒ£ **filter()** â†’ *narrow transformation*
3ï¸âƒ£ **groupBy().agg()** â†’ *wide transformation (shuffle)*
4ï¸âƒ£ **count()** â†’ *action*
5ï¸âƒ£ **collect()** â†’ *action* (but on the **filtered DataFrame**, not aggregated one)

---

## **Step-by-Step Execution Flow**

### **Step 1 & 2 â€” Read + Filter**

* **Read CSV** is not an action; Spark just builds the logical plan.
* **filter()** is **narrow** (no shuffle).

ðŸ’¡ At this stage â†’ **still no job triggered**.

---

### **Step 3 â€” groupBy + agg**

* **groupBy().agg()** is a **wide transformation** â†’ introduces **shuffle boundary**.
* This will split execution into **two stages** when an action is triggered later.

---

### **Step 4 â€” count()**

* First **action** â†’ **triggers Job 1**.
* Execution plan:

  * **Stage 1:** Read CSV â†’ Filter â†’ Shuffle Write (groupBy).
  * **Stage 2:** Shuffle Read â†’ Aggregate â†’ Count.

ðŸ“Œ **Job 1 â†’ 2 stages**.

---

### **Step 5 â€” collect() on filtered DataFrame**

* This uses the DataFrame **before groupBy()**.
* No shuffle needed here â€” only **narrow transformations** from Read â†’ Filter â†’ Collect.

ðŸ“Œ **Job 2 â†’ 1 stage**.

---

## **Final Answer**

* **Total Jobs:** **2**

  * Job 1 â†’ triggered by `count()` on aggregated data.
  * Job 2 â†’ triggered by `collect()` on filtered data.

* **Total Stages:** **3**

  * Job 1 â†’ **Stage 1** (narrow ops before shuffle) + **Stage 2** (shuffle read + agg).
  * Job 2 â†’ **Stage 3** (narrow ops only).

---

## **Icon Visualization**

```
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  Read CSV   â”‚
          â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚ Narrow
          â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
          â”‚   filter()  â”‚
          â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚ Wide (Shuffle)
                â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ groupBy+agg â”‚
          â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
          â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
          â”‚   count()   â”‚  â† **Job 1**  
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          Stage 1 â†’ Stage 2

   (Parallel branch for filtered DataFrame)
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  filter()   â”‚
          â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  collect()  â”‚  â† **Job 2**  
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          Stage 3
```

---

âœ… **Final Table:**

| Action      | Job | Stages |
| ----------- | --- | ------ |
| `count()`   | 1   | 2      |
| `collect()` | 2   | 1      |
| **Total**   | 2   | 3      |

---

# **Caching Between Actions**

> You have a DataFrame and perform these operations:
>
> * Line 1: Read data from Parquet.
> * Line 2: `filter()` (narrow transformation).
> * Line 3: `map()` (narrow transformation).
> * Line 4: `cache()` the result.
> * Line 5: `count()` (action).
> * Line 6: `collect()` (action) on the cached DataFrame.
>
> **Question:** How many jobs and stages will be created, and what difference does caching make here?

## **Step-by-Step Execution Flow**

### **Step 1 â€” Read Parquet**

* Reading Parquet is a **lazy** operation.
* No job is triggered yet â€” Spark just builds the logical plan.

---

### **Step 2 & 3 â€” filter() + map()**

* Both are **narrow transformations** (no shuffle).
* Still **no job triggered** yet â€” plan just keeps extending.

---

### **Step 4 â€” cache()**

* `cache()` is also **lazy** â€” it only marks the DataFrame to be persisted **when an action runs**.
* No execution happens yet.

---

### **Step 5 â€” count()**

* First **action** â†’ triggers **Job 1**.
* Job execution:

  * **Stage 1:** Read Parquet â†’ Filter â†’ Map â†’ Store in cache â†’ Count.
  * Since no wide transformation is involved, only **1 stage** is created.

ðŸ“Œ **Job 1 â†’ 1 stage**.

---

### **Step 6 â€” collect() on cached DataFrame**

* Second **action** â†’ triggers **Job 2**.
* Because the DataFrame is **cached**, Spark will **reuse the cached data** instead of recomputing from source.
* Only **1 stage** is executed â€” data is fetched directly from cache into driver.

ðŸ“Œ **Job 2 â†’ 1 stage**.

---

## **Without Caching**

If there was **no cache()**, the second action (`collect()`) would have to **recompute** from the source:

* Read Parquet â†’ Filter â†’ Map again.
* Still 1 stage per job, but **data would be read & processed twice**.

With caching:

* Data is computed once (Job 1) and stored in memory/disk.
* Job 2 simply reads from cache â€” **faster execution**.

---

## **Final Answer**

| Action      | Job | Stages | Notes                                                       |
| ----------- | --- | ------ | ----------------------------------------------------------- |
| `count()`   | 1   | 1      | Reads from source, applies transformations, stores in cache |
| `collect()` | 2   | 1      | Reads from cache                                            |
| **Total**   | 2   | 2      | Caching prevents recomputation                              |

---

## **Icon Visualization**

```
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Read Parquet â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ Narrow
         â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   filter()    â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ Narrow
         â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
         â”‚    map()      â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ Cache
         â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   count()     â”‚ â† **Job 1** (Stage 1)
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  collect()    â”‚ â† **Job 2** (Stage 2 - Reads from cache)
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

âœ… **Key point interviewers look for here:**
Caching **doesnâ€™t reduce job count**, but it **avoids recomputation** in later jobs, improving performance for iterative or repeated actions.

---

# **Multiple Wide Transformations Before Action**

> You have:
>
> * Line 1: Read from JSON.
> * Line 2: `filter()` (narrow transformation).
> * Line 3: `join()` with another DataFrame (wide transformation).
> * Line 4: `groupBy()` and `agg()` (wide transformation).
> * Line 5: `show()` (action).
>
> **Question:** How many stages are created? Will there be one or two wide shuffles?

Letâ€™s break down this **Multiple Wide Transformations Before Action** case carefully,
because this is one of those interview questions where people often assume **â€œwide transformations always cause separate stagesâ€** â€” but thatâ€™s not always true.

---

## **Step-by-Step Execution Flow**

### **Step 1 â€” Read from JSON**

* Reading JSON is **lazy**, no execution yet.

---

### **Step 2 â€” filter()**

* **Narrow transformation** â€” no shuffle boundary introduced.
* Still **part of the same stage** when action runs.

---

### **Step 3 â€” join()**

* **Wide transformation** â€” introduces **shuffle boundary**.
* This means Spark will create:

  * **Stage 1:** Read + Filter + Shuffle Write.
  * **Stage 2:** Shuffle Read + Join output.

---

### **Step 4 â€” groupBy().agg()**

* This is **another wide transformation**.
* Whether it creates a **new shuffle** depends on **whether Spark can pipeline it with the previous stage**:

ðŸ’¡ In this case:

* `join()` â†’ produces output that **needs to be grouped again** (keys may be different from join shuffle keys).
* So Spark must **reshuffle** for `groupBy()`.
* This creates **another stage**.

---

### **Step 5 â€” show()**

* First **action** triggers execution of all pending transformations.
* Execution plan now has **two shuffle boundaries**, so:

ðŸ“Œ **Stage 1:** Read â†’ Filter â†’ Shuffle Write (for join).
ðŸ“Œ **Stage 2:** Shuffle Read (join) â†’ Shuffle Write (for groupBy).
ðŸ“Œ **Stage 3:** Shuffle Read (groupBy) â†’ Aggregation â†’ Show.

---

## **Final Answer**

* **Number of stages:** **3**

  * Stage 1: Before join shuffle.
  * Stage 2: After join, before groupBy shuffle.
  * Stage 3: After groupBy shuffle.

* **Number of wide shuffles:** **2**

  * First: For `join()`
  * Second: For `groupBy()`.

---

## **Icon Visualization**

```
Stage 1 (Narrow Ops)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â†’ filter() â†’ shuffle write (join)
â”‚ Read JSON  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Stage 2 (First Shuffle Read)
shuffle read (join) â†’ shuffle write (groupBy)

Stage 3 (Second Shuffle Read)
shuffle read (groupBy) â†’ aggregation â†’ show()
```

---

âœ… **Key Interview Insight:**
Not every sequence of wide transformations can be **combined** â€”
if the second wide transformation uses a **different partitioning key** than the first shuffle, Spark will **reshuffle** and create another stage.

---

# **Action Inside a Loop**

> You have:
>
> * Read a DataFrame from a CSV file.
> * Apply `filter()` (narrow transformation).
> * Inside a for loop (runs 3 times), you call `count()` each time.
>
> **Question:** How many jobs will run in total? How many stages are created?

---

## **Step-by-Step Reasoning**

### **Step 1 â€” Read CSV**

* Lazily creates the DataFrame â€” **no job yet**.

---

### **Step 2 â€” filter()**

* **Narrow transformation**, still **lazy**.
* Plan so far: `Read â†’ Filter` (no shuffle needed).

---

### **Step 3 â€” for loop with count() 3 times**

* `count()` is an **action** â€” each call triggers a **new job**.
* Since **no caching** is used, Spark recomputes the entire lineage each time.
* For each job:

  * **Stage 1:** Read CSV â†’ Filter â†’ Count (only narrow transformations, so just 1 stage).

---

## **Final Answer**

* **Number of jobs:** **3** (one per `count()` call).
* **Number of stages:** **3 total** (1 stage per job Ã— 3 jobs).
* **Reason:** No caching â†’ full recomputation for each action.

---

## **Icon Visualization**

```
for i in range(3):
    count()  â† Action â†’ New Job Every Time
```

**Job 1:**
ðŸ“„ Read CSV â†’ Filter â†’ Count (Stage 1)

**Job 2:**
ðŸ“„ Read CSV â†’ Filter â†’ Count (Stage 2)

**Job 3:**
ðŸ“„ Read CSV â†’ Filter â†’ Count (Stage 3)

---

ðŸ’¡ **With cache():**

* If you had called `.cache()` after filter(),

  * First `count()` â†’ Job 1 (Stage 1) would compute & store in cache.
  * Next two `count()` calls â†’ Job 2 & Job 3 would simply read from cache (still separate jobs, but no recomputation).

---

âœ… **Key Interview Insight:**
Spark does **not** automatically cache results between actions.
Multiple actions on the same DataFrame without caching â†’ multiple jobs + full recomputation each time.

---

# **Persist with Different Storage Levels**

> You have:
>
> * Read data.
> * Apply a series of narrow transformations.
> * `persist(StorageLevel.DISK_ONLY)`.
> * Call `count()` (action).
> * Apply another transformation and call `collect()` (action).
>
> **Question:** How many jobs and stages will be created? How would the behavior differ if you used `MEMORY_ONLY` instead?

Letâ€™s unpack this **Persist with Different Storage Levels** case step by step,
because interviewers often want you to reason about **persistence + storage level effects**.

---

## **Step-by-Step Execution Flow**

### **Step 1 â€” Read Data**

* Lazily creates the DataFrame â€” no job yet.

---

### **Step 2 â€” Narrow Transformations**

* Still lazy â€” no job triggered yet.
* No shuffle introduced.

---

### **Step 3 â€” persist(StorageLevel.DISK\_ONLY)**

* Marks the DataFrame to be stored **on disk only** when computed for the first time.
* No execution yet â€” persist itself doesnâ€™t trigger a job.

---

### **Step 4 â€” count()**

* First **action** â†’ triggers **Job 1**.
* Execution plan:

  * **Stage 1:** Read â†’ Narrow transformations â†’ Write to Disk (persist) â†’ Count.
  * Only **1 stage** because all transformations are narrow.

ðŸ“Œ **Job 1 â†’ 1 stage**.

---

### **Step 5 â€” Another Transformation + collect()**

* You now have: **Persisted DataFrame â†’ Another transformation â†’ collect()**.
* This triggers **Job 2**:

  * Reads persisted data **from disk**.
  * Applies the additional transformation.
  * Collects to driver.
  * Again only **1 stage** because transformations are narrow.

ðŸ“Œ **Job 2 â†’ 1 stage**.

---

## **Final Answer**

| Action      | Job | Stages | Notes                                             |
| ----------- | --- | ------ | ------------------------------------------------- |
| `count()`   | 1   | 1      | Reads from source, persists to disk, counts       |
| `collect()` | 2   | 1      | Reads from disk cache, applies new transformation |
| **Total**   | 2   | 2      | Persistence avoids recomputation from source      |

---

## **Behavior Change with MEMORY\_ONLY**

| Storage Level    | Effect                                                                                                                                                                                         |
| ---------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **DISK\_ONLY**   | Data is stored only on disk. Second job must **read from disk** (slower).                                                                                                                      |
| **MEMORY\_ONLY** | Data is stored in memory (RAM). Second job reads **directly from memory** (much faster). If memory is insufficient, Spark recomputes partitions on demand (can cause performance variability). |

ðŸ’¡ **Key point:** Persist/caching **does not reduce the number of jobs** â€” it reduces recomputation **within jobs triggered by subsequent actions**.

---

## **Icon Visualization**

```
Stage 1 (Job 1)
Read â†’ Narrow ops â†’ Persist(DISK_ONLY) â†’ count()

Stage 2 (Job 2)
Read persisted data â†’ New transformation â†’ collect()
```

---

# **Chained Narrow and Wide Transformations**

> You have:
>
> * Line 1: Read data.
> * Line 2: `map()` (narrow).
> * Line 3: `flatMap()` (narrow).
> * Line 4: `reduceByKey()` (wide).
> * Line 5: `mapValues()` (narrow).
> * Line 6: `collect()` (action).
>
> **Question:** How many stages will be created, and where is the shuffle happening?

Letâ€™s break down this **Chained Narrow and Wide Transformations** case step-by-step,
because here the key is to spot **exactly where the shuffle happens**.

---

## **Step-by-Step Reasoning**

### **Line 1 â€” Read data**

* Lazy operation â€” no job yet.

---

### **Line 2 â€” map()**

* **Narrow transformation** â€” stays within the same partition.
* Still part of the same stage.

---

### **Line 3 â€” flatMap()**

* **Narrow transformation** â€” no shuffle.
* Still in same stage.

---

### **Line 4 â€” reduceByKey()**

* **Wide transformation** â€” requires grouping all values with the same key.
* Causes a **shuffle boundary**:

  * Spark writes intermediate data (shuffle write) and then redistributes partitions.
  * This **ends Stage 1** and starts **Stage 2**.

---

### **Line 5 â€” mapValues()**

* **Narrow transformation** â€” operates on data within each partition.
* Stays in **Stage 2**.

---

### **Line 6 â€” collect()**

* **Action** â€” triggers the whole computation.

---

## **Final Stage Breakdown**

* **Stage 1:**
  Read â†’ map() â†’ flatMap() â†’ **shuffle write** (reduceByKey).

* **Stage 2:**
  **shuffle read** (reduceByKey output) â†’ mapValues() â†’ collect().

---

## **Final Answer**

* **Number of stages:** **2**
* **Where shuffle happens:** At `reduceByKey()` â€” Spark must move data across the cluster based on keys.

---

## **Icon Visualization**

```
Stage 1
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Read      â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
      â–¼
   map()   (narrow)
      â–¼
 flatMap() (narrow)
      â–¼
reduceByKey() (wide - shuffle write)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ shuffle boundary
Stage 2
shuffle read â†’ mapValues() (narrow) â†’ collect()
```

---

ðŸ’¡ **Key interview takeaway:**

* Narrow transformations can be chained in the same stage.
* Wide transformations **always** cause a shuffle boundary â†’ new stage.
* Here, only **one wide transformation** â†’ **two stages** total.

---

# **Two Actions After Wide Transformation**

> You have:
>
> * Read from Parquet.
> * Apply `reduceByKey()` (wide transformation).
> * Call `count()` (action).
> * Call `saveAsTextFile()` (action).
>
> **Question:** How many jobs will be triggered? Will the shuffle happen twice or be reused?

This **Two Actions After Wide Transformation** scenario is a great one for testing if someone understands **Sparkâ€™s lazy evaluation** *and* **shuffle reuse**.

---

## **Step-by-Step Execution Flow**

### **Step 1 â€” Read from Parquet**

* Lazy operation â€” no job yet.

---

### **Step 2 â€” reduceByKey()**

* **Wide transformation** â€” will cause a **shuffle boundary** when an action is triggered.
* Data must be **grouped by key** across partitions.

---

### **Step 3 â€” count()**

* First action â†’ **triggers Job 1**:

  * **Stage 1:** Read Parquet â†’ Shuffle Write (reduceByKey).
  * **Stage 2:** Shuffle Read â†’ Count.

ðŸ“Œ **Job 1 â†’ 2 stages**.

---

### **Step 4 â€” saveAsTextFile()**

* Second action â†’ **triggers Job 2**:

  * **Hereâ€™s the tricky part:** Spark **does not** automatically cache shuffle results.
  * So `reduceByKey()` will be recomputed â†’ **shuffle happens again**.
  * **Stage 1:** Read Parquet â†’ Shuffle Write (reduceByKey).
  * **Stage 2:** Shuffle Read â†’ Save as text file.

ðŸ“Œ **Job 2 â†’ 2 stages**.

---

## **Final Answer**

* **Number of jobs:** **2** (one per action).
* **Shuffle happens twice** (because no caching is applied).
* **Total stages:** **4** (2 stages per job).

---

## **Optimization Insight**

If you did:

```python
rdd = rdd.reduceByKey(...).cache()
```

Then:

* First action (`count()`) would trigger computation & store shuffle output in memory/disk.
* Second action (`saveAsTextFile()`) would read from the cached result â€” **no second shuffle**.

---

## **Icon Visualization (Without Cache)**

```
Job 1:
Stage 1: Read â†’ reduceByKey() â†’ shuffle write
Stage 2: shuffle read â†’ count()

Job 2:
Stage 3: Read â†’ reduceByKey() â†’ shuffle write
Stage 4: shuffle read â†’ saveAsTextFile()
```

---

ðŸ’¡ **Key Interview Takeaway:**
Without caching, wide transformations **will be recomputed** for every action â€” leading to multiple shuffles. Caching can save both compute and network overhead.

---
