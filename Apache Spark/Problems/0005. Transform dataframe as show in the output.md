## PySpark GroupBy and List Aggregation. Transform the below dataframe as show in the output

Given a DataFrame:

| Col1 | col2 |
| ---- | ---- |
| 1    | a    |
| 1    | b    |
| 1    | c    |
| 2    | d    |
| 2    | e    |
| 3    | g    |
| 3    | h    |

Output:

| Col1 | list       |
| ---- | ---------- |
| 1    | \[a, b, c] |
| 2    | \[d, e]    |
| 3    | \[g, h]    |

---

### âœ… **Solution:**

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import collect_list, col

# Create SparkSession
spark = SparkSession.builder.appName("GroupAndCollectList").getOrCreate()

# Sample data
data = [(1, 'a'), (1, 'b'), (1, 'c'),
        (2, 'd'), (2, 'e'),
        (3, 'g'), (3, 'h')]

# Create DataFrame
df = spark.createDataFrame(data, ["Col1", "col2"])

# Group by and aggregate
aggDf = df.groupBy(col("Col1")).agg(collect_list(col("col2")).alias("list"))

# Show result
aggDf.show(truncate=False)
```

---

### ðŸ§¾ **Output:**

```
+-----+---------+
|Col1 |list     |
+-----+---------+
|1    |[a, b, c]|
|2    |[d, e]   |
|3    |[g, h]   |
+-----+---------+
```
