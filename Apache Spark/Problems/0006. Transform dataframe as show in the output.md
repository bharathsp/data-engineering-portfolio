
**Input Data:**

```
1|~abc|~100
2|~gef|~200
3|~jkl|~300
```

**Ouptut Data:**

```
+---+----+-----+
|id |name|value|
+---+----+-----+
|1  |abc |100  |
|2  |gef |200  |
|3  |jkl |300  |
+---+----+-----+
```
---

### ‚úÖ **Solution Using `spark.read.csv(..., sep='|')`:**

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import regexp_replace

# Create Spark session
spark = SparkSession.builder.appName("UseSepPipe").getOrCreate()

# Read the data using sep='|'
df = spark.read.csv("input.txt", sep="|", inferSchema=True).toDF("id", "name", "value")

# Remove '~' from name and value columns
cleaned_df = df.select(
    "id",
    regexp_replace("name", "~", "").alias("name"),
    regexp_replace("value", "~", "").alias("value")
)

# Show result
cleaned_df.show()
```

---

### üßæ **Output:**

```
+---+----+-----+
|id |name|value|
+---+----+-----+
|1  |abc |100  |
|2  |gef |200  |
|3  |jkl |300  |
+---+----+-----+
```

---

### üóÇÔ∏è Note:

Great question! The line below is **not valid** as written:

```python
spark.read.format("csv").option("delimiter", "|~").load("path")
```

---

### ‚ùå What's wrong?

**Delimiter can't be multi-character (`|~`)** in CSV parsing ‚Äî Spark's CSV reader supports **only a single character** as a delimiter.
