Provide Pyspark equivalent for the below SQL query
```sql
SELECT 
    c.name AS customer_name,
    c.region,
    COUNT(DISTINCT o.order_id) AS total_orders,
    SUM(oi.quantity * oi.price_per_unit) AS total_spent,
    ROUND(AVG(o.total_amount), 2) AS avg_order_value,
    CASE 
        WHEN SUM(o.total_amount) > 1000 THEN 'VIP'
        WHEN SUM(o.total_amount) BETWEEN 500 AND 1000 THEN 'Regular'
        ELSE 'New'
    END AS customer_tier
FROM customers c
JOIN orders o ON c.customer_id = o.customer_id
LEFT JOIN order_items oi ON o.order_id = oi.order_id
WHERE o.order_date BETWEEN '2023-01-01' AND '2023-12-31'
GROUP BY c.customer_name, c.region
```
---

Here's the **PySpark equivalent** of your SQL query that performs:

* `JOIN`s between `customers`, `orders`, and `order_items`
* Filters by `order_date`
* Aggregates (total orders, total spent, avg order value)
* Applies a `CASE`-like logic for `customer_tier`

---

### âœ… **PySpark Equivalent Code**

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, sum, countDistinct, avg, round, when

spark = SparkSession.builder.appName("CustomerAnalytics").getOrCreate()

# Assuming DataFrames are already loaded
# customers_df, orders_df, order_items_df

# Step 1: Join DataFrames
joined_df = customers_df.alias("c") \
    .join(orders_df.alias("o"), col("c.customer_id") == col("o.customer_id")) \
    .join(order_items_df.alias("oi"), col("o.order_id") == col("oi.order_id"), how="left")

# Step 2: Filter by order_date
filtered_df = joined_df.filter(
    (col("o.order_date") >= "2023-01-01") & (col("o.order_date") <= "2023-12-31")
)

# Step 3: Group and Aggregate
agg_df = filtered_df.groupBy("c.name", "c.region").agg(
    countDistinct("o.order_id").alias("total_orders"),
    sum(col("oi.quantity") * col("oi.price_per_unit")).alias("total_spent"),
    round(avg("o.total_amount"), 2).alias("avg_order_value"),
    sum("o.total_amount").alias("sum_total_amount")
)

# Step 4: Add customer_tier column using when()
final_df = agg_df.withColumn(
    "customer_tier",
    when(col("sum_total_amount") > 1000, "VIP")
    .when((col("sum_total_amount") >= 500) & (col("sum_total_amount") <= 1000), "Regular")
    .otherwise("New")
).drop("sum_total_amount")  # Drop intermediate column

# Show results
final_df.select(
    col("name").alias("customer_name"),
    "region",
    "total_orders",
    "total_spent",
    "avg_order_value",
    "customer_tier"
).show()
```

---

### ðŸ” Assumptions

* You have these DataFrames:

  * `customers_df` with columns: `customer_id`, `name`, `region`
  * `orders_df` with columns: `order_id`, `customer_id`, `order_date`, `total_amount`
  * `order_items_df` with columns: `order_id`, `quantity`, `price_per_unit`
* All columns are correctly typed (`order_date` is `StringType` or `DateType`)
* If your `order_date` is `StringType`, ensure it's in `'yyyy-MM-dd'` format. If it's not, convert using `to_date()`.
