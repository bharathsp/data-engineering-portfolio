### ðŸ”¹ `groupByKey()`

**Definition:**
Groups all the values for each key into a single sequence (Iterable).

```python
rdd.groupByKey()
```

**Example:**

```python
data = [("a", 1), ("b", 2), ("a", 3)]
rdd = sc.parallelize(data)
rdd.groupByKey().mapValues(list).collect()
# Output: [('a', [1, 3]), ('b', [2])]
```

**Use Case:** When you need to **preserve all values** associated with a key.

**Cons:**

* **Shuffles all values** across the network before grouping.
* **Memory intensive**: All values for a key must fit in memory.

---

### ðŸ”¹ `reduceByKey()`

**Definition:**
Merges the values for each key using an **associative and commutative** reduce function.

```python
rdd.reduceByKey(lambda a, b: a + b)
```

**Example:**

```python
data = [("a", 1), ("b", 2), ("a", 3)]
rdd = sc.parallelize(data)
rdd.reduceByKey(lambda a, b: a + b).collect()
# Output: [('a', 4), ('b', 2)]
```

**Use Case:** When you need to **aggregate** (sum, count, multiply, etc.) values for each key.

**Pros:**

* Combines values **locally** on each partition before shuffling.
* **More efficient** than `groupByKey()` in terms of network and memory.

---

### ðŸ”¸ Summary Table

| Feature      | `groupByKey()`             | `reduceByKey()`              |
| ------------ | -------------------------- | ---------------------------- |
| Operation    | Groups values              | Aggregates values            |
| Shuffling    | High (all values shuffled) | Low (combines locally first) |
| Memory usage | High (stores all values)   | Low (aggregated data only)   |
| Performance  | Slower                     | Faster and more efficient    |
| Use case     | Need all individual values | Need aggregated results      |

---

âœ… **Recommendation**:
Use `reduceByKey()` over `groupByKey()` wherever possible to reduce shuffle and memory usage.
