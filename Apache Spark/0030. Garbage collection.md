### üßπ What is Garbage Collection (GC)?

**Garbage Collection (GC)** is the **automatic process of identifying and freeing up memory** that is no longer in use by the program.

When a program creates objects or variables, they occupy memory (RAM). Over time, many of these objects are no longer needed. GC reclaims this unused memory so it can be used again ‚Äî **without the programmer needing to manually release it**.

---

### üìå Why is Garbage Collection Required?

| Reason                                | Explanation                                                                                                       |
| ------------------------------------- | ----------------------------------------------------------------------------------------------------------------- |
| üß† **Avoid memory leaks**             | Without GC, unused objects would remain in memory, eventually causing the program to crash due to lack of memory. |
| ‚öôÔ∏è **Automates memory management**    | Developers don‚Äôt have to manually allocate or free memory (unlike in languages like C/C++).                       |
| üöÄ **Improves application stability** | GC helps ensure that memory is reused efficiently, reducing the chances of out-of-memory (OOM) errors.            |
| üìâ **Prevents dangling pointers**     | No risk of accessing memory that has already been freed (common in manual memory management).                     |

---

### üîç How it Works (Simplified):

1. GC **tracks all references** to objects in memory.
2. If an object has **no references pointing to it**, it's considered "garbage."
3. The GC **frees** the memory used by that object.
4. This space becomes available for future objects.

---

### üõ†Ô∏è In Different Languages:

| Language                 | Garbage Collection                                                              |
| ------------------------ | ------------------------------------------------------------------------------- |
| **Java / Scala / Spark** | Managed by JVM using algorithms like G1GC, CMS, etc.                            |
| **Python**               | Reference counting + generational garbage collection (`gc` module)              |
| **C / C++**              | ‚ùå No built-in GC ‚Äî manual memory management (use `malloc/free` or `new/delete`) |
| **JavaScript**           | Automatically done in modern engines like V8                                    |

---

### üßº In Spark/PySpark:

* JVM GC manages memory for executors (shuffle, cache, broadcast).
* Python GC manages local variables/data structures on the driver.
* Improper GC tuning or memory leaks can lead to job failures, especially in large data pipelines.

---

### ‚úÖ Summary:

> **Garbage Collection is a memory cleaning process that automatically reclaims memory used by objects no longer needed, preventing memory leaks and improving program performance.**

---

### üßπ Garbage Collection in PySpark

Garbage collection in PySpark involves both **Python-level** and **JVM-level** memory management. Understanding both is important for optimizing memory usage and avoiding OOM (Out Of Memory) errors.

---

## üîÅ **1. Python Garbage Collection (Driver/Executors running Python code)**

PySpark runs on top of Python using the Py4J gateway to interact with the JVM. The Python objects you create are subject to **Python's built-in garbage collector**, which uses **reference counting + generational GC**.

### üîß Manual Python GC (Optional):

```python
import gc
gc.collect()  # Manually trigger garbage collection in Python
```

However, PySpark jobs usually don‚Äôt require this unless you're managing large local data structures (e.g., large list/dict accumulations).

---

## üîÅ **2. JVM Garbage Collection (Spark Core - Executors)**

Spark runs on the JVM, and most memory is consumed on the **JVM side** during shuffles, caching, joins, etc.

### üß† JVM GC Details:

* JVM heap is split into **Young**, **Old**, and **Metaspace** regions.
* GC is handled using algorithms like **G1GC**, **ParallelGC**, or **CMS**.
* Spark allows tuning GC using JVM flags.

---

## ‚öôÔ∏è Spark GC Configuration Tips

### üß© a. Set GC Algorithms (in `spark-submit` or `spark-defaults.conf`)

```bash
--conf "spark.executor.extraJavaOptions=-XX:+UseG1GC"
--conf "spark.driver.extraJavaOptions=-XX:+UseG1GC"
```

### üìâ b. Tune Executor Memory

```bash
--executor-memory 4G
--driver-memory 2G
```

### üßΩ c. Avoid Excessive GC by:

* Repartitioning or coalescing to optimize shuffle size
* Caching wisely: use `persist(StorageLevel.MEMORY_AND_DISK)`
* Unpersisting unused RDDs/DataFrames:

  ```python
  df.unpersist()
  ```

### üîç d. GC Logs for Debugging:

Enable GC logs:

```bash
--conf "spark.executor.extraJavaOptions=-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps"
```

---

## üßº Best Practices

| Task                     | Action                                                    |
| ------------------------ | --------------------------------------------------------- |
| Cache only what's reused | Use `.cache()` or `.persist()` carefully                  |
| Release memory           | Call `.unpersist()`                                       |
| Control partitions       | Use `repartition()` to avoid tiny tasks or large shuffles |
| Broadcast small lookups  | Use `broadcast()` to reduce memory footprint              |

---

### üì¶ Summary

* Python objects ‚Üí Python GC (`gc.collect()`)
* JVM memory (executors) ‚Üí JVM GC (G1GC, etc.)
* Use `.unpersist()` to clean up cached data
* Tune `--executor-memory`, GC algorithms, and partitions to avoid memory leaks and OOM
