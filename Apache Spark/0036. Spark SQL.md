# ğŸ§© **What is Spark SQL?**

* **Spark SQL** is a **module in Apache Spark** for working with **structured and semi-structured data**.
* It allows you to use **SQL queries** inside Spark, along with the **DataFrame API**.
* You can query data from multiple sources: **Parquet, JSON, Hive, ORC, Avro, JDBC, CSV**.

ğŸ‘‰ In simple terms:
Itâ€™s like having a **SQL engine inside Spark**, so you can write queries just like in a database, but they run **in parallel across a cluster**.

---

## âš¡ Key Features of Spark SQL

* ğŸ› ï¸ **SQL + DataFrame API** â†’ Use either SQL syntax or Python/Scala/Java API.
* ğŸ” **Schema-aware** â†’ Knows column names & types (unlike RDDs).
* âš¡ **Catalyst Optimizer** â†’ Optimizes queries for best execution.
* ğŸ”„ **Integration** â†’ Works with Hive Metastore, BI tools (Tableau, Power BI).
* ğŸ“Š **Unified access** â†’ Query data from Hive tables, Parquet, JSON, JDBC databases.

---

# ğŸ—ï¸ **How Spark SQL Works**

1. **You write a query** (SQL or DataFrame API).
2. Spark converts it into a **Logical Plan** (what to do).
3. The **Catalyst Optimizer** rewrites it into an **Optimized Logical Plan**.
4. It becomes a **Physical Plan** (how to do it).
5. Spark executes it in a **distributed way** on the cluster.

---

# ğŸ“Š Examples

## Example 1: Running SQL Query

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("SparkSQLExample").getOrCreate()

# Create DataFrame
data = [("Alice", 25), ("Bob", 30), ("Cathy", 28)]
df = spark.createDataFrame(data, ["name", "age"])

# Register as SQL temporary view
df.createOrReplaceTempView("people")

# Run SQL query
result = spark.sql("SELECT name FROM people WHERE age > 27")
result.show()
```

**Output:**

```
+----+
|name|
+----+
| Bob|
|Cathy|
+----+
```

---

## Example 2: Using DataFrame API

```python
df.filter(df.age > 27).select("name").show()
```

**Output:**

```
+----+
|name|
+----+
| Bob|
|Cathy|
+----+
```

ğŸ‘‰ Same result as SQL query, but using Python API.

---

# ğŸ–¼ï¸ Analogy

Think of Spark as a **distributed kitchen ğŸ³**:

* **RDD API** = You cook everything manually (low-level).
* **DataFrame API** = You use a recipe (higher-level).
* **Spark SQL** = You just **order food in plain English** (â€œSELECT pizza WHERE spicyâ€).

---

# âœ… Summary

* **Spark SQL** is a module in Spark for **querying structured data** using SQL or APIs.
* It provides **ease of use** (SQL language), **performance** (Catalyst Optimizer), and **integration** (Hive, JDBC, BI tools).
* Best choice for **ETL, analytics, and working with tabular data** in Spark.
