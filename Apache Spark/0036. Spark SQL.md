# 🧩 **What is Spark SQL?**

* **Spark SQL** is a **module in Apache Spark** for working with **structured and semi-structured data**.
* It allows you to use **SQL queries** inside Spark, along with the **DataFrame API**.
* You can query data from multiple sources: **Parquet, JSON, Hive, ORC, Avro, JDBC, CSV**.

👉 In simple terms:
It’s like having a **SQL engine inside Spark**, so you can write queries just like in a database, but they run **in parallel across a cluster**.

---

## ⚡ Key Features of Spark SQL

* 🛠️ **SQL + DataFrame API** → Use either SQL syntax or Python/Scala/Java API.
* 🔍 **Schema-aware** → Knows column names & types (unlike RDDs).
* ⚡ **Catalyst Optimizer** → Optimizes queries for best execution.
* 🔄 **Integration** → Works with Hive Metastore, BI tools (Tableau, Power BI).
* 📊 **Unified access** → Query data from Hive tables, Parquet, JSON, JDBC databases.

---

# 🏗️ **How Spark SQL Works**

1. **You write a query** (SQL or DataFrame API).
2. Spark converts it into a **Logical Plan** (what to do).
3. The **Catalyst Optimizer** rewrites it into an **Optimized Logical Plan**.
4. It becomes a **Physical Plan** (how to do it).
5. Spark executes it in a **distributed way** on the cluster.

---

# 📊 Examples

## Example 1: Running SQL Query

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("SparkSQLExample").getOrCreate()

# Create DataFrame
data = [("Alice", 25), ("Bob", 30), ("Cathy", 28)]
df = spark.createDataFrame(data, ["name", "age"])

# Register as SQL temporary view
df.createOrReplaceTempView("people")

# Run SQL query
result = spark.sql("SELECT name FROM people WHERE age > 27")
result.show()
```

**Output:**

```
+----+
|name|
+----+
| Bob|
|Cathy|
+----+
```

---

## Example 2: Using DataFrame API

```python
df.filter(df.age > 27).select("name").show()
```

**Output:**

```
+----+
|name|
+----+
| Bob|
|Cathy|
+----+
```

👉 Same result as SQL query, but using Python API.

---

# 🖼️ Analogy

Think of Spark as a **distributed kitchen 🍳**:

* **RDD API** = You cook everything manually (low-level).
* **DataFrame API** = You use a recipe (higher-level).
* **Spark SQL** = You just **order food in plain English** (“SELECT pizza WHERE spicy”).

---

# ✅ Summary

* **Spark SQL** is a module in Spark for **querying structured data** using SQL or APIs.
* It provides **ease of use** (SQL language), **performance** (Catalyst Optimizer), and **integration** (Hive, JDBC, BI tools).
* Best choice for **ETL, analytics, and working with tabular data** in Spark.
