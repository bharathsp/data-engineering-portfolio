# âš¡ **What is Spark Streaming?**

* **Spark Streaming** is a component of Apache Spark for **real-time data processing**.
* It ingests live data streams (e.g., from **Kafka, Flume, HDFS, TCP sockets**) and processes them in **mini-batches**.
* Works on top of the Spark Core RDD API.
* Successor: **Structured Streaming** (uses DataFrames & SQL for real-time).

ğŸ‘‰ Think of it as Sparkâ€™s way of handling **continuous data** rather than static datasets.

---

# ğŸ—ï¸ How Spark Streaming Works

1. Data arrives in **real-time streams** (e.g., tweets, logs, sensor data).
2. Spark Streaming splits the data into **micro-batches** (default: every few seconds).
3. Each batch is processed using Sparkâ€™s RDD/DataFrame operations.
4. Results are pushed out to databases, dashboards, or file systems.

---

# ğŸ› ï¸ Core Component â†’ `StreamingContext`

* Entry point for Spark Streaming.
* Wraps around `SparkContext`.
* Defines **batch interval** (e.g., every 5 seconds).

---

# ğŸ“Š Example: Word Count on Streaming Data

```python
from pyspark import SparkContext
from pyspark.streaming import StreamingContext

# Create Spark Context
sc = SparkContext("local[2]", "NetworkWordCount")

# Create Streaming Context with batch interval = 5 sec
ssc = StreamingContext(sc, 5)

# Create a DStream (stream of data from TCP socket)
lines = ssc.socketTextStream("localhost", 9999)

# Split into words
words = lines.flatMap(lambda line: line.split(" "))

# Count words
word_counts = words.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)

# Print results to console
word_counts.pprint()

# Start Streaming
ssc.start()
ssc.awaitTermination()
```

---

# â–¶ï¸ How to Run This Example

1. Start a TCP server on port **9999** (e.g., using `nc` command in Linux/macOS):

   ```bash
   nc -lk 9999
   ```
2. Type messages into the terminal (e.g., `hello spark hello world`).
3. Spark Streaming job will process and output word counts in **5-second batches**.

---

# ğŸ–¼ï¸ Real-Life Use Cases

* ğŸ“¡ **IoT devices** â†’ Processing sensor data in real time.
* ğŸ¦ **Social media feeds** â†’ Analyzing tweets as they arrive.
* ğŸ“Š **Log monitoring** â†’ Detecting anomalies in server logs.
* ğŸ’³ **Fraud detection** â†’ Flagging suspicious transactions instantly.

---

# ğŸ–¼ï¸ Analogy

Imagine a **conveyor belt ğŸ¢**:

* Data keeps arriving continuously.
* Spark Streaming **cuts it into small trays (micro-batches)**.
* Each tray is processed like a small dataset using Spark.

---

# âœ… Summary

* **Spark Streaming** â†’ Real-time processing using micro-batches.
* Entry point = **`StreamingContext`**.
* Integrates with **Kafka, Flume, HDFS, TCP sockets**.
* Superseded by **Structured Streaming** (preferred for new projects).
