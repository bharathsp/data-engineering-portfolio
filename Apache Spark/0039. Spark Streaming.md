# ⚡ **What is Spark Streaming?**

* **Spark Streaming** is a component of Apache Spark for **real-time data processing**.
* It ingests live data streams (e.g., from **Kafka, Flume, HDFS, TCP sockets**) and processes them in **mini-batches**.
* Works on top of the Spark Core RDD API.
* Successor: **Structured Streaming** (uses DataFrames & SQL for real-time).

👉 Think of it as Spark’s way of handling **continuous data** rather than static datasets.

---

# 🏗️ How Spark Streaming Works

1. Data arrives in **real-time streams** (e.g., tweets, logs, sensor data).
2. Spark Streaming splits the data into **micro-batches** (default: every few seconds).
3. Each batch is processed using Spark’s RDD/DataFrame operations.
4. Results are pushed out to databases, dashboards, or file systems.

---

# 🛠️ Core Component → `StreamingContext`

* Entry point for Spark Streaming.
* Wraps around `SparkContext`.
* Defines **batch interval** (e.g., every 5 seconds).

---

# 📊 Example: Word Count on Streaming Data

```python
from pyspark import SparkContext
from pyspark.streaming import StreamingContext

# Create Spark Context
sc = SparkContext("local[2]", "NetworkWordCount")

# Create Streaming Context with batch interval = 5 sec
ssc = StreamingContext(sc, 5)

# Create a DStream (stream of data from TCP socket)
lines = ssc.socketTextStream("localhost", 9999)

# Split into words
words = lines.flatMap(lambda line: line.split(" "))

# Count words
word_counts = words.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)

# Print results to console
word_counts.pprint()

# Start Streaming
ssc.start()
ssc.awaitTermination()
```

---

# ▶️ How to Run This Example

1. Start a TCP server on port **9999** (e.g., using `nc` command in Linux/macOS):

   ```bash
   nc -lk 9999
   ```
2. Type messages into the terminal (e.g., `hello spark hello world`).
3. Spark Streaming job will process and output word counts in **5-second batches**.

---

# 🖼️ Real-Life Use Cases

* 📡 **IoT devices** → Processing sensor data in real time.
* 🐦 **Social media feeds** → Analyzing tweets as they arrive.
* 📊 **Log monitoring** → Detecting anomalies in server logs.
* 💳 **Fraud detection** → Flagging suspicious transactions instantly.

---

# 🖼️ Analogy

Imagine a **conveyor belt 🎢**:

* Data keeps arriving continuously.
* Spark Streaming **cuts it into small trays (micro-batches)**.
* Each tray is processed like a small dataset using Spark.

---

# ✅ Summary

* **Spark Streaming** → Real-time processing using micro-batches.
* Entry point = **`StreamingContext`**.
* Integrates with **Kafka, Flume, HDFS, TCP sockets**.
* Superseded by **Structured Streaming** (preferred for new projects).
