## üî• What is **Cache** in Apache Spark?

* In Apache Spark, **cache** is a mechanism used to **store intermediate results of RDDs or DataFrames in memory** across the cluster, allowing faster access during repeated use.
* When a dataset is cached, Spark avoids recomputing it from the original source or lineage graph each time it's needed, thus significantly improving performance for iterative algorithms, repeated queries, or actions like `count()`, `show()`, or `collect()`.
* Caching is a **lazy operation**, meaning the actual data is not cached until the first action is triggered.
* While caching accelerates execution, it also consumes memory, so it should be used judiciously when the dataset is reused and fits comfortably in available memory.


---

## üöÄ When to Use `cache()`?

| ‚úÖ **Use Cache When**                                        |
| ----------------------------------------------------------- |
| RDD/DataFrame is **used multiple times** in your workflow.  |
| The transformation is **expensive** (joins, aggregations).  |
| You want to **optimize iterative operations** (like in ML). |
| You want to avoid **re-reading data from disk**.            |

---

## ‚õî When **Not** to Use `cache()`?

| ‚ùå **Avoid Cache When**                              |
| --------------------------------------------------- |
| Dataset is **used only once**.                      |
| The data is **too large to fit in memory**.         |
| You're using multiple huge datasets simultaneously. |
| It leads to **memory pressure or spilling**.        |

---

## ‚úÖ Advantages of Caching

* ‚ö° **Improves performance** by avoiding repeated computation.
* üí° Useful for **iterative algorithms** like KMeans, PageRank.
* üß† **Stores intermediate results** for faster access.
* ‚ôªÔ∏è Reduces I/O operations from disk.

---

## ‚ùå Disadvantages of Caching

* üßÆ Consumes memory on worker nodes.
* üí• Can cause **OutOfMemory errors** if memory is limited.
* üîÅ Requires **manual un-caching** when not needed (`unpersist()`).

---

## üß™ Example: PySpark Code with and without Cache

```python
from pyspark.sql import SparkSession
import time

spark = SparkSession.builder.appName("CacheExample").getOrCreate()

# Read a large dataset (replace with actual path)
df = spark.read.csv("large_dataset.csv", header=True, inferSchema=True)

# Transformation (expensive join or filter)
filtered_df = df.filter(df["some_column"] > 1000)

# Without caching
start_time = time.time()
filtered_df.count()  # First action
filtered_df.count()  # Re-computes everything again
end_time = time.time()
print("Time without cache:", end_time - start_time)

# With caching
filtered_df.cache()

start_time = time.time()
filtered_df.count()  # First action - triggers cache
filtered_df.count()  # Pulled from memory
end_time = time.time()
print("Time with cache:", end_time - start_time)
```

---

## üìà Performance Comparison

| Metric            | Without Cache | With Cache   |
| ----------------- | ------------- | ------------ |
| First `.count()`  | 10 seconds    | 10 seconds   |
| Second `.count()` | 10 seconds    | **0.5 sec**  |
| ‚è± Total Time      | 20 seconds    | **10.5 sec** |

‚û°Ô∏è **\~2x faster overall** when using cache if reused multiple times.

---

## üßπ Don't Forget

Always **release memory** when caching is no longer needed:

```python
filtered_df.unpersist()
```
