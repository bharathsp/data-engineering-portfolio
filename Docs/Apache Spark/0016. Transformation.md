## 🔁 What is a **Transformation** in Spark?

In Apache Spark, a **transformation** is an operation that **creates a new RDD or DataFrame** from an existing one. Transformations are **lazy**, meaning they **don’t immediately execute** — instead, they **build a logical execution plan** (called a DAG), which is only executed when an **action** is triggered.

> ✅ **Transformations define “what to do,” not “when to do it.”**

---

## 🔄 What Happens Internally During a Transformation?

1. Spark records the transformation operation (e.g., `filter`, `select`, `map`) in the **lineage graph**.
2. No actual data processing occurs until an **action** is called.
3. When an action is triggered, Spark:

   * Optimizes the transformations.
   * Splits them into **stages and tasks**.
   * Executes them **in parallel** across worker nodes.

---

## 📋 Types of Transformations in Spark

### 🔹 Narrow Transformations:

* Each partition of the parent RDD/DataFrame is used by **only one partition** of the child.
* **No shuffle required**.
* ✅ Faster and more efficient.

| Example    | Description                       |
| ---------- | --------------------------------- |
| `map()`    | Applies a function to each record |
| `filter()` | Filters records by a condition    |
| `select()` | Selects specific columns          |

---

### 🔸 Wide Transformations:

* Involve **shuffling data across partitions/nodes**.
* One partition of child may depend on **multiple partitions of parent**.

| Example      | Description                            |
| ------------ | -------------------------------------- |
| `groupBy()`  | Groups records by key (causes shuffle) |
| `join()`     | Joins two DataFrames                   |
| `distinct()` | Removes duplicates                     |

---

## 🧪 PySpark Code Example – Transformations

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("TransformationExample").getOrCreate()

# Read input data (transformation – lazy)
df = spark.read.csv("data.csv", header=True, inferSchema=True)

# Transformation 1: Select specific columns
df_selected = df.select("name", "age")

# Transformation 2: Filter based on a condition
df_filtered = df_selected.filter(df_selected["age"] > 30)

# Transformation 3: Create new column
df_transformed = df_filtered.withColumn("age_plus_10", df_filtered["age"] + 10)

# No computation has happened yet!

# Action: Triggers execution
df_transformed.show()
```

---

### ✅ Transformations Applied

| Step | Function       | Type   | Description                             |
| ---- | -------------- | ------ | --------------------------------------- |
| 1    | `select()`     | Narrow | Chooses columns                         |
| 2    | `filter()`     | Narrow | Applies condition on rows               |
| 3    | `withColumn()` | Narrow | Adds computed column                    |
| 4    | `show()`       | Action | Triggers computation and result display |

---

## 🧠 Summary of Transformations

| Feature             | Transformation                                         |
| ------------------- | ------------------------------------------------------ |
| Triggers Execution? | ❌ No                                                   |
| Lazy Evaluated?     | ✅ Yes                                                  |
| Types               | Narrow & Wide                                          |
| Output              | New RDD or DataFrame (immutable)                       |
| Examples            | `map()`, `filter()`, `select()`, `join()`, `groupBy()` |

---

### 🔄 DAG Flow Visualization

```text
Read CSV
   ↓
 select("name", "age")
   ↓
 filter(age > 30)
   ↓
 withColumn("age_plus_10")
   ↓
 show()  ← Triggers all above steps
```
