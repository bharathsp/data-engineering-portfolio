## ğŸ” What is a **Transformation** in Spark?

In Apache Spark, a **transformation** is an operation that **creates a new RDD or DataFrame** from an existing one. Transformations are **lazy**, meaning they **donâ€™t immediately execute** â€” instead, they **build a logical execution plan** (called a DAG), which is only executed when an **action** is triggered.

> âœ… **Transformations define â€œwhat to do,â€ not â€œwhen to do it.â€**

---

## ğŸ”„ What Happens Internally During a Transformation?

1. Spark records the transformation operation (e.g., `filter`, `select`, `map`) in the **lineage graph**.
2. No actual data processing occurs until an **action** is called.
3. When an action is triggered, Spark:

   * Optimizes the transformations.
   * Splits them into **stages and tasks**.
   * Executes them **in parallel** across worker nodes.

---

## ğŸ“‹ Types of Transformations in Spark

### ğŸ”¹ Narrow Transformations:

* Each partition of the parent RDD/DataFrame is used by **only one partition** of the child.
* **No shuffle required**.
* âœ… Faster and more efficient.

| Example    | Description                       |
| ---------- | --------------------------------- |
| `map()`    | Applies a function to each record |
| `filter()` | Filters records by a condition    |
| `select()` | Selects specific columns          |

---

### ğŸ”¸ Wide Transformations:

* Involve **shuffling data across partitions/nodes**.
* One partition of child may depend on **multiple partitions of parent**.

| Example      | Description                            |
| ------------ | -------------------------------------- |
| `groupBy()`  | Groups records by key (causes shuffle) |
| `join()`     | Joins two DataFrames                   |
| `distinct()` | Removes duplicates                     |

---

## ğŸ§ª PySpark Code Example â€“ Transformations

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("TransformationExample").getOrCreate()

# Read input data (transformation â€“ lazy)
df = spark.read.csv("data.csv", header=True, inferSchema=True)

# Transformation 1: Select specific columns
df_selected = df.select("name", "age")

# Transformation 2: Filter based on a condition
df_filtered = df_selected.filter(df_selected["age"] > 30)

# Transformation 3: Create new column
df_transformed = df_filtered.withColumn("age_plus_10", df_filtered["age"] + 10)

# No computation has happened yet!

# Action: Triggers execution
df_transformed.show()
```

---

### âœ… Transformations Applied

| Step | Function       | Type   | Description                             |
| ---- | -------------- | ------ | --------------------------------------- |
| 1    | `select()`     | Narrow | Chooses columns                         |
| 2    | `filter()`     | Narrow | Applies condition on rows               |
| 3    | `withColumn()` | Narrow | Adds computed column                    |
| 4    | `show()`       | Action | Triggers computation and result display |

---

## ğŸ§  Summary of Transformations

| Feature             | Transformation                                         |
| ------------------- | ------------------------------------------------------ |
| Triggers Execution? | âŒ No                                                   |
| Lazy Evaluated?     | âœ… Yes                                                  |
| Types               | Narrow & Wide                                          |
| Output              | New RDD or DataFrame (immutable)                       |
| Examples            | `map()`, `filter()`, `select()`, `join()`, `groupBy()` |

---

### ğŸ”„ DAG Flow Visualization

```text
Read CSV
   â†“
 select("name", "age")
   â†“
 filter(age > 30)
   â†“
 withColumn("age_plus_10")
   â†“
 show()  â† Triggers all above steps
```
