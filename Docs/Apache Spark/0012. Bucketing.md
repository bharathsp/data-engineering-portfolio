## üßä What is **Bucketing** in Spark?

**Bucketing** is a technique in Apache Spark that **divides data into fixed number of buckets (files) based on the hash of a column**. Unlike partitioning, which organizes data by values (directories), bucketing groups **similar values into evenly distributed buckets**, enabling **faster joins, filters, and aggregations**.

---

## üîÑ What Happens During Bucketing?

1. Spark computes a **hash on the specified column**.
2. The data is **divided into *N* buckets** (e.g., 4 buckets).
3. Each record is **written to one of the buckets** based on the hash.
4. During **joins** or **reads**, Spark uses bucket information to **avoid full table scans** and **shuffle**.

> Bucketing is especially powerful when **both datasets in a join are bucketed on the join key** and **number of buckets match**.

---

## ‚úÖ When to Use Bucketing?

| ‚úÖ Ideal Scenarios                                                   |
| ------------------------------------------------------------------- |
| You frequently perform **joins** on specific columns.               |
| You want to **optimize performance without increasing partitions**. |
| You‚Äôre doing **repeated queries** over a dimension column.          |
| Data size is **medium to large**, and **not extremely skewed**.     |

---

## ‚ùå When Not to Use Bucketing?

| ‚ùå Avoid When...                                        |
| ------------------------------------------------------ |
| Data is **small** or doesn‚Äôt benefit from reuse.       |
| Data has **high skew** (some buckets may end up huge). |
| You‚Äôre using **streaming data** or write-once jobs.    |
| You don‚Äôt need to **optimize joins** or **reads**.     |

---

## ‚úÖ Advantages of Bucketing

* ‚ö° **Faster joins** when both datasets are bucketed on join keys.
* üîÑ **Reduces shuffle and sort operations**.
* üì¶ More **efficient file layout** for read-heavy workloads.
* üß† Better memory management due to predictable bucket sizes.

---

## ‚ùå Disadvantages of Bucketing

* ‚ö†Ô∏è Needs **manual management** of bucket count and keys.
* üíΩ Bucketing is done only on **write**, not automatically.
* üïí **Initial bucketing is slow** (extra sort/write step).
* ‚ùó Bucketing is **static** ‚Äî hard to scale with changing data volumes.

---

## üîß Bucketing Syntax in Spark (DataFrame API)

```python
df.write.bucketBy(4, "customer_id").sortBy("customer_id").saveAsTable("bucketed_customers")
```

> Note: Bucketing requires writing to a **Hive-compatible table** (with Hive support enabled).

---

## üß™ PySpark Code Example ‚Äî Before vs After Bucketing

### ‚ö†Ô∏è Without Bucketing (Regular Join with Shuffle)

```python
df1 = spark.read.csv("orders.csv", header=True, inferSchema=True)
df2 = spark.read.csv("customers.csv", header=True, inferSchema=True)

# Regular join (causes shuffle)
start = time.time()
df1.join(df2, on="customer_id").count()
end = time.time()
print("Join time without bucketing:", end - start)
```

---

### ‚úÖ With Bucketing (Optimized Join)

```python
# Enable Hive support
spark = SparkSession.builder \
    .appName("BucketingExample") \
    .enableHiveSupport() \
    .getOrCreate()

# Create bucketed tables
df1.write.bucketBy(8, "customer_id").sortBy("customer_id").mode("overwrite").saveAsTable("bucketed_orders")
df2.write.bucketBy(8, "customer_id").sortBy("customer_id").mode("overwrite").saveAsTable("bucketed_customers")

# Read from bucketed tables
bkt_orders = spark.table("bucketed_orders")
bkt_customers = spark.table("bucketed_customers")

# Perform optimized join
start = time.time()
bkt_orders.join(bkt_customers, "customer_id").count()
end = time.time()
print("Join time with bucketing:", end - start)
```

---

## üìà Performance Comparison

| Metric           | Without Bucketing | With Bucketing      |
| ---------------- | ----------------- | ------------------- |
| Join Shuffle     | ‚úÖ Yes             | ‚ùå No (bucket-aware) |
| Join Time        | \~12 sec          | **\~6 sec**         |
| Shuffle Files    | Many              | Few                 |
| CPU & Disk Usage | High              | Lower               |

---

## üîÅ Bucketing vs Partitioning vs Repartitioning

| Feature       | Bucketing                     | Partitioning                   | Repartitioning           |
| ------------- | ----------------------------- | ------------------------------ | ------------------------ |
| Based On      | **Hash of column**            | Column value (directory)       | Random or column-based   |
| Output Format | Sorted into bucket files      | Subfolders per partition       | Re-partitioned in memory |
| Shuffle Avoid | ‚úÖ Yes for joins/aggregates    | ‚ùå Only avoids reads            | ‚ùå Causes shuffle         |
| Scalability   | Fixed buckets (manual tuning) | Scales with column cardinality | Fully dynamic            |

---

## ‚úÖ Summary

* Use **bucketing** when performance of **joins**, **aggregations**, and **scans** is crucial.
* Works best when **both datasets** are bucketed on the **same column** with the **same number of buckets**.
* Provides **shuffle-free** and **fast** operations at the cost of **additional write time** and **storage planning**.
