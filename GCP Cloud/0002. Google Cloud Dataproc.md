# Google Cloud Dataproc

<img width="106" height="256" alt="image" src="https://github.com/user-attachments/assets/4ebcdeff-9565-460e-b9b3-68cc58c47f75" />

**Google Cloud Dataproc** is a **fully managed service** for running **Apache Spark**, **Apache Hadoop**, **Apache Hive**, **Presto**, and other **open-source big data tools** on Google Cloud. It allows you to **process and analyze large datasets** using familiar frameworks without the overhead of managing infrastructure.

---

### üöÄ Key Highlights of Dataproc:

| Feature                  | Description                                                               |
| ------------------------ | ------------------------------------------------------------------------- |
| **Managed Clusters**     | Quickly create, scale, and delete clusters with Spark, Hadoop, Hive, etc. |
| **Fast Deployment**      | Clusters can be created in **under 90 seconds**.                          |
| **Auto-scaling**         | Automatically adds/removes worker nodes based on load.                    |
| **Integration with GCP** | Works with BigQuery, Cloud Storage, Bigtable, Pub/Sub, etc.               |
| **Customizable**         | Use initialization actions to install libraries or configure software.    |
| **Cost-effective**       | Pay-per-use and supports preemptible VMs to reduce cost.                  |

---

### üîß Components You Can Run on Dataproc:

* **Apache Spark** ‚Äì for distributed data processing and ML
* **Apache Hadoop** ‚Äì for batch processing (MapReduce)
* **Apache Hive** ‚Äì for SQL-like querying on big data
* **Apache Pig**, **Presto**, **Flink**, and more

---

### üß† How It Works:

1. **Create a Dataproc Cluster**: Define number of nodes, machine types, and software.
2. **Submit Jobs**: Spark, Hadoop, Hive, or PySpark jobs.
3. **Monitor Jobs**: Using the Dataproc UI or GCP Monitoring.
4. **Shut Down Cluster** (optional): To avoid unnecessary cost.

---

### üìÅ Example Use Case

You have logs stored in **Google Cloud Storage** and want to analyze user activity:

* Use **Dataproc** to run a **PySpark job** that:

  * Reads logs from GCS.
  * Filters and aggregates data.
  * Writes output back to GCS or loads into BigQuery.

---

### üîÑ Comparison: Dataproc vs. BigQuery

| Feature    | Dataproc                       | BigQuery                                       |
| ---------- | ------------------------------ | ---------------------------------------------- |
| Use Case   | Custom ETL, complex pipelines  | Interactive analytics, dashboards              |
| Processing | Spark, Hadoop (code-driven)    | SQL-based                                      |
| Management | Managed clusters               | Serverless                                     |
| Real-time  | Good with structured streaming | Limited (uses batch load or streaming inserts) |

---

### ‚úÖ Best For:

* Data Engineers and Data Scientists needing **flexible and programmable pipelines**
* Running existing Hadoop/Spark workloads in the cloud
* Building custom **machine learning workflows** with PySpark or Scala
