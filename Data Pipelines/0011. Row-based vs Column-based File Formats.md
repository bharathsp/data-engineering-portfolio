# 🔹 Row-based vs Column-based File Formats

## 1️⃣ Row-based File Formats

* Store data **row by row** (record by record).
* Each row contains all the fields of that record together.

📂 **Examples**:

* CSV (Comma-Separated Values)
* JSON
* Avro (commonly used in streaming pipelines)

📌 **Advantages**:

* **Fast for OLTP / transactional queries** → If you usually need the *whole record*, row storage is best.
* Good for **real-time inserts and updates**.
* Simple and human-readable (CSV, JSON).
* Best suited for **event logs, streaming, APIs**.

📌 **When to Use**:

* **Streaming data ingestion** (Kafka → Avro).
* **Transactional systems** (storing complete orders, user profiles, etc.).
* **Workloads with frequent inserts/updates/deletes**.
* **Row-level access** (you almost always read the full record).

---

## 2️⃣ Column-based File Formats

* Store data **column by column**.
* All values of the same column are stored together.

📂 **Examples**:

* Parquet (Apache, widely used in Spark, Hive, BigQuery, etc.)
* ORC (Optimized Row Columnar, common in Hive/Presto)
* Delta Lake (built on Parquet with versioning)

📌 **Advantages**:

* **Fast for OLAP / analytical queries** → Ideal for *scans, aggregations, filtering*.
* Excellent **compression** (same type values stored together).
* **Schema evolution support** (especially Parquet & ORC).
* Reduced I/O → read only needed columns, not entire rows.
* Great for **data warehouses & lakes**.

📌 **When to Use**:

* **Analytics & reporting** (e.g., "average sales per region").
* **Data lakes & warehouses** (Snowflake, BigQuery, Synapse, Databricks).
* **Machine learning feature stores** (reading subsets of columns).
* **Large-scale historical datasets** with heavy scans.

---

# 🔹 Comparison Table

| Feature              | Row-based (CSV, JSON, Avro) | Column-based (Parquet, ORC, Delta) |
| -------------------- | --------------------------- | ---------------------------------- |
| **Storage format**   | Row by row                  | Column by column                   |
| **Best for**         | OLTP, transactions          | OLAP, analytics                    |
| **Read pattern**     | Full records                | Specific columns, aggregations     |
| **Write pattern**    | Frequent inserts/updates    | Bulk writes, batch loads           |
| **Compression**      | Lower                       | Higher (better space efficiency)   |
| **Schema evolution** | Good (Avro)                 | Good (Parquet, ORC, Delta)         |
| **Examples**         | CSV, JSON, Avro             | Parquet, ORC, Delta                |

---

# 🔹 Practical Scenarios

✅ **Row-based**:

* Kafka streams storing events in **Avro** format.
* API returning **JSON** responses.
* Transactional DB extracts into **CSV**.

✅ **Column-based**:

* Storing clickstream data in **Parquet** for BI dashboards.
* ML training pipeline reading **only features** from Delta/Parquet.
* BigQuery / Synapse warehouses (columnar storage under the hood).

---

👉 **Rule of Thumb**:

* If you need **fast writes and complete records → Row-based**.
* If you need **fast reads, aggregations, and analytics → Column-based**.

---

# 🔹 Benchmark Example: CSV vs Parquet in Spark

Imagine we have a **100 million row dataset** of e-commerce sales:

```text
order_id, customer_id, region, product, quantity, price, timestamp
```

---

## 1️⃣ Write dataset in **CSV (row-based)**

```python
df.write.format("csv").option("header", "true").save("/mnt/data/sales_csv/")
```

* Stored row by row.
* Each record has **all fields** together.
* Compression not great (unless gzip applied).

---

## 2️⃣ Write dataset in **Parquet (column-based)**

```python
df.write.format("parquet").save("/mnt/data/sales_parquet/")
```

* Stored by columns.
* Built-in compression & encoding.
* Schema stored in metadata.

---

## 3️⃣ Query Example: Aggregation (OLAP style)

🔹 Question: *“What’s the total sales per region in the last year?”*

```python
# Query from CSV
csv_df = spark.read.format("csv").option("header", "true").load("/mnt/data/sales_csv/")
csv_df.groupBy("region").sum("price").show()

# Query from Parquet
parquet_df = spark.read.format("parquet").load("/mnt/data/sales_parquet/")
parquet_df.groupBy("region").sum("price").show()
```

---

## 4️⃣ Performance Comparison

| Format      | Size on Disk | Time to Load | Query Runtime | Notes                                   |
| ----------- | ------------ | ------------ | ------------- | --------------------------------------- |
| **CSV**     | \~80 GB      | 60 sec       | 3–4 min       | Reads *all columns* even if only 2 used |
| **Parquet** | \~20 GB      | 10 sec       | 20–30 sec     | Reads only needed columns, compressed   |

📌 **Key Insights**:

* Parquet is **\~4x smaller** on disk due to compression.
* Parquet query is **6–10x faster** since only `region` and `price` columns are read.
* CSV wastes I/O by scanning all columns, even unused ones.

---

## 5️⃣ When CSV Still Wins

* **Streaming ingestion** (simple append, Avro is often better though).
* **Interoperability** (easy to open anywhere).
* **Small datasets** where performance/size isn’t critical.

---

👉 So in practice:

* Store **raw/landing data** in **row-based** (CSV/JSON/Avro).
* Store **curated/analytics-ready data** in **columnar formats** (Parquet/ORC/Delta).
