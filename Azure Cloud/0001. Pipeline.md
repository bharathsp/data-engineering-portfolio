## Pipeline

In **Azure Data Factory (ADF)**, a **pipeline** is a logical grouping of activities that perform a unit of work. A pipeline allows you to manage, schedule, and monitor a series of **data movement** and **data transformation** tasks as a single workflow.

### üîπ Key Points about Pipelines in ADF:

| **Feature**          | **Description**                                                                                                                                                                       |
| -------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Definition**       | A container for a sequence of activities.                                                                                                                                             |
| **Purpose**          | Orchestrate data movement (copy data), data transformation (using data flows or external services), and control flow (using loops, conditionals, etc.).                               |
| **Activities**       | Pipelines can include various types of activities like **Copy Activity**, **Data Flow**, **Stored Procedure**, **Web Activity**, **Lookup**, **If Condition**, **ForEach**, and more. |
| **Triggering**       | Can be triggered manually, via schedule, event-based trigger (e.g., file arrival), or from another pipeline.                                                                          |
| **Parameterization** | Pipelines can be parameterized to allow dynamic behavior.                                                                                                                             |
| **Monitoring**       | Pipelines provide built-in monitoring to track the execution status of each activity.                                                                                                 |

---

### üîß Example Use Case:

Suppose you want to:

1. Copy data from an on-premise SQL Server to Azure Blob Storage.
2. Clean and transform that data using Data Flows.
3. Load it into Azure SQL Database.

You would create a pipeline with:

* A **Copy Activity** for step 1.
* A **Data Flow Activity** for step 2.
* A **Stored Procedure Activity** for step 3.

---

### üîÅ Common Control Flow Activities:

* **If Condition**: Executes one set of activities if a condition is true, another if false.
* **ForEach**: Iterates over a collection.
* **Until**: Loops until a condition is met.
* **Wait**: Pauses the pipeline for a defined duration.

---

### üìå Summary:

A **pipeline in ADF** is like a workflow or ETL job that organizes and runs a series of actions on your data‚Äîfrom extraction, through transformation, to loading. It gives you full control over how data moves and changes in your cloud environment.
