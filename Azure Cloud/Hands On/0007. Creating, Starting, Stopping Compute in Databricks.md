âœ… **Part 1 â€” What is a Databricks Compute (Cluster)** <br>
âœ… **Part 2 â€” Step-by-step: Creating a Cluster** <br>
âœ… **Part 3 â€” Starting & Stopping Compute** <br>
âœ… **Part 4 â€” Key Configuration Options Explained** <br>
âœ… **Part 5 â€” Best Practices** <br>

---

# âš™ï¸ **Part 1: What is a Databricks Compute (Cluster)?**

A **cluster (compute)** in Databricks is a group of virtual machines (**VMs**) that run Spark jobs.
Itâ€™s what executes:

* Notebooks ğŸ§¾
* Jobs (batch/streaming) ğŸ“¦
* ML model training ğŸ§ 

Think of it as your **engine** for data and AI workloads.

---

# ğŸš€ **Part 2: Steps to Create a Databricks Compute / Cluster**

### **ğŸ”¹ Step 1: Go to Databricks Workspace**

1. Sign in to **Azure Portal** â†’ Go to your **Databricks resource**
2. Click **Launch Workspace**
   ![ğŸŒ](https://img.icons8.com/fluency/24/data.png)

<img width="504" height="971" alt="image" src="https://github.com/user-attachments/assets/5b62316b-cb72-4eef-8b92-cb6616b9e922" />

---

### **ğŸ”¹ Step 2: Open Compute Tab**

* In the **left sidebar**, click **Compute âš™ï¸**
* Click **Create Compute (or Create Cluster)**

<img width="1920" height="1976" alt="screencapture-adb-2501887048148376-16-azuredatabricks-net-compute-clusters-new-2025-10-08-14_43_24" src="https://github.com/user-attachments/assets/6e7746e6-aa4f-47e5-bdf8-8e0690bf6600" />

---

### **ğŸ”¹ Step 3: Enter Basic Configuration**

| Setting                        | Description                                    | Example                                              |
| ------------------------------ | ---------------------------------------------- | ---------------------------------------------------- |
| **Compute Name**               | Give a unique name                             | `data-eng-cluster`                                   |
| **Cluster Mode**               | Choose between **Single Node** or **Standard** | `Standard` (for most use cases)                      |
| **Databricks Runtime Version** | Choose runtime (includes Spark + libraries)    | `14.3 LTS (includes Apache Spark 3.5.0, Scala 2.12)` |
| **Access Mode**                | Choose how users access data                   | `Single user` or `Shared`                            |
| **Autopilot Options**          | Optionally enable **Photon** (faster queries)  | âœ… Enable Photon                                      |
| **Worker Type / Driver Type**  | VM type for cluster nodes                      | `Standard_DS3_v2`                                    |
| **Min/Max Workers**            | Define autoscaling range                       | `2â€“8`                                                |
| **Enable Autoscaling**         | Automatically scale based on workload          | âœ… Enable                                             |
| **Termination**                | Set idle time before auto-termination          | `30 minutes`                                         |

---

### **ğŸ”¹ Step 4: (Optional) Advanced Configuration**

Expand **Advanced Options**:

* **Environment Variables** â€” for Spark config or credentials
* **Init Scripts** â€” run setup scripts on cluster startup
* **Tags** â€” assign cost tracking or owner info
* **Spark Config** â€” example:

  ```bash
  spark.sql.shuffle.partitions=200
  spark.databricks.delta.preview.enabled=true
  ```

---

### **ğŸ”¹ Step 5: Create Cluster**

Click **Create Compute**
â³ Wait for the cluster to start (takes 2â€“5 mins)

<img width="1017" height="608" alt="image" src="https://github.com/user-attachments/assets/422794ee-e248-4ed3-970f-eb7841969cc2" />

Status â†’ ğŸŸ¢ **Running** once ready.

---

# ğŸ§  **Part 3: Starting & Stopping Compute**

| Action                | How to Do It                                            | Notes                                 |
| --------------------- | ------------------------------------------------------- | ------------------------------------- |
| â–¶ï¸ **Start**          | Go to **Compute** â†’ find your cluster â†’ click **Start** | Takes ~2â€“3 mins                       |
| â¸ï¸ **Stop**           | Click **Stop**                                          | Suspends compute (saves cost)         |
| â™»ï¸ **Restart**        | Click **Restart**                                       | Applies new configuration             |
| ğŸ—‘ï¸ **Delete**        | Click **Delete**                                        | Permanently removes cluster           |
| âš™ï¸ **Auto-Terminate** | Set idle timeout                                        | Auto-stops after X mins of inactivity |

ğŸ’° **Note:** Stopped clusters **donâ€™t incur compute charges**, only **DBU + storage** if running.

---

# ğŸ§¾ **Part 4: Key Configuration Options Explained**

| Option                      | Description                        | Use Case                    |
| --------------------------- | ---------------------------------- | --------------------------- |
| **Standard Cluster**        | Shared by multiple users           | Collaborative analytics     |
| **Single Node Cluster**     | One VM only (no workers)           | Development, testing        |
| **Job Cluster**             | Auto-created for a single job run  | Automated pipelines         |
| **Photon Engine**           | Native vectorized query engine     | High-performance SQL        |
| **Autopilot / Autoscaling** | Adds/removes workers automatically | Cost optimization           |
| **Termination Timeout**     | Auto-stops after idle              | Avoid billing for idle time |

---

# ğŸ **Part 5: Best Practices**

âœ… Use **autoscaling** to handle variable workloads efficiently.
âœ… Always enable **auto-termination** (15â€“30 min recommended).
âœ… Tag clusters with:

```
Owner = Bharath
Environment = Dev
Project = Customer360
```

âœ… Choose **Photon-enabled runtimes** for faster query execution.
âœ… Use **job clusters** for production jobs to avoid manual management.
âœ… Keep **Unity Catalog + Shared Access** in mind if multiple workspaces use the same data.

---

# ğŸ’¡ **Bonus â€” Shortcut via Notebook Command**

You can also **create or attach** a cluster using the command bar in Databricks Notebook:

* Top-left of the notebook â†’ Click â€œAttach toâ€ â†’ Select existing cluster or â€œCreate new clusterâ€.

---

# ğŸ“Š Visual Summary (Icons)

ğŸ§  **Databricks Workspace**
â¬‡ï¸
âš™ï¸ **Compute (Cluster)** â†’ Runs your Spark code
ğŸ—‚ï¸ **Attached to Notebook / Job**
â¬‡ï¸
ğŸ“ˆ **Processes data in Storage (ADLS / DBFS)**
