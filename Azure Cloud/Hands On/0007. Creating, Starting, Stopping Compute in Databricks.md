✅ **Part 1 — What is a Databricks Compute (Cluster)** <br>
✅ **Part 2 — Step-by-step: Creating a Cluster** <br>
✅ **Part 3 — Starting & Stopping Compute** <br>
✅ **Part 4 — Key Configuration Options Explained** <br>
✅ **Part 5 — Best Practices** <br>

---

# ⚙️ **Part 1: What is a Databricks Compute (Cluster)?**

A **cluster (compute)** in Databricks is a group of virtual machines (**VMs**) that run Spark jobs.
It’s what executes:

* Notebooks 🧾
* Jobs (batch/streaming) 📦
* ML model training 🧠

Think of it as your **engine** for data and AI workloads.

---

# 🚀 **Part 2: Steps to Create a Databricks Compute / Cluster**

### **🔹 Step 1: Go to Databricks Workspace**

1. Sign in to **Azure Portal** → Go to your **Databricks resource**
2. Click **Launch Workspace**
   ![🌐](https://img.icons8.com/fluency/24/data.png)

<img width="504" height="971" alt="image" src="https://github.com/user-attachments/assets/5b62316b-cb72-4eef-8b92-cb6616b9e922" />

---

### **🔹 Step 2: Open Compute Tab**

* In the **left sidebar**, click **Compute ⚙️**
* Click **Create Compute (or Create Cluster)**

<img width="1920" height="1976" alt="screencapture-adb-2501887048148376-16-azuredatabricks-net-compute-clusters-new-2025-10-08-14_43_24" src="https://github.com/user-attachments/assets/6e7746e6-aa4f-47e5-bdf8-8e0690bf6600" />

---

### **🔹 Step 3: Enter Basic Configuration**

| Setting                        | Description                                    | Example                                              |
| ------------------------------ | ---------------------------------------------- | ---------------------------------------------------- |
| **Compute Name**               | Give a unique name                             | `data-eng-cluster`                                   |
| **Cluster Mode**               | Choose between **Single Node** or **Standard** | `Standard` (for most use cases)                      |
| **Databricks Runtime Version** | Choose runtime (includes Spark + libraries)    | `14.3 LTS (includes Apache Spark 3.5.0, Scala 2.12)` |
| **Access Mode**                | Choose how users access data                   | `Single user` or `Shared`                            |
| **Autopilot Options**          | Optionally enable **Photon** (faster queries)  | ✅ Enable Photon                                      |
| **Worker Type / Driver Type**  | VM type for cluster nodes                      | `Standard_DS3_v2`                                    |
| **Min/Max Workers**            | Define autoscaling range                       | `2–8`                                                |
| **Enable Autoscaling**         | Automatically scale based on workload          | ✅ Enable                                             |
| **Termination**                | Set idle time before auto-termination          | `30 minutes`                                         |

---

### **🔹 Step 4: (Optional) Advanced Configuration**

Expand **Advanced Options**:

* **Environment Variables** — for Spark config or credentials
* **Init Scripts** — run setup scripts on cluster startup
* **Tags** — assign cost tracking or owner info
* **Spark Config** — example:

  ```bash
  spark.sql.shuffle.partitions=200
  spark.databricks.delta.preview.enabled=true
  ```

---

### **🔹 Step 5: Create Cluster**

Click **Create Compute**
⏳ Wait for the cluster to start (takes 2–5 mins)

<img width="1017" height="608" alt="image" src="https://github.com/user-attachments/assets/422794ee-e248-4ed3-970f-eb7841969cc2" />

Status → 🟢 **Running** once ready.

---

# 🧠 **Part 3: Starting & Stopping Compute**

| Action                | How to Do It                                            | Notes                                 |
| --------------------- | ------------------------------------------------------- | ------------------------------------- |
| ▶️ **Start**          | Go to **Compute** → find your cluster → click **Start** | Takes ~2–3 mins                       |
| ⏸️ **Stop**           | Click **Stop**                                          | Suspends compute (saves cost)         |
| ♻️ **Restart**        | Click **Restart**                                       | Applies new configuration             |
| 🗑️ **Delete**        | Click **Delete**                                        | Permanently removes cluster           |
| ⚙️ **Auto-Terminate** | Set idle timeout                                        | Auto-stops after X mins of inactivity |

💰 **Note:** Stopped clusters **don’t incur compute charges**, only **DBU + storage** if running.

---

# 🧾 **Part 4: Key Configuration Options Explained**

| Option                      | Description                        | Use Case                    |
| --------------------------- | ---------------------------------- | --------------------------- |
| **Standard Cluster**        | Shared by multiple users           | Collaborative analytics     |
| **Single Node Cluster**     | One VM only (no workers)           | Development, testing        |
| **Job Cluster**             | Auto-created for a single job run  | Automated pipelines         |
| **Photon Engine**           | Native vectorized query engine     | High-performance SQL        |
| **Autopilot / Autoscaling** | Adds/removes workers automatically | Cost optimization           |
| **Termination Timeout**     | Auto-stops after idle              | Avoid billing for idle time |

---

# 🏁 **Part 5: Best Practices**

✅ Use **autoscaling** to handle variable workloads efficiently.
✅ Always enable **auto-termination** (15–30 min recommended).
✅ Tag clusters with:

```
Owner = Bharath
Environment = Dev
Project = Customer360
```

✅ Choose **Photon-enabled runtimes** for faster query execution.
✅ Use **job clusters** for production jobs to avoid manual management.
✅ Keep **Unity Catalog + Shared Access** in mind if multiple workspaces use the same data.

---

# 💡 **Bonus — Shortcut via Notebook Command**

You can also **create or attach** a cluster using the command bar in Databricks Notebook:

* Top-left of the notebook → Click “Attach to” → Select existing cluster or “Create new cluster”.

---

# 📊 Visual Summary (Icons)

🧠 **Databricks Workspace**
⬇️
⚙️ **Compute (Cluster)** → Runs your Spark code
🗂️ **Attached to Notebook / Job**
⬇️
📈 **Processes data in Storage (ADLS / DBFS)**
