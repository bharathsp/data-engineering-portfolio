### ‚úÖ **Question:**

You are given a user activity dataset containing the following columns:

* `user_id` ‚Äì unique identifier for each user
* `Timestamp` ‚Äì timestamp of each activity
* `activity_type` ‚Äì type of activity (`login`, `logout`, `addcart`, etc.)

---

### **Tasks:**

1. Calculate the **total session time per user per day**, where a session is defined as the time between a `login` and a directly following `logout`.
2. Display the **count of each activity type per user per day**.
3. Show basic statistics such as total records and number of unique users.
4. **Drop incomplete records** (rows with `null` values).
5. **Save the daily activity counts** to a Parquet file.

---

### ‚úÖ **PySpark Code Solution:**

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, unix_timestamp, lag, sum as spark_sum, count, to_date
from pyspark.sql.window import Window

# Create Spark session
spark = SparkSession.builder.appName("UserActivity").getOrCreate()

# Load CSV file
df = spark.read.csv("activity_data.csv", header=True, inferSchema=True)

# Print total records
print("Record count:", df.count())

# Print unique user count
print("Number of users:", df.select("user_id").distinct().count())

# Drop rows with null values
df_clean = df.dropna()

# Add activity_date column
df_clean = df_clean.withColumn("activity_date", to_date("Timestamp"))

# Filter only login/logout records
log_df = df_clean.filter(col("activity_type").isin("login", "logout")) \
    .withColumn("ts", unix_timestamp(col("Timestamp"))) \
    .orderBy("user_id", "activity_date", "ts")

# Define window spec
window = Window.partitionBy("user_id", "activity_date").orderBy("ts")

# Add previous activity type and timestamp
log_df = log_df.withColumn("prev_type", lag("activity_type").over(window)) \
               .withColumn("prev_ts", lag("ts").over(window))

# Compute session time (only logout events following login)
session_times = log_df.filter((col("activity_type") == "logout") & (col("prev_type") == "login")) \
                      .withColumn("session_time", col("ts") - col("prev_ts"))

# Total session time per user per day
session_summary = session_times.groupBy("user_id", "activity_date") \
                               .agg(spark_sum("session_time").alias("total_session_time"))

session_summary.show()

# Count of each activity per user per day
activity_counts = df_clean.groupBy("activity_date", "user_id", "activity_type") \
                          .agg(count("*").alias("activity_count"))

activity_counts.show()

# Save activity counts as Parquet
activity_counts.write.mode("overwrite").parquet("output/activity_counts.parquet")
```

---

### üìù **Summary of Outputs:**

1. `session_summary` ‚Üí Total session time (in seconds) per user per day
2. `activity_counts` ‚Üí Count of each activity per user per day
3. Parquet file saved to: `output/activity_counts.parquet`
