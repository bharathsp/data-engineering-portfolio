### 🔍 How to Identify Skewed Data in Big Data Systems (like PySpark or SQL)

**Skewed data** refers to **uneven distribution** of data across partitions, leading to performance bottlenecks — especially in operations like **joins**, **groupBy**, or **aggregations**.

---

## ✅ Common Symptoms of Data Skew:

| Symptom                                | Description                                                                |
| -------------------------------------- | -------------------------------------------------------------------------- |
| 🐢 One task is much slower than others | When most Spark tasks finish quickly but a few take a very long time.      |
| 🧊 Task stragglers                     | Some tasks get stuck at 99% progress for a long time.                      |
| 💥 OOM errors                          | Executors run out of memory due to too much data in a single partition.    |
| ⚖️ Large file size variation           | When output partitions vary widely in size (e.g., some 1 MB, one is 2 GB). |

---

## 🔬 How to Identify Skewed Data

### 🔹 1. **Use Data Profiling (group and count)**

```python
df.groupBy("key_column").count().orderBy("count", ascending=False).show()
```

This will show keys with a **very high number of rows**, indicating skew.

---

### 🔹 2. **Check Spark UI (Stages & Tasks tab)**

* Go to **Spark UI** → **Stages** → Click on a Stage
* Look at the **Task duration**, **Shuffle Read Size**, and **Bytes Spilled**
* **Skewed tasks** will show:

  * Significantly higher shuffle read size
  * Long task duration
  * More memory/disk spill

---

### 🔹 3. **Use `df.rdd.glom().map(len).collect()`**

Check **data distribution** across partitions:

```python
df.rdd.glom().map(len).collect()
```

If output looks like `[0, 1000, 5, 3, 1000000]`, it's skewed — one partition has most of the data.

---

### 🔹 4. **Use Spark’s built-in metrics**

* Enable adaptive query execution (AQE) to identify skew:

```python
spark.conf.set("spark.sql.adaptive.enabled", "true")
```

Then Spark can log skewed joins and even attempt to mitigate them.

---

### 🔹 5. **Use SQL to inspect frequency distribution**

```sql
SELECT key_column, COUNT(*) as freq
FROM table
GROUP BY key_column
ORDER BY freq DESC
```

---

## 🧠 Example Scenario

Let’s say you're joining on a column `customer_id`, and one customer appears 10 million times, while others appear 100 times. This will **cause skew** in the shuffle phase.

---

## 🚨 Bonus: Common Columns That Cause Skew

| Column                    | Why it gets skewed                               |
| ------------------------- | ------------------------------------------------ |
| `status`, `type`          | Few distinct values (e.g., "Active", "Inactive") |
| `country`, `region`       | One or two dominant values (like US or India)    |
| `user_id` or `product_id` | Some very active users/products                  |

---

## ✅ What Next?

Once you've **identified skew**, you can mitigate it using:

* **Salting the key** (for skewed joins)
* **Broadcast joins** (for small tables)
* **Repartitioning**
* **Using adaptive execution** (`spark.sql.adaptive.skewJoin.enabled=true`)
