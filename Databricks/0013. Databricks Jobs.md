# 🔹 What is a **Job** in Databricks?

A **Job** in Databricks is a **way to run a task (or set of tasks) on a schedule or on demand**.
It’s the **basic execution unit** for automation in Databricks.

---

## 🔹 Key Features of Databricks Jobs

### 1. **Task Execution** 🧩

* A job can run one or more **tasks**, such as:

  * A **notebook**
  * A **JAR file** (Scala/Java code)
  * A **Python script**
  * A **SQL query or dashboard refresh**
  * A **Delta Live Table (DLT) pipeline**

---

### 2. **Scheduling** ⏱️

* Jobs can be triggered by:

  * Manual run (on-demand)
  * Time-based schedule (cron expressions)
  * External events via API

---

### 3. **Clusters** ☁️

* Jobs run on **job clusters** (temporary clusters auto-created for execution).
* Cluster lifecycle: created → run job → terminated (to save costs).

---

### 4. **Monitoring** 🔔

* Each job run has logs, execution time, and results.
* You can configure **alerts** on success/failure.
* Run history stored for auditing and debugging.

---

### 5. **Retries & Dependencies** 🔄

* Supports **retries** on failure.
* Tasks within a job can have **dependencies** (task A must finish before task B).

---

## 🔹 Jobs vs. Workflows

| Feature    | **Jobs** ⚙️                       | **Workflows** 🔄                                                    |
| ---------- | --------------------------------- | ------------------------------------------------------------------- |
| Purpose    | Run and schedule **tasks**        | Orchestrate **complex pipelines** (multiple jobs & tasks)           |
| Scope      | Single job with tasks             | End-to-end workflow with dependencies                               |
| UI         | Jobs UI                           | Workflows UI (but both are integrated now)                          |
| Complexity | Simple (ETL script, notebook run) | Advanced orchestration (branching, parallel tasks, retries, alerts) |

👉 In fact, **Workflows is built on top of Jobs** — Workflows = Jobs + orchestration & monitoring features.

---

## 🔹 Visual Flow

**Example: A Job**

```
Job: Daily ETL Pipeline
   └── Task: Run Notebook "Ingest_and_Transform"
```

**Example: A Workflow (built with Jobs)**

```
Workflow: Analytics Pipeline
   ├── Task 1: Ingest Data (Job 1 - Notebook)
   ├── Task 2: Transform Data (Job 2 - PySpark Script)
   ├── Task 3: Train ML Model (Job 3 - MLflow)
   └── Task 4: Refresh Dashboard (Job 4 - SQL Query)
```

---

✅ **In short**:

* **Job** = The execution unit (run notebooks, scripts, queries, pipelines).
* **Workflows** = Bigger picture → orchestrates multiple jobs & tasks into end-to-end pipelines.
