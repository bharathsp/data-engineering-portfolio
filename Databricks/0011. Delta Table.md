## Delta Table

A **Delta Table** is a **data storage format** based on **Delta Lake**, an open-source storage layer that brings **ACID transactions**, **versioning**, and **schema enforcement** to data lakes (like S3, ADLS, etc.), typically used with **Apache Spark**.

Delta Tables are foundational to the **Databricks Lakehouse architecture**, blending the reliability of data warehouses with the scale of data lakes.

---

### ğŸ§  **In Simple Terms:**

A **Delta Table** is like a **Parquet table with superpowers** â€” you can query it like a normal table, but it supports updates, deletes, merges, time travel, and much more.

---

### âœ… **Key Features of Delta Table**

| Feature                     | Description                                                         |
| --------------------------- | ------------------------------------------------------------------- |
| ğŸ”„ **ACID Transactions**    | Guarantees consistency for concurrent reads/writes                  |
| ğŸ“œ **Schema Enforcement**   | Prevents bad data from corrupting the table                         |
| ğŸ•’ **Time Travel**          | Query previous versions of data using timestamps or version numbers |
| ğŸ—ƒï¸ **Efficient Storage**   | Stores data in **Parquet** + transaction log (`_delta_log`)         |
| ğŸ” **Fast Reads/Writes**    | Optimized with **data skipping**, **Z-ordering**, and **caching**   |
| ğŸ”§ **Merge/Upsert Support** | Supports `MERGE`, `UPDATE`, and `DELETE`                            |
| ğŸš€ **Streaming + Batch**    | Works seamlessly with streaming and batch data                      |

---

### ğŸ“‚ **Delta Table File Structure**

```
/my_delta_table/
â”œâ”€â”€ part-0001.parquet
â”œâ”€â”€ part-0002.parquet
â””â”€â”€ _delta_log/
    â”œâ”€â”€ 00000000000000000000.json
    â”œâ”€â”€ 00000000000000000001.json
    â””â”€â”€ ...
```

---

### ğŸ’» **Creating Delta Tables in PySpark**

#### 1ï¸âƒ£ **From a DataFrame**

```python
df.write.format("delta").save("/path/to/delta-table")
```

#### 2ï¸âƒ£ **Using SQL (Databricks or Spark SQL)**

```sql
CREATE TABLE sales
USING DELTA
AS SELECT * FROM sales_data;
```

---

### ğŸ” **Update, Merge, and Delete in Delta Table**

```python
from delta.tables import DeltaTable

delta_table = DeltaTable.forPath(spark, "/path/to/delta-table")

# Update
delta_table.update(
    condition="region = 'APAC'",
    set={"sales": "sales * 1.1"}
)

# Delete
delta_table.delete("sales < 100")

# Merge (Upsert)
delta_table.alias("target").merge(
    source_df.alias("source"),
    "target.id = source.id"
).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()
```

---

### ğŸ•°ï¸ **Time Travel**

```python
# Read an old version
df = spark.read.format("delta").option("versionAsOf", 2).load("/path/to/delta-table")

# Or read as of timestamp
df = spark.read.format("delta").option("timestampAsOf", "2024-07-10").load("/path/to/delta-table")
```

---

### ğŸ”„ **Convert Parquet to Delta**

```python
from delta.tables import DeltaTable

DeltaTable.convertToDelta(spark, "parquet.`/path/to/parquet-folder`")
```

---

### ğŸ§¾ Summary

| Property         | Delta Table                      |
| ---------------- | -------------------------------- |
| Storage Format   | Parquet + Delta transaction log  |
| ACID Support     | âœ… Yes                            |
| Time Travel      | âœ… Yes                            |
| Schema Evolution | âœ… Yes                            |
| Update/Delete    | âœ… Yes (not in regular Parquet)   |
| Query Support    | SQL / DataFrame API / Delta APIs |
