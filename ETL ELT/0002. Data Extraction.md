## ðŸ”¹ **Data Extraction Fundamentals**

> **Data Extraction** is the first step in ETL/ELT, where data is pulled from one or more **source systems** for processing.

---

## ðŸ”¸ **1. RDBMS (Relational Databases)**

> e.g., **PostgreSQL, MySQL, Oracle, SQL Server**

### âœ… Techniques:

* SQL Queries (`SELECT * FROM table`)
* JDBC/ODBC connectors
* Incremental extraction using:

  * **Timestamps** (`WHERE updated_at > last_run_time`)
  * **Auto-incrementing IDs**
  * **Triggers or Change Data Capture (CDC)**

### âœ… Tools:

* Apache Sqoop (for Hadoop)
* Airbyte, Fivetran, Talend
* Python + `psycopg2`, `sqlalchemy`, or `mysql-connector`

### âœ… Example (Python + psycopg2 for PostgreSQL):

```python
import psycopg2
conn = psycopg2.connect(database="sales_db", user="user", password="pwd", host="localhost")
cur = conn.cursor()
cur.execute("SELECT * FROM transactions WHERE updated_at > '2023-01-01'")
rows = cur.fetchall()
```

---

## ðŸ”¸ **2. APIs (REST/SOAP)**

> e.g., **Salesforce API, Google Ads API, Twitter API**

### âœ… Techniques:

* REST APIs using `requests`, `http.client`
* Authentication:

  * OAuth2, API Keys, Tokens
* Pagination, throttling, retries
* JSON/XML response parsing

### âœ… Tools:

* Python (requests, urllib)
* Airbyte, Stitch
* Postman (for testing)

### âœ… Example (REST API using Python):

```python
import requests
headers = {'Authorization': 'Bearer YOUR_TOKEN'}
response = requests.get("https://api.example.com/data", headers=headers)
data = response.json()
```

---

## ðŸ”¸ **3. Flat Files (CSV, JSON, Excel)**

> e.g., Data dumps, data exchange files

### âœ… Techniques:

* Local files or cloud storage (S3, GCS, Azure Blob)
* Read using `pandas`, `csv`, `json` libraries

### âœ… Example (Python + Pandas):

```python
import pandas as pd
df = pd.read_csv('sales_data.csv')
```

### âœ… Tools:

* Python, Spark
* AWS Glue, Informatica, NiFi

---

## ðŸ”¸ **4. Logs (App Logs, Server Logs, Access Logs)**

> e.g., Nginx logs, application logs in S3, error logs

### âœ… Techniques:

* Streaming (Kafka, Fluentd)
* Batch ingestion via cron jobs or ETL scripts
* Regex and parsing logic for structured logs

### âœ… Tools:

* Logstash (from ELK Stack)
* Fluent Bit, Filebeat, AWS CloudWatch Logs
* Python: `open()`, `re`, `json` modules

---

## ðŸ”¸ **5. NoSQL Databases**

> e.g., **MongoDB, Cassandra, DynamoDB**

### âœ… Techniques:

* MongoDB: BSON documents, use queries with `pymongo`
* Cassandra: Query via CQL (Cassandra Query Language)

### âœ… Tools:

* Airbyte, Talend
* PyMongo (MongoDB), Cassandra driver for Python

### âœ… Example (MongoDB):

```python
from pymongo import MongoClient
client = MongoClient("mongodb://localhost:27017")
db = client['analytics']
records = db.transactions.find({"status": "paid"})
```

---

## ðŸ”¹ **Best Practices for Extraction**

* Use incremental loads (avoid full extraction)
* Add metadata columns: `extracted_at`, `source_system`
* Implement logging & error handling
* Monitor for schema drift and API changes
* Validate extracted data (row count, nulls, etc.)
