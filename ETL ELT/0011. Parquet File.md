## ðŸ”¹ What is a Parquet File?

> **Parquet** is a **columnar storage file format** designed for efficient data storage and retrieval, especially suited for **big data workloads**.

* Developed by **Apache**.
* Commonly used with tools like **Apache Spark**, **Hive**, **Presto**, **AWS Athena**, **BigQuery**, and **Snowflake**.

---

## ðŸ”¹ What Kind of Format is Parquet?

| Feature              | Description                                     |
| -------------------- | ----------------------------------------------- |
| **Storage Format**   | **Columnar** (vs. row-based like CSV)           |
| **Encoding**         | Binary + Optional compression                   |
| **Schema Support**   | Yes (schema is embedded in file)                |
| **Self-describing**  | Yes (metadata + data)                           |
| **Interoperability** | Supports multiple languages (Java, Python, C++) |

---

## ðŸ”¹ Parquet vs Row-Based Formats (e.g., CSV, JSON)

| Feature     | **Parquet (Columnar)**        | **CSV/JSON (Row-based)**     |
| ----------- | ----------------------------- | ---------------------------- |
| Reads       | Fast for **specific columns** | Reads whole row, even unused |
| Writes      | More complex                  | Simple                       |
| Compression | Better                        | Less efficient               |
| Schema      | Yes (strongly typed)          | No (loosely typed)           |
| File Size   | Smaller (due to compression)  | Larger                       |

---

## ðŸ”¹ Advantages of Parquet Format

### âœ… 1. **Efficient Columnar Storage**

* Stores each column separately â†’ Enables fast reads of only relevant columns
* Ideal for **analytics**, **OLAP**, and **queries on subsets of columns**

### âœ… 2. **Compression & Encoding**

* Built-in compression (Snappy, GZIP, Brotli) reduces storage costs
* Column-based compression is more effective due to similar data types

### âœ… 3. **Schema Evolution**

* Supports changes in schema (e.g., adding new columns)
* Embeds schema and metadata in the file

### âœ… 4. **Interoperability with Big Data Tools**

* Read/write compatible with Spark, Hive, Presto, Snowflake, AWS Athena, and more

### âœ… 5. **Optimized for Cloud Storage**

* Performs well on S3, GCS, Azure Blob due to block-based structure

---

## ðŸ”¹ Real-World Use Cases

| Use Case                         | Why Parquet Works Well              |
| -------------------------------- | ----------------------------------- |
| Data Lake storage (e.g., AWS S3) | Small file size + efficient scans   |
| ETL pipelines using Apache Spark | Parallel columnar processing        |
| BI tools querying large datasets | Read only queried columns           |
| Machine learning feature store   | Efficient load of selected features |

---

## ðŸ”§ Python Example: Writing and Reading Parquet using Pandas

```python
import pandas as pd

# Create sample data
df = pd.DataFrame({
    'id': [1, 2, 3],
    'name': ['Alice', 'Bob', 'Charlie'],
    'salary': [50000, 60000, 70000]
})

# Write to Parquet
df.to_parquet('employees.parquet', index=False)

# Read from Parquet
df_parquet = pd.read_parquet('employees.parquet')
print(df_parquet)
```

> Youâ€™ll need `pyarrow` or `fastparquet` installed.

---

## ðŸ”§ PySpark Example:

```python
df.write.mode("overwrite").parquet("s3://my-bucket/data/employees/")
df = spark.read.parquet("s3://my-bucket/data/employees/")
```

---

## ðŸ”¹ Summary

| Feature       | Parquet Format                           |
| ------------- | ---------------------------------------- |
| File Type     | **Columnar**, binary, compressed         |
| Best For      | Big data, analytics, cloud-based storage |
| Compression   | Built-in, efficient (Snappy, GZIP)       |
| Tools Support | Spark, Hive, Athena, Snowflake, etc.     |
