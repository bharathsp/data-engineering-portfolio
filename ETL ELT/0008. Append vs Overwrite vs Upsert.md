## üîπ 1. **Append**

### ‚úÖ **Definition:**

> Adds new data **to the end** of an existing dataset or table, **without modifying existing records**.

### üì¶ **Use Case Example:**

* Appending **daily sales transactions** to a master `sales.csv` file.
* Adding **new user sign-ups** to a customer table.

---

### üêç **Python Example (Pandas)**:

```python
import pandas as pd

# Existing dataset
existing_df = pd.read_csv("sales_master.csv")

# New data to append
new_data = pd.DataFrame({
    'order_id': [1004, 1005],
    'amount': [250, 400]
})

# Append and save
updated_df = pd.concat([existing_df, new_data], ignore_index=True)
updated_df.to_csv("sales_master.csv", index=False)
```

---

### ‚ö° **PySpark Example**:

```python
df_new.write.mode("append").csv("s3://bucket/sales_data/")
```

---

## üîπ 2. **Overwrite**

### ‚úÖ **Definition:**

> Replaces the **entire existing dataset or table** with the new data.

### üì¶ **Use Case Example:**

* Weekly refreshed **product catalog** from a supplier.
* Recomputing **monthly aggregated KPIs** and replacing old results.

---

### üêç **Python Example (Pandas)**:

```python
new_df = pd.DataFrame({
    'product_id': [1, 2],
    'product_name': ['Pen', 'Pencil']
})

# Overwrite previous version
new_df.to_csv("product_catalog.csv", index=False)
```

---

### ‚ö° **PySpark Example**:

```python
df_new.write.mode("overwrite").parquet("s3://bucket/product_catalog/")
```

---

## üîπ 3. **Upsert (Merge)**

### ‚úÖ **Definition:**

> Combines **insert** and **update**:
>
> * **Insert** new records
> * **Update** existing records (matched by key)

### üì¶ **Use Case Example:**

* Syncing **CRM data** where some customer info is new, some updated.
* Ingesting **event logs** where a few event IDs may reappear with changes.

---

### üêç **Python Example (Pandas)**:

```python
# Existing customer data
df_existing = pd.DataFrame({
    'customer_id': [1, 2],
    'name': ['Alice', 'Bob'],
    'country': ['US', 'UK']
})

# New batch with an update to ID 2 and a new customer
df_new = pd.DataFrame({
    'customer_id': [2, 3],
    'name': ['Bob Updated', 'Charlie'],
    'country': ['Canada', 'Germany']
})

# Perform upsert
df_merged = pd.concat([df_existing, df_new]).drop_duplicates(subset='customer_id', keep='last')
```

---

### ‚ö° **PySpark Upsert using Delta Lake (merge)**:

```python
from delta.tables import DeltaTable

delta_table = DeltaTable.forPath(spark, "/mnt/data/customers")
updates_df = spark.read.format("parquet").load("/mnt/data/new_customers")

# Perform merge
delta_table.alias("target").merge(
    updates_df.alias("source"),
    "target.customer_id = source.customer_id"
).whenMatchedUpdateAll() \
 .whenNotMatchedInsertAll() \
 .execute()
```

---

## üîπ Summary Table

| Operation | What It Does                          | Common Use Case                 | Python Functionality    |
| --------- | ------------------------------------- | ------------------------------- | ----------------------- |
| Append    | Adds new rows to existing dataset     | Daily logs, transaction history | `pd.concat()`           |
| Overwrite | Replaces the entire dataset           | KPI refresh, product catalog    | `to_csv(..., mode='w')` |
| Upsert    | Updates matching records, inserts new | CRM sync, user profile updates  | Merge + dedup logic     |
