## üîπ What is Bulk Loading?

> **Bulk loading** is a high-performance data loading technique where **large volumes of data** (typically from files like CSV/Parquet) are ingested into a **data warehouse in batches** using optimized commands.

---

## ‚úÖ Why Use Bulk Loading?

| Benefit              | Explanation                                            |
| -------------------- | ------------------------------------------------------ |
| **Faster ingestion** | Loads thousands/millions of rows in seconds            |
| **Parallelism**      | Leverages parallel loading using warehouse nodes       |
| **Compression**      | Supports compressed file formats (e.g., GZIP, Parquet) |
| **Cost-efficient**   | Reduces compute cost compared to row-by-row inserts    |

---

## üî∏ Amazon Redshift ‚Äì `COPY` Command

### ‚úÖ Syntax:

```sql
COPY schema.table
FROM 's3://your-bucket/data/'
CREDENTIALS 'aws_access_key_id=...;aws_secret_access_key=...'
CSV IGNOREHEADER 1;
```

### üîß Features:

* Supports formats: CSV, JSON, Parquet, AVRO
* Supports compression: gzip, bzip2
* Parallel loading from multiple files

### üß† Real-Life Use Case:

> Load daily sales data from S3 (compressed `.csv.gz` files) into the `sales_facts` table.

---

### üêç **Python Example with `psycopg2`**:

```python
import psycopg2

conn = psycopg2.connect(dbname='mydb', user='admin', password='pwd', host='redshift-url')
cur = conn.cursor()

copy_sql = """
COPY sales_facts FROM 's3://data-bucket/sales_2025.csv.gz'
CREDENTIALS 'aws_access_key_id=...;aws_secret_access_key=...'
CSV GZIP IGNOREHEADER 1;
"""

cur.execute(copy_sql)
conn.commit()
```

---

## üî∏ Snowflake ‚Äì `PUT` + `COPY INTO`

### ‚úÖ Step 1: Upload file to Snowflake **stage** (internal or external)

```sql
PUT file://local/path/sales.csv @%sales_table;
```

> Uploads local file to a stage linked to the `sales_table`

---

### ‚úÖ Step 2: Load data into table

```sql
COPY INTO sales_table
FROM @%sales_table
FILE_FORMAT = (TYPE = CSV FIELD_OPTIONALLY_ENCLOSED_BY = '"' SKIP_HEADER = 1);
```

---

### üîß Features:

* Supports external stages: S3, GCS, Azure
* Supports formats: CSV, JSON, Avro, Parquet
* Validates schema, errors, and load metadata

---

### üß† Real-Life Use Case:

> Upload a ZIP file with product data from a supplier and bulk load into `product_dim` in Snowflake using a **named stage**.

---

### üêç **Python Example with `snowflake-connector-python`**:

```python
import snowflake.connector

conn = snowflake.connector.connect(user='...', password='...', account='...')
cs = conn.cursor()

# Step 1: Upload file
cs.execute("PUT file://product.csv @my_stage")

# Step 2: Load into table
cs.execute("""
COPY INTO product_dim
FROM @my_stage/product.csv
FILE_FORMAT = (TYPE = 'CSV' SKIP_HEADER = 1);
""")
```

---

## üîπ Bulk Load Best Practices

| Tip                                 | Description                                           |
| ----------------------------------- | ----------------------------------------------------- |
| Split large files into smaller ones | Enables parallel loading across clusters              |
| Use compressed formats              | Reduces transfer time and cost (e.g., `.csv.gz`)      |
| Validate schema before loading      | Avoids truncation and misalignment issues             |
| Monitor using system tables/logs    | Redshift: STL\_LOAD\_ERRORS, Snowflake: COPY\_HISTORY |

---

## üîπ Summary Table

| Platform      | Bulk Command        | Source                  | Format Support     | Example Tool                 |
| ------------- | ------------------- | ----------------------- | ------------------ | ---------------------------- |
| **Redshift**  | `COPY`              | S3                      | CSV, JSON, Parquet | `psycopg2`, `boto3`          |
| **Snowflake** | `PUT` + `COPY INTO` | Internal/External Stage | CSV, JSON, Parquet | `snowflake-connector-python` |
